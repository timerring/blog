[{"id":0,"href":"/posts/0404-The-invitation-only-mechanism-from-clubhouse-to-manus/","title":"The Invitation Only Mechanism: From Clubhouse to Manus","section":"Blog","content":"虽然自小就听到过会员制的概念，但是也仅限于超市或者药店作为一种鼓励积分的方式，亦或是理发店等固定销售半径的捆绑销售的手段。不同于现在人们所谈论“邀请会员制”的概念，关于“邀请会员制”，我认为英文中有一个非常有意思的词，叫做 exclusive，意思是“只，仅仅”，但是在礼物等场景中 exclusive 尝尝用来形容“特定的，独家的”，追根溯源，本质上都是一种排他性(exclusivity)的文化覆盖。\n之前在宣传自己的项目时，通过观察和实践，我发现普遍邀请制的论坛或者网站，活跃度都普遍比非邀请制的要高，这是一个非常反直觉的事情。对于提供同等的服务情况下，按照预想情况，非邀请制的平台门槛更低，用户更多，一个大基数的平台活跃度理应超过相对封闭的邀请制平台。但是相反的结果或许能说明一些问题。\n我真正意义上第一次接触邀请制，应该是在 2021 年使用 Clubhouse1。这个软件由 ex-google Rohan Seth 和 Paul Davison 在 2019 年成立，提供的服务很简单，就是多人实时在线语音聊天，当然这个功能其实并不稀奇，每个国家的通讯软件很早之前都有类似于群聊天的功能。但是真正让它风靡的原因就是 exclusivity。\n首先，作为一个 invitation-only 的形式，以聊天室的形式建立社交关系。因此初始的种子用户相当关键，他们找到了创投界的 celebrities ，自上而下积累优质用户。同时限制了入驻用户的邀请名额，每人两个 invitation head count，此外还有预留邀请 reserved以及 Waiting List，这是第一层 exclusivity。 再者，在应用中存在不同的 Club2，每个 Club 也有自己的 admin，审批着对应 Club membership 的名额。这是第二层的 exclusivity。 最后，应用主要还是通过 Room 的聊天室形式建立社交关系，线上实时语音往往相较于视频直播等方式会给人拉进距离的感觉，就像是打电话，你能感受到此刻人就在电话的另一端讲述、聆听。但是又同时有排他性，因为只有被邀请的人才能参与讨论，其他的 visitors 只能聆听，在实际的活动又形成了第三层 exclusivity。 这三层的 exclusivity 使得 Clubhouse 在早期迅速积累了大量的用户，同时又保持了相当高的活跃度和粘性，成为了现象级的产品，但是它并不是第一个通过邀请制成功的产品。\n回归根源，最早的邀请制产品大概是 1997 年的 SixDegrees，起源于 Frigyes Karinthy 提出的 Six Degrees of Separation 理论，即世界上所有人都能通过不多于 6 条认识链相互联系。再到后期的 Facebook，冷启动也是通过向校内的 KOL 发送注册信件发展的，从最初的 Only @harvard.edu 再到 Company email，通过邀请制构建互信的网络，同时能激发没有加入者的 FOMO 心理。\n类似的例子还有很多，Linkedin 作为邀请制形成圈子的代表，也是通过硅谷企业家和投资人作为构建的关键核心，出现了爆炸式的增长。还有gmail 在 2004 年愚人节推出，起初也是 Invitation-only，很快就大受欢迎，所有人争先恐后地体验，因为最初的用户将会获得永久的好处，早期体验用户能申请到他们想要的用户名，而后面的用户则不得不在名字后加一堆后缀了。同样邀请制也对宣传有益，早期的用户更愿意为了受邀请而在 appstore 中增加好评。\n当然也包括最近广为传播的 Manus，Manus 预料到预告中的首个通用 Agent 会起所有人的关注，因此配合 KOL 的宣传以及邀请制，不仅可以获得更广的传播，同时也能很好地控制用户数，防止更大的服务器开销，尽管面临着很多质疑，但是在宣传层面上的量级远远超过肖弘之前的产品 Monica。\n当然邀请制并非长期发展的 silver bullet，最早知乎的冷启动也是通过邀请制完成的，从 2013 年开放注册之后，就会面临内容质量下滑，早期优质用户流失。更糟糕的是，很多论坛可能基于安全考量，会设置注册一天内禁止操作，但这并非最明智的决定，因为用户一般都是想要交流才会选择注册，等待的过程最终会造成留存量降低，毕竟大部分人记不得昨天都做了些什么。\n从这种意义上来看，产品的冷启动方式并非只有 AB 测试，邀请会员制或许也是不可忽视的重要途径。在所有人都想尽可能地投放更多广告占领市场，计算 Customer Acquisition Cost 的时候，邀请会员制这种看似退，实则进，以杠杆撬动广告的起步方式倒是很有意思。\nhttps://en.wikipedia.org/wiki/Clubhouse_(app)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://support.clubhouse.com/hc/en-us\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":1,"href":"/posts/how-to-use-git-submodule/","title":"How to Use Git Submodule","section":"Blog","content":"Git submodule1 is a feature of Git that allows you to include a Git repository as a subdirectory of another Git repository. It is often used to manage dependencies between projects.\nHow to add a submodule # For the introduced repository, you don\u0026rsquo;t need to do anything special.\nFor the parent repository, you need to do the following steps:\nAdd the submodule to the repository: git submodule add https://github.com/another/repo.git path/to/submodules/another_repo Then you will find the .gitmodules file in the root directory of the repository, which is used to manage the submodules.\nInitialize the submodule: git submodule init If you want to update the submodule, you can use the following command: git submodule update You should commit this change: git add . git commit -m \u0026#34;Add submodule\u0026#34; git push origin \u0026lt;branch-name\u0026gt; If you want to remove the submodule, you can use the following command: git submodule deinit \u0026lt;submodule-path\u0026gt; How to update the submodule # You can enter the submodule directory and make some commits, then push or pull. Then you can back to the parent repository and commit the changes.\nHow can other people clone the repository with the submodule? # The same operation as above:\ngit clone \u0026lt;repository-url\u0026gt; git submodule init git submodule update # or two in one git submodule update --init --recursive or just one line command:\ngit clone --recurse-submodules \u0026lt;repository\u0026gt; How to remove the submodule # git submodule deinit \u0026lt;submodule-path\u0026gt; git rm \u0026lt;submodule-path\u0026gt; Reference # Git Submodules\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":2,"href":"/posts/live-streaming-infra-and-protocols/","title":"Live Streaming infra and protocols","section":"Blog","content":"In our daily life, you may always see the live streaming on the internet, such as the live streaming of the video conference, the live streaming of the sports event, the live streaming of the online course, etc. And sometimes you even start your own live streaming, but we all get used to just clicking the button on the screen to start the live streaming, do you know what is happening behind the scene?\nIntroduction # Before we start, we need to know some common terms in the live streaming.\nThe process of live streaming from alicloud\u0026lt;sup id=\u0026#34;fnref:1\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;#fn:1\u0026#34; class=\u0026#34;footnote-ref\u0026#34; role=\u0026#34;doc-noteref\u0026#34;\u0026gt;1\u0026lt;/a\u0026gt;\u0026lt;/sup\u0026gt; The blue line is the push-pull logic of the live streaming. The yellow line is the bussiness logic. Push streaming (End) # Push streaming means the process of sending the live streaming data(collect audio and video data) to the streaming media server. In this process, the network demand is high, you should better use the stable network environment to push the streaming data. In this process, the common protocol is RTMP. And the data format is encoded video eg. h264, h265, encoded audio eg. aac. (The decode process is done in the playback end.)\nNormally, the CDN server is used to cache the live streaming as well as pull the streaming from the origin server if itself receives the large number of requests.\nOrigin server # The origin servers are comprised of the streaming media server and the bussiness server end. The streaming media server is responsible for the live streaming data, and the bussiness server end is responsible for the bussiness logic such as the danmaku, the chat or even the payment of the sales.\nPull streaming (End) # The pull streaming end is also called as the playback end. The devices like the smart phone, the tablet or the computer can pull the live streaming data from the CDN server. Pull streaming means the process of receiving the live streaming data from the server. The common protocol is RTMP, HLS, HDL(HTTP-FLV).\nThe common protocols # RTMP # Real Time Messaging Protocol, which is a TCP based protocol (by Adobe), so the data will be transferred in the TCP tunnel. The most video format is flv. The port is 1935.\nIt has some advantages:\nRTMP is a TCP based protocol, so it can use the TCP long connection to transfer the data, while the 1935 port may be blocked. It will not generate the files in the streaming media server. So it can be lower latency. HLS # HTTP Live Streaming, which is a protocol of Apple. It has a great compatibility in every platform. It is used to transfer the video segments ts. The streaming media server will use m3u8 index file to manage the video segments(ensure the order of the video segments). The media server will cache the video data into a ts file around 10 seconds. Then when the client requests the video data, the media server will send the newest ts file to the client. It has good mobile compatibility. Besides, the HLS is based on the HTTP protocol, so everytime it requests the video data, it will be a new HTTP request, which will increase the loadency. From above we can analyze the HLS cannot meet the requirement of the low latency. 2\nIt still has some advantages:\nHLS is based on the HTTP protocol, so it can easily bypass the firewall. The player uses the ts file, so the client can change the bit rate easily. HTTP-FLV / HDL # HTTP-FLV is a lightweight protocol for live streaming. It is used to transfer the video eg. flv in http, and it is based on the HTTP protocol with port 80. The advantage is that the header information is simple still the RTMP has a complex handshake protocol. So it will be little faster than the RTMP.\nMPEG-DASH # Moving Picture Experts Group-Dynamic Adaptive Streaming Over HTTP is the first international streaming media protocol based on HTTP. Its unique value is that it can almost universally play videos, supporting formats including H.264, H.265, VP8/9, and AV1.\nWebsocket # Normally, the websocket protocol is not used in the streaming media, it is often used in the bussiness end for communication and interation. The interation between the host and the audience is jit. So the http cannot takes the role due to the fact that it is stateless and connectionless. Generally, the websocket is used to achieve the goal.\nPractical situation # Normal, the live streaming system will be:\nThe host pushes the streaming via RTMP. The client will pull the streaming via HTTP-FLV or HLS. References # The Details of the Stress Testing Guarantee Technology behind Livestreaming during Double 11\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHTTP Live Streaming\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":3,"href":"/posts/implement-danmaku-rendering-algorithm-from-scratch/","title":"Implement danmaku rendering algorithm from scratch","section":"Blog","content":"This article presents a comprehensive implementation of a danmaku rendering algorithm from the ground up, along with a thorough analysis of the danmaku rendering algorithm. The source code is available on GitHub.\nThe component of the danmaku rendering # Normally, a danmaku rendering image is comprised of the following components:\nSuperchat message Gift message(include premium member joining info) Bottom danmakus Rolling danmakus The components of the danmaku rendering So how to implement the danmaku rendering algorithm from scratch? We should know the main format of the danmaku file.\nXML Danmaku File Structure # First, we can analyze the structure of XML danmaku file.\n\u0026lt;?xml version=\u0026#39;1.0\u0026#39; encoding=\u0026#39;utf-8\u0026#39;?\u0026gt; \u0026lt;i\u0026gt; \u0026lt;chatserver\u0026gt;chat.bilibili.com\u0026lt;/chatserver\u0026gt; \u0026lt;chatid\u0026gt;0\u0026lt;/chatid\u0026gt; \u0026lt;mission\u0026gt;0\u0026lt;/mission\u0026gt; \u0026lt;maxlimit\u0026gt;0\u0026lt;/maxlimit\u0026gt; \u0026lt;state\u0026gt;0\u0026lt;/state\u0026gt; \u0026lt;real_name\u0026gt;0\u0026lt;/real_name\u0026gt; \u0026lt;source\u0026gt;e-r\u0026lt;/source\u0026gt; \u0026lt;metadata\u0026gt; \u0026lt;user_name\u0026gt;user_name\u0026lt;/user_name\u0026gt; \u0026lt;room_id\u0026gt;room_id\u0026lt;/room_id\u0026gt; \u0026lt;room_title\u0026gt;room_title\u0026lt;/room_title\u0026gt; \u0026lt;area\u0026gt;area\u0026lt;/area\u0026gt; \u0026lt;parent_area\u0026gt;parent_area\u0026lt;/parent_area\u0026gt; \u0026lt;live_start_time\u0026gt;2024-12-01T18:03:59+08:00\u0026lt;/live_start_time\u0026gt; \u0026lt;record_start_time\u0026gt;2024-12-01T18:05:02+08:00\u0026lt;/record_start_time\u0026gt; \u0026lt;/metadata\u0026gt; \u0026lt;d p=\u0026#34;0.000,1,25,5816798,1733047466414,0,73c9f86f,-1189105972\u0026#34; uid=\u0026#34;0\u0026#34; user=\u0026#34;X***\u0026#34;\u0026gt;?\u0026lt;/d\u0026gt; \u0026lt;d p=\u0026#34;0.000,1,25,5816798,1733047471983,0,73c9f86f,-1054085047\u0026#34; uid=\u0026#34;0\u0026#34; user=\u0026#34;X***\u0026#34;\u0026gt;good\u0026lt;/d\u0026gt; \u0026lt;/i\u0026gt; The XML file is comprised of main 2 elements:\nmetadata: contains the information of the live room or video d: contains the danmaku information danmaku info # The general of element is as follows:\n\u0026lt;d p=\u0026#34; 0.000, 1, 25,5816798,1733047466414, 0, 73c9f86f,-1189105972\u0026#34; uid=\u0026#34;0\u0026#34; user=\u0026#34;X***\u0026#34;\u0026gt;?\u0026lt;/d\u0026gt; \u0026lt;d p=\u0026#34;{time},{type},{size},{color},{timestamp},{pool},{uid_crc32},{row_id}\u0026#34; uid=\u0026#34;{uid}\u0026#34; user=\u0026#34;{user}\u0026#34;\u0026gt;{text}\u0026lt;/d\u0026gt; time: the time of the danmaku show up type: the type of the danmaku size: the size of the danmaku, 12 tiny，16 very small, 18 small, 25 middle, 36 large, 45 very large, 64 huge color: the decimal RGB color of the danmaku, eg hexadecimal: #FFFFFF -\u0026gt; decimal: 16777215 timestamp: the timestamp of the danmaku pool: the danmaku pool type uid_crc32: the crc32 hash of the danmaku sender\u0026rsquo;s uid, designed for ignore specific user\u0026rsquo;s danmaku row_id: the row id of the danmaku, designed for the history danmaku The relationship of type and pool:\npool\\type 1 4 5 6 7 9 0 roll bottom top reverse special1 / 1 / / / / precise2 / 2 / / / / / bas ASS File Structure # SSA # The SSA is the abbreviation of Sub Station Alpha, which is a subtitle format used in many video players. It can implement more complex subtitle effects than SRT.\nASS # The ASS is the abbreviation of Advanced SubStation Alpha, which is the V4 version of SSA.\nThe basic structure of ASS file3 is as follows:\n[Script Info] ScriptType: v4.00+ Collisions: Normal PlayResX: 720 PlayResY: 1280 Timer: 100.0000 WrapStyle: 2 ScaledBorderAndShadow: yes [V4+ Styles] Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding Style: R2L,Microsoft YaHei,38,\u0026amp;H4BFFFFFF,\u0026amp;H00FFFFFF,\u0026amp;H00000000,\u0026amp;H1E6A5149,0,0,0,0,100.00,100.00,0.00,0.00,1,0.0,1.0,8,0,0,0,1 Style: L2R,Microsoft YaHei,38,\u0026amp;H4BFFFFFF,\u0026amp;H00FFFFFF,\u0026amp;H00000000,\u0026amp;H1E6A5149,0,0,0,0,100.00,100.00,0.00,0.00,1,0.0,1.0,8,0,0,0,1 Style: TOP,Microsoft YaHei,38,\u0026amp;H4BFFFFFF,\u0026amp;H00FFFFFF,\u0026amp;H00000000,\u0026amp;H1E6A5149,0,0,0,0,100.00,100.00,0.00,0.00,1,0.0,1.0,8,0,0,0,1 Style: BTM,Microsoft YaHei,38,\u0026amp;H4BFFFFFF,\u0026amp;H00FFFFFF,\u0026amp;H00000000,\u0026amp;H1E6A5149,0,0,0,0,100.00,100.00,0.00,0.00,1,0.0,1.0,8,0,0,0,1 Style: SP,Microsoft YaHei,38,\u0026amp;H00FFFFFF,\u0026amp;H00FFFFFF,\u0026amp;H00000000,\u0026amp;H1E6A5149,0,0,0,0,100.00,100.00,0.00,0.00,1,0.0,1.0,7,0,0,0,1 Style: message_box,Microsoft YaHei,28,\u0026amp;H00FFFFFF,\u0026amp;H00FFFFFF,\u0026amp;H00000000,\u0026amp;H1E6A5149,0,0,0,0,100.00,100.00,0.00,0.00,1,0.0,0.7,7,0,0,0,1 [Events] Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text Dialogue: 0,0:00:00.00,0:00:12.00,R2L,,0000,0000,0000,,{\\move(735,1,-15,1)}{\\c\u0026amp;HDEC158}? Dialogue: 0,0:00:00.00,0:00:12.00,R2L,,0000,0000,0000,,{\\move(751,39,-31,39)}{\\c\u0026amp;HDEC158}good From above we can find the ASS file is comprised of 3 parts:\n[Script Info] [V4+ Styles] [Events] Script Info # [Script Info] ScriptType: v4.00+ # The ass is the v4 version of the ssa Collisions: Normal # The collisions type PlayResX: 720 # The X resolution of the video PlayResY: 1280 # The Y resolution of the video Timer: 100.0000 # This is a percentage, above 100 means the danmaku will display faster and faster. WrapStyle: 2 # Whether to change lines, due to the bilibili restrictions, this value is always 2. ScaledBorderAndShadow: yes # Whether to scale the border and shadow of the video with the resolution In the [Script Info] part, the key and changeable parameters are:\nPlayResX: the X resolution of the video, default is 1920 PlayResY: the Y resolution of the video, default is 1080 V4+ Styles # This part is some predefined styles for the [Events] part.\n[V4+ Styles] Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding Style: R2L,Microsoft YaHei,38,\u0026amp;H4BFFFFFF,\u0026amp;H00FFFFFF,\u0026amp;H00000000,\u0026amp;H1E6A5149,0,0,0,0,100.00,100.00,0.00,0.00,1,0.0,1.0,8,0,0,0,1 Style: L2R,Microsoft YaHei,38,\u0026amp;H4BFFFFFF,\u0026amp;H00FFFFFF,\u0026amp;H00000000,\u0026amp;H1E6A5149,0,0,0,0,100.00,100.00,0.00,0.00,1,0.0,1.0,8,0,0,0,1 Style: TOP,Microsoft YaHei,38,\u0026amp;H4BFFFFFF,\u0026amp;H00FFFFFF,\u0026amp;H00000000,\u0026amp;H1E6A5149,0,0,0,0,100.00,100.00,0.00,0.00,1,0.0,1.0,8,0,0,0,1 Style: BTM,Microsoft YaHei,38,\u0026amp;H4BFFFFFF,\u0026amp;H00FFFFFF,\u0026amp;H00000000,\u0026amp;H1E6A5149,0,0,0,0,100.00,100.00,0.00,0.00,1,0.0,1.0,8,0,0,0,1 Style: SP,Microsoft YaHei,38,\u0026amp;H00FFFFFF,\u0026amp;H00FFFFFF,\u0026amp;H00000000,\u0026amp;H1E6A5149,0,0,0,0,100.00,100.00,0.00,0.00,1,0.0,1.0,7,0,0,0,1 Style: message_box,Microsoft YaHei,28,\u0026amp;H00FFFFFF,\u0026amp;H00FFFFFF,\u0026amp;H00000000,\u0026amp;H1E6A5149,0,0,0,0,100.00,100.00,0.00,0.00,1,0.0,0.7,7,0,0,0,1 Most of the styles can be easily understood.\nIn the [V4+ Styles] part, the key and changeable parameters are:\nFontname: the font name, default is Microsoft YaHei Fontsize: the font size, default is 38 MarginL: the left margin, default is 0 MarginR: the right margin, default is 0 MarginV: the vertical margin, default is 0 Events # [Events] Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text Dialogue: 0,0:00:00.00,0:00:12.00, R2L, , 0000, 0000, 0000, ,{\\move(735,1,-15,1)}{\\c\u0026amp;HDEC158}? Dialogue: 0,0:00:00.00,0:00:12.00, R2L, , 0000, 0000, 0000, ,{\\move(751,39,-31,39)}{\\c\u0026amp;HDEC158}good In the [Events] part, the key and changeable parameters are:\nLayer: the layer of the danmaku Start: the start time of the danmaku End: the end time of the danmaku Style: the style of the danmaku, Name, MarginL, MarginR, MarginV, Effect: always empty About the Layer parameter:\nThe R2L danmaku and Superchat danmaku are in the layer 0. The BTM danmaku and gift danmaku are in the layer 1. About the Start and End time:\nBy default the scroll time is 12 seconds. The fix time of BTM danmaku is 5 seconds. About the Style parameter:\nR2L: right to left, which is corresponding to the 1 danmaku type in the XML file. BTM: bottom to top, which is corresponding to the 4 danmaku type in the XML file. message_box: message box, which is corresponding to the \u0026lt;sc\u0026gt; or \u0026lt;gift\u0026gt; in the XML file. Convert # From above analysis, we should convert the danmaku from XML to ASS.\nSo next I will analyze the conversion in terms of specific danmaku.\nR2L and BTM danmaku(Normal danmaku) # The xml file is as follows:\n\u0026lt;d p=\u0026#34; 0.000, 1, 25,5816798,1733047466414, 0, 73c9f86f,-1189105972\u0026#34; uid=\u0026#34;0\u0026#34; user=\u0026#34;X***\u0026#34;\u0026gt;?\u0026lt;/d\u0026gt; \u0026lt;d p=\u0026#34;837.163, 4, 25,5816798,1732882824163, 0, f201ec3c,51587109\u0026#34; uid=\u0026#34;0\u0026#34; user=\u0026#34;S***\u0026#34;\u0026gt;what？\u0026lt;/d\u0026gt; \u0026lt;d p=\u0026#34;{time},{type},{size},{color},{timestamp},{pool},{uid_crc32},{row_id}\u0026#34; uid=\u0026#34;{uid}\u0026#34; user=\u0026#34;{user}\u0026#34;\u0026gt;{text}\u0026lt;/d\u0026gt; The ass file is as follows:\nFormat: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text Dialogue: 0,0:00:00.00,0:00:12.00, R2L, , 0000, 0000, 0000, ,{\\move(735,1,-15,1)}{\\c\u0026amp;HDEC158}? Dialogue: 1,0:13:57.16,0:14:02.16, BTM, , 0000, 0000, 0000, ,{\\pos(960,1043)}{\\c\u0026amp;HDEC158}what？ For the normal R2L and BTM danmaku, the Layer is 0. Start time is the time in the XML file. End time is the time in the XML file plus 12 seconds(which can be changed). Style is R2L or BTM. Name is empty. MarginL is 0000. MarginR is 0000. MarginV is 0000. Effect is empty.\nText is the text in the XML file. It contains three parts:\n{\\move(735,1,-15,1)}: the Move Effect for the R2L danmaku, which is the position of the danmaku. The first and second number inside is the initial position(x, y) of the danmaku, and the third and fourth number is the final position(x, y) of the danmaku. The y should be the same. The algorithm I will analyze in the next section. {\\pos(960,1043)}: the Pos Effect for the BTM danmaku, which is the position of the danmaku. The number inside is the position(x, y) of the danmaku. The algorithm I will analyze in the next section. {\\c\u0026amp;HDEC158}: the color of the danmaku, which can be converted from the decimal color in the XML file. text: the text of the danmaku. Move Effect # The move effect is the most important part of the R2L danmaku.\nX Position movement # The basic algorithm is from the initial position and the final position of the danmaku. We should keep the y position the same. Let\u0026rsquo;s talk about the x position first.\nInitial position: (resolution.x + text_length/2, PositionY) Final position: (-1 * text_length / 2, PositionY) So how to get the text_length?\nWe should count cnt the every character\u0026rsquo;s length in UTF-8 encoding. So we use the 0xC0 to judge whether the current byte is the start of a multi-byte character, because 110- ---- is 0xC0, and 0x00 to 0x7F is the ASCII character, which is a single-byte character. Therefore, the second judgment is that it must be less than 0x80. If it is not UTF-8 encoding, we here directly use the strlen function.\nSo the text_length can be approximated by the following formula:\ntext_length = cnt * int((fontSizeSet + (fontSizeInXml - 25)) / 1.2);\nThe fontSizeInXml which is in the XML file is the size of the danmaku. So in the simple version, we can just use the formula below:\ntext_length = cnt * int((fontSizeSet) / 1.2);\nY Position movement # So how should we arrange the position of y? We should ensure that the next danmaku would not be overlapped with the previous one.\nI think we can return to the problem of the distance between the current danmaku and the previous one. To avoid the situation of catching up, we only need to ensure that the next danmaku in the same track(eg. n) cannot catch up with the previous one(eg. n-1), thus will ensure it cannot catch up with the n-2 indirectly. If it can catch up, we should compare the next danmaku data in the next track.\nFirst, the total distance of each danmaku is fixed, which is text_length + resolution.X. Then the scrolling time is fixed, I think the default time is 12s, so we can calculate the speed of each danmaku V_i, and each track only needs to store the start time of the last danmaku start_time and the length of the danmaku length.\nThen we need to determine whether the next danmaku B can catch up with the previous danmaku A, first, we need to calculate the distance difference Delta_X. That is, Delta_X = (start_time_B - start_time_A) × V_a - text_length_A / 2 - text_length_B / 2. Of course, for aesthetic reasons, a certain margin distance can be reserved. Then, we judge whether this distance can be covered within the remaining time. The speed difference is Delta_V = V_B - V_A, and the approximate time required for catching up can be calculated as Delta_T = Delta_X / Delta_V. We just need to ensure that Start_time_B - Start_time_A \u0026gt; Delta_T to ensure that the two danmakus will not overlap.\nSo, in the end, actually only one array and one formula are needed. Of course, in the case of a large number of danmakus, overlapping may still occur. Therefore, a fallback strategy is also required. This fallback strategy is quite simple. We just need to add a flag during each comparison to save the track number with the largest negative time difference. In this way, even if there is a catch-up situation, the danmakus will meet at the far right of the danmaku area. In the case of a large number of danmakus, the viewing experience can still be guaranteed. This method can theoretically accommodate more non-overlapping danmakus in extreme situations.\ndef get_position_y(font_size, appear_time, text_length, resolution_x, roll_time, array): velocity = (text_length + resolution_x) / roll_time best_row = 0 best_bias = float(\u0026#34;-inf\u0026#34;) for i in range(array.rows): previous_appear_time = array.get_time(i) if previous_appear_time \u0026lt; 0: array.set_time_length(i, appear_time, text_length) return 1 + i * font_size previous_length = array.get_length(i) previous_velocity = (previous_length + resolution_x) / roll_time delta_velocity = velocity - previous_velocity # abs_velocity = abs(delta_velocity) # The initial difference length delta_x = (appear_time - previous_appear_time) * previous_velocity - ( previous_length + text_length ) / 2 # If the initial difference length is negative, which means overlapped. Skip. if delta_x \u0026lt; 0: continue if delta_velocity \u0026lt;= 0: array.set_time_length(i, appear_time, text_length) return 1 + i * font_size delta_time = delta_x / delta_velocity bias = appear_time - previous_appear_time - delta_time if bias \u0026gt; 0: array.set_time_length(i, appear_time, text_length) return 1 + i * font_size else: if bias \u0026gt; best_bias: best_bias = bias best_row = i return 1 + best_row * font_size Pos Effect # The pos effect is the most important part of the BTM danmaku. Due to the display of every danmaku is around 5 seconds, so we should make sure this single danmaku would not be overlapped in this period. And if there are other danmaku in this period, we should move the new generated danmaku up. So our core algorithm is to get the PositionY of the danmaku is as belows:\n# Bottom danmaku algorithm def get_fixed_y(font_size, appear_time, resolution_y, array): best_row = 0 best_bias = -1 # record the best bias for i in range(array.rows): previous_appear_time = array.get_time(i) if previous_appear_time \u0026lt; 0: array.set_time_length(i, appear_time, 0) return resolution_y - font_size * (i + 1) + 1 # return the bottom line of the screen. else: delta_time = appear_time - previous_appear_time # if the time gap is larger than 5 seconds(the previous danmaku display time is over), if delta_time \u0026gt; 5: # we can move the new danmaku to this line. array.set_time_length(i, appear_time, 0) return resolution_y - font_size * (i + 1) + 1 else: # if the time gap is less than 5 seconds, we can record this bias and row num, # then continue to the next epoch. if delta_time \u0026gt; best_bias: best_bias = delta_time best_row = i return resolution_y - font_size * (best_row + 1) + 1 # return the best line in the screen. Superchat danmaku # File structure # The super chat xml format is as follows:\n\u0026lt;sc ts=\u0026#34;50.000\u0026#34; uid=\u0026#34;the_user_id\u0026#34; user=\u0026#34;the_user_name\u0026#34; price=\u0026#34;30\u0026#34; time=\u0026#34;60\u0026#34;\u0026gt;This is a superchat\u0026lt;/sc\u0026gt; ts: The superchat appears time. uid: The user id. user: The user name. price: The price of the superchat. time: The display time of the superchat. Regarding the specific rules on Bilibili, super chats with different prices will have different display durations and word limitations. The details are as follows4:\nprice(¥) price range display duration chinese characters limit 30 30≤ custom price ＜50 60s 40 50 50≤ custom price ＜100 2min 50 100 100≤ custom price ＜500 5min 60 500 500≤ custom price ＜1000 30min 80 1000 1000≤ custom price ＜2000 1h 90 2000 custom price ≥2000 2h 100 The ass file is as follows:\nDialogue: 0,0:00:59.20,0:01:10.00,message_box,,0000,0000,0000,,{\\pos(20,826.0)\\c\u0026amp;HFFF5ED\\p1\\bord0\\shad0}m 0 19 b 0 9.5 9.5 0 19 0 l 481 0 b 490.5 0 500 9.5 500 19 l 500 78 l 0 78 Dialogue: 0,0:00:59.20,0:01:10.00,message_box,,0000,0000,0000,,{\\pos(20,904.0)\\p1\\c\u0026amp;HB2602A\\bord0\\shad0}m 0 0 l 500 0 l 500 29 b 500 38.5 490.5 48 481 48 l 19 48 b 9.5 48 0 38.5 0 29 Dialogue: 1,0:00:59.20,0:01:10.00,message_box,,0000,0000,0000,,{\\pos(20,832.0)\\c\u0026amp;H653617\\b1\\bord0\\shad0}The user name Dialogue: 1,0:00:59.20,0:01:10.00,message_box,,0000,0000,0000,,{\\pos(20,870.0)\\c\u0026amp;H313131\\fs30\\bord0\\shad0}SuperChat CNY 30 Dialogue: 1,0:00:59.20,0:01:10.00,message_box,,0000,0000,0000,,{\\pos(20,904.0)\\c\u0026amp;HFFFFFF\\bord0\\shad0}The display time of the superchat. Every superchat danmaku message box is comprised of 5 parts:\nthe upper box the lower box the user name the price the superchat text Postion Arrangement Algorithm # For the upper box and lower box paramaters, I will not show them here, if you are interested, you can refer to the function draw_lower_box and draw_upper_box in my repository for more details, and many drawing parameters are referenced from the DanmakuFactory5, very grateful to the author. Here I would like to talk about how to make the superchat message box move according to other superchats appear and disappear.\nAs we can see, the superchat show up and disappear in chronological order, so how can we define the superchat position in a specific timespot?\nMy solution is: calculate the position of the superchat message box every time the superchat changes its position in its life cycle. The concrete algorithm is as follows:\ndef render_superchat(ass_file, sc_font_size, resolution_y, data): \u0026#34;\u0026#34;\u0026#34; Render superchat events to the ass file. Args: ass_file (str): The path to the ass file. sc_font_size (int): The superchat font size, which is used to calculate some render parameters. resolution_y (int): The resolution y, which is used to calculate the initial y coordinate. data (list): The data to render, which is a list of superchat events. \u0026#34;\u0026#34;\u0026#34; # get all events events = [] for i, ( start, end, sc_height, user_name, price, text, btm_box_height, process_record, ) in enumerate(data): events.append((start, \u0026#34;start\u0026#34;, i)) events.append((end, \u0026#34;end\u0026#34;, i)) # sort events by time events.sort() # still alive active = [] # process each event for time, event_type, index in events: current_start = data[index][0] current_end = data[index][1] # if it is a start event, new superchat appears if event_type == \u0026#34;start\u0026#34;: for active_index in active: active_start = data[active_index][0] active_end = data[active_index][1] # if the current superchat appears in the duration of the active superchat if active_start \u0026lt;= current_start \u0026lt; active_end: # then it will record the height change of the active superchat data[active_index][7] += f\u0026#34;-{data[index][2]}@{time} \u0026#34; active.append(index) else: # the superchat will disappear, so remove it from the active list first active.remove(index) # then check if the current superchat appears in the duration of the active superchat for active_index in active: active_start = data[active_index][0] active_end = data[active_index][1] if active_start \u0026lt;= current_start \u0026lt; active_end and time \u0026lt; active_end: # if the current superchat disappears in the duration of the active superchat # then record the height change. data[active_index][7] += f\u0026#34;+{data[index][2]}@{time} \u0026#34; # then parse the result, and write the superchat to the ass file according to the result for i, ( start, end, sc_height, user_name, price, text, btm_box_height, result, ) in enumerate(data): # print(f\u0026#34;\\nSC {i} ({start}-{end}):\u0026#34;) # Initial y coordinate previous_y = resolution_y - sc_font_size * 2 current_y = previous_y - sc_height current_time = start # print(f\u0026#34;Time {start}: y = {current_y}, previous_y = {previous_y}\u0026#34;) # if the position has changed if result: changes = result.strip().split() for change in changes: delta_y, time = change.split(\u0026#34;@\u0026#34;) prev_time = current_time current_time = float(time) # the shift height height_change = float(delta_y[1:]) SuperChat( prev_time, current_time, user_name, price, btm_box_height, current_y, previous_y, text, sc_font_size, ).write_superchat(ass_file) previous_y = current_y if delta_y[0] == \u0026#34;-\u0026#34;: current_y -= height_change else: current_y += height_change # print(f\u0026#34;Time {time}: y = {current_y}, previous_y = {previous_y}\u0026#34;) prev_time = current_time current_time = end SuperChat( prev_time, current_time, user_name, price, btm_box_height, current_y, previous_y, text, sc_font_size, ).write_superchat(ass_file) Here I uncommented the print test code, so you can see the parsed result of the sample.xml.\nSC 0 (10.0-70.0): Time 10.0: y = 1078, previous_y = 1204 Time 50.0: y = 952.0, previous_y = 1078 Time 59.0: y = 826.0, previous_y = 952.0 SC 1 (50.0-110.0): Time 50.0: y = 1078, previous_y = 1204 Time 59.0: y = 952.0, previous_y = 1078 SC 2 (59.0-119.0): Time 59.0: y = 1078, previous_y = 1204 SC 3 (185.0-245.0): Time 185.0: y = 1040, previous_y = 1204 Time 217.0: y = 914.0, previous_y = 1040 SC 4 (217.0-337.0): Time 217.0: y = 1078, previous_y = 1204 Time 269.0: y = 914.0, previous_y = 1078 Time 303.0: y = 712.0, previous_y = 914.0 Time 329.0: y = 876.0, previous_y = 712.0 SC 5 (269.0-329.0): Time 269.0: y = 1040, previous_y = 1204 Time 303.0: y = 838.0, previous_y = 1040 SC 6 (303.0-363.0): Time 303.0: y = 1002, previous_y = 1204 Then we have the danmaku position of every timespot, so we can render the super chat in the ass file easily.\nBubble Movement Effect Algorithm # But, are these the best display? I think not. If we only consider the position, the video will be too boring, so we should add some animation to the superchat appearance. Think about the superchat like a bubble, it will move up and down according to other superchat appearance and disappearance. The picture will be more suitable for the audience.\nSo now we can think about the algorithm of the bubble movement effect. As we talked above, this movement can be implemented by the move effect.\nFirst of all, the new superchat will appear in the bottom of the screen, so the initial y position is the resolution_y - font_size * 2 (We should reserve some space for the gift danmaku, I will introduce them in the next section). And the show up position will be the initial y + the message box height.\nThen, how will the new superchat effect other still alive superchat? At the timespot, others will move up the new superchat height as well. So the move parameter will be like from current_y to current_y - new_superchat_height.\nAnd if there is a specific superchat disappear, the superchat eariler than it will move down the superchat height. So the move parameter will be like from current_y to current_y + disappear_superchat_height. Then everything makes sense.\nThen we should think about, when should we start the movement? There will be many solutions, but I think add it at the change position time of the superchat is the best choice. And make the movement a specific duration, like 0.2 second.\nSo the process will be:\nchange position time + 0.2s: make the superchat move effect. change position time + 0.2s ~ next change position time: make the superchat pos effect. You can check the superchat.py for more details.\nGift and guard danmaku # File structure # The xml file is as follows:\n\u0026lt;!--\u0026gt;gift danmaku\u0026lt;--\u0026gt; \u0026lt;gift ts=\u0026#34;11.00\u0026#34; uid=\u0026#34;0\u0026#34; user=\u0026#34;xxx\u0026#34; giftname=\u0026#34;情书\u0026#34; giftcount=\u0026#34;1\u0026#34; cointype=\u0026#34;金瓜子\u0026#34; price=\u0026#34;5200\u0026#34;/\u0026gt; \u0026lt;gift ts=\u0026#34;13.00\u0026#34; uid=\u0026#34;0\u0026#34; user=\u0026#34;yyy\u0026#34; giftname=\u0026#34;小花花\u0026#34; giftcount=\u0026#34;1\u0026#34; cointype=\u0026#34;金瓜子\u0026#34; price=\u0026#34;100\u0026#34;/\u0026gt; \u0026lt;gift ts=\u0026#34;{time}\u0026#34; uid=\u0026#34;{uid}\u0026#34; user=\u0026#34;{username}\u0026#34; giftname=\u0026#34;{giftname}\u0026#34; giftcount=\u0026#34;{count}\u0026#34; cointype=\u0026#34;金瓜子\u0026#34; price=\u0026#34;{price}\u0026#34;/\u0026gt; \u0026lt;!--\u0026gt;all guard danmaku\u0026lt;--\u0026gt; \u0026lt;guard ts=\u0026#34;18.000\u0026#34; uid=\u0026#34;873268\u0026#34; user=\u0026#34;xxx\u0026#34; giftname=\u0026#34;舰长\u0026#34; count=\u0026#34;1\u0026#34; price=\u0026#34;198000\u0026#34; level=\u0026#34;3\u0026#34;/\u0026gt; \u0026lt;guard ts=\u0026#34;18.000\u0026#34; uid=\u0026#34;873268\u0026#34; user=\u0026#34;yyy\u0026#34; giftname=\u0026#34;提督\u0026#34; count=\u0026#34;1\u0026#34; price=\u0026#34;1998000\u0026#34; level=\u0026#34;2\u0026#34;/\u0026gt; \u0026lt;guard ts=\u0026#34;18.000\u0026#34; uid=\u0026#34;873268\u0026#34; user=\u0026#34;zzz\u0026#34; giftname=\u0026#34;总督\u0026#34; count=\u0026#34;1\u0026#34; price=\u0026#34;19998000\u0026#34; level=\u0026#34;1 \u0026#34;/\u0026gt; \u0026lt;guard ts=\u0026#34;{time}\u0026#34; uid=\u0026#34;{uid}\u0026#34; user=\u0026#34;{username}\u0026#34; giftname=\u0026#34;{giftname}\u0026#34; count=\u0026#34;{count}\u0026#34; price=\u0026#34;{price}\u0026#34; level=\u0026#34;{level}\u0026#34;/\u0026gt; The ass file is as follows:\nFormat: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text Dialogue: 0,0:00:00.20,0:00:02.00,message_box, , 0000, 0000, 0000,,{\\pos(0,1242)}{\\c\u0026amp;H1C7795\\b1}{username}:{\\c\u0026amp;H1C7795\\b0} 粉丝团灯牌 x1 Set Layer to 0, Start to time in xml, End time + duration time, Style to message_box, Name to empty, MarginL to 0000, MarginR to 0000, MarginV to 0000, Effect to \\pos or \\move, Text to the gift message. \\move and \\pos are same as above. b1 and b0 are the start and end of the font bold, \u0026amp;H1C7795 is the font color.\nGift and Guard Danmaku Display Algorithm # Gift and guard danmaku will be displayed in a box range, the height of the box is two font sizes. Gift and guard danmaku will scroll from bottom to top, and the earliest danmaku will be removed when the number of danmaku exceeds the limit.\nFirst, parse the xml, find the part that contains \u0026ldquo;gift\u0026rdquo; and \u0026ldquo;guard\u0026rdquo;, and store the information in gift_list. Then, calculate the display time and position of the gift and output the ass file according to these information.\nThe key points to note are:\nMerge adjacent same danmaku\nSort all gift by appearance time in ascending order. Then merge the adjacent same danmaku in gift_list. The definition of adjacent same danmaku is: the same user, the same gift, and the adjacent time is not more than mrege_interval(default 5s, configurable). The start time of the merged danmaku is the start time of the earliest danmaku, and the end time is the end time of the latest danmaku. Handle different danmaku with the same time\nSince the start time of danmaku from different users may be the same, which will cause display conflicts, so we need to adjust the time of the danmaku to avoid conflicts. Traverse all danmaku in chronological order. If the start time of a gift danmaku is the same as or earlier than the previous one, then delay the start time of the danmaku and the end time correspondingly. Delay until the maximum number of danmaku is reached. Note that: since the minimum time interval of adjacent gift danmaku is 1s, so the number of delayed danmaku multiplied by the delay time cannot exceed 1s. Handle the display time and position of gift danmaku\nWhen outputting, the display time and position of gift danmaku need to be adjusted according to the end time of \\pos, but if there is a new danmaku joining in the final end time, then \\pos needs to end earlier. To achieve this adjustment, we check whether there is a danmaku with a start time earlier than the end time of the current gift danmaku in the final end time. If there is, then we will advance the end time of the current gift danmaku to the start time of the next danmaku. Use an active danmaku list active_danmaku_list to record the current danmaku that is being displayed. The length of the list should not exceed the maximum number of danmaku that is set. When the list length exceeds the limit, the earliest danmaku should be deleted. Danmaku display and movement transition\nDanmaku movement transition: leave a transition time before each \\pos(), for smooth transition from the previous position to the current \\pos() position. This transition time can be configured. For danmaku that exceeds the display range, add a mask \\clip() when it is removed, so that the danmaku only displays the content within the mask. The height of the mask should be equal to the display box height, or twice the font size. The result display # Reference # special means: [{x1(0-1)|(px)},{y1},\u0026quot;{Aplha0(0-1)}-{Alpha1}\u0026quot;,{Lifetime},\u0026quot;{Text}\u0026quot;,{Z_Rotation},{Y_Rotation},{x2},{y2},{Move_Time(ms)},{Delay_Time(ms)},{Outline[01]},\u0026quot;{Fontname}\u0026quot;,{Linear_Speedup[01]},[\u0026quot;SVG_Path\u0026quot;]]\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nprecise means: [{x1(0-1)|(px)},{y1},\u0026quot;{Aplha0(0-1)}-{Alpha1}\u0026quot;,{Lifetime},\u0026quot;{Text}\u0026quot;,{Z_Rotation},{Y_Rotation},{x2},{y2},{Move_Time(ms)},{Delay_Time(ms)},{Outline[01]},\u0026quot;{Fontname}\u0026quot;,{Linear_Speedup(Bool)}]\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://web.archive.org/web/20210604141133/https://www.douban.com/note/658520175/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://live.bilibili.com/blackboard/live-superchat-intro-web.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/hihkm/DanmakuFactory\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":4,"href":"/posts/is-there-an-rss-renaissance-in-the-ai-era/","title":"Is There an Rss Renaissance in the AI Era?","section":"Blog","content":"RSS: RDF Site Summary or Really Simple Syndication。\n如果说 search 是信息 pull 的代表，那毫无疑问，RSS 就是信息 push 的代表。简单地讲，就是一种信息聚合的服务，通过一种标准化的消息来源的格式规范，聚合多个网站的内容更新并且通知订阅者。尽管听起来很复杂，但是 RSS 本质上只需要两步，从目标网站上找到 RSS 链接，并把它添加到自己内容收取的服务里。\n起源和发展 # RSS 在互联网早期非常流行，人们往往会订阅不同的网站列表或者博客，每天像取报纸一样能浏览到一整天的更新与消息。但是当时网站的主要收入还是靠广告，像之前我在 Thinking About Advertisement from an Open Source Perspective 中分析过的手机之家以及字节一样。但是渐渐网站发现 RSS 是一个反商业的模式，因此渐渐取消了 RSS 的订阅方式，当然大部分人可能会认为“反商业”意味着通过 RSS 的方式没有办法获得足够的广告点击率，从而没有广告费用的支撑。\n但是我认为这可能只是一部分原因，更深层次的原因在于无法形成商业闭环。这也是 “wifi 万能钥匙” 为什么选择在线阅读以及短剧等等业务的原因。在我小时候在那个 30M 流量卖到 5 块钱的年代，Wi-Fi 万能钥匙相当风靡，不对其业务原理进行评价，但是用户每次使用都形成了有效的广告曝光。现在网上经常有人讨论，“wifi 万能钥匙” 作为一个工具，为什么要 “不务正业“ 做成短视频，小说，新闻的大杂烩。这些人没有意识到，单纯作为工具类应用，没有商业闭环，依靠广告，点击率再高也无法走向更大的市场，因此依靠作为网络的入口，提供更多的网络娱乐服务是一个非常好的商业闭环，这一步是至关重要的。\n毕竟，对于互联网应用而言，没有“闭环“，哪来的“护城河”？\n还有一个原因，就是社交网络的兴起 —— Facebook 在 2006 年推出了 News Feed，核心就是社交 UGC，这股热风将互联网从泡沫中拉起，带向了一个新的高度。而 RSS 衰弱的标志性事件我认为是 2013 年 Google Reader 的关停。当时已经开始尝试算法推荐的张一鸣还写了一篇 《为佩奇关闭Google Reader的魄力叫好！》1 赞扬了Page 关停 RSS 阅读器的决心，在文章中他分析了能够持久使用 RSS 的人必须具备信息组织能力强且能够控制订阅源的数量，订阅近百个信息源，最终却来不及看，将信息获取变为负担，就背离了一开始的初心。\n改进 # RSS 最终走向衰落的原因就是因为在商业上无人获益，但是如果加上商业呢？很明显，公众号就是一个相对平衡的解决方案，不过公众号有时过于激进的商业化广告规模投放，一个页面仅由一条文章（其余折叠）和三四条推荐广告铺满，中间还加载着视频号的内容，从官感上来说也不是以用户为中心了。知乎也能算作一个 RSS 的整合了，不仅是每个创作者的推送，消息话题本身也能被订阅以及推送，尽管如此，商业化的成果仍然是见仁见智。\nAI 时代的复兴？ # 回想起之前听过的一场播客，founder 极力地分析，然后给产品加上宏大的叙事，讲述自己的研究，就是没有从根本上出发考虑，本质来看就是一款套着 AI 壳的 RSS 产品，产品的出发点和研究是很割裂的，因为本质上方向就有问题。在没有找到 PMF 之前，将故事编织地非常宏大，盲目地将某轮融的估值巨大，反而越大越危险，无论是对下一轮的融资亦或是并购都很难有正面影响。平台最终的意义就是社交，本质上就是以人为中心，以用户为中心的。按照这个产品的逻辑，将某个消息以人工方式或是热点方式，从多个平台上抓下来，让模型总结一遍重构消息，再推送给用户，没有模式的创新，没有场景的转变，为什么能被用户选择呢？按照今日头条的故事刻舟求剑是很刻板的（这让我想起前些天看到的某个创业团队的社交账号，其中北大女生在某平台上自豪宣布文科生的她勇敢是某专业唯一一个选择物理辅修的同学，只是因为在她认为很多企业家都是物理本科毕业，简介还写着复刻硅谷神话，我不禁哑然失笑。不是出于好奇或者兴趣，仅仅因为模仿而刻舟求剑，因果倒置，千年前的故事至今还在上演）。\n回归主题，应用的本质应该回归到用户，而信息只是一种载体。我并不反对套壳形式重做以前的东西，只是做出来的东西要真的能找到对应的 PMF，而不应该是拿着锤子找钉子的故事。简单的讲，真的所有的消息都适合只读梗概吗？真正意义上的优质信息不是意味着包罗万象，说的非常全面，而是在于逻辑清晰，分析深入，有很好的例子或者数据辅助理解。而只是模型总结一遍新闻，丢失了应该保留的逻辑，说一些模版化的废话，终究是行不通的。就这个场景来看，我认为即刻做的倒是很好的解决方案，同样是 RSS 消息类型，即刻抓住社交与互动的场景，让你和与你有相近的信息爱好的人有的聊，至少可以收取同一个信息的时候获得共鸣。\nRSS 在 AI 时代会重新复兴吗？\n最后，本站的 RSS 链接: https://blog.timerring.com/index.xml 欢迎添加阅读！\nhttps://m.techweb.com.cn/article/2013-03-14/1283033.shtml\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":5,"href":"/posts/a-shopping-experience-in-pangdonglai-supermarket/","title":"A Shopping Experience in Pang Donglai Supermarket","section":"Blog","content":"前天在广州找朋友时发现附近有一家永辉超市，它是去年年末自主改调的“胖东来”超市，我一下子来了兴致，之前虽然也去过河南很多城市，但是没有去过许昌和新乡(New York)，而胖东来又不在河南之外开店，所以一直缺少体验的机会。在胖东来仔细转了一遍后，我对胖东来的服务有了更进一步的理解和感受。\n转型 # 首先，普及一个我很早以前听到的概念，商场里最挣钱的部分是自制的部分。这并非是指某一种商品，而是统计后的结果。\n在近些年里，大型的综合商超基本上完成了从“渠道盈利模式”向“价值链盈利模式”的转型，过往的渠道盈利之中，商超作为商品售卖的主要渠道，主要通沟以下几种形式进行盈利：\nSlotting Fee: 商品进入商超货架的门槛费用； Display Fee: 黄金位置的额外收费； Promotion Fee: 节庆活动、海报的赞助费用； Rebate: 按销售额抽取一定比例的佣金； Accounting period: 利用应付账款账期（通常60-90天）获取资金沉淀收益。 作为渠道的方式，及时商品滞销，商超仍然有稳定的收益与现金流。轻资产的方式也更加容易复制推广。\n但是随着互联网+快递作为一种新型的组合疯狂增长，线下商超的渠道优势逐渐减弱，多数商品选择大力投放线上渠道，商超很难再向过往稳定地获取收益。对于商超而言，想要获得更高的毛利率只能转型，向供应链的深处进发。\n价值链盈利是通过整合或优化供应链各环节（生产、物流、销售）降低成本或创造溢价，包括以下步骤：\n商品差价: 通过直采、自有品牌等方式压缩中间成本； 效率红利: 库存周转提升、损耗降低带来的隐性收益； 增值服务: 会员费、数据服务等衍生收入。 全球商超连锁进入世界 500 强的企业有很多，但是针对的目标人群却并不一样，对于主要的美式家庭而言，一周去一次商场，采购一周所需要的物资，因此山姆，costco 里出售的都是大包装产品，并且由于美式家庭一般不吃鲜活，冰冻偏多，因此超市也往往不卖鲜活。但是对于国内家庭而言，超市离家近并且食材是为近两天准备的，因此生鲜以及生熟食部分的重要比例就提升很多了。此外自建供应链的品牌也占了非常重要的比例。\n而胖东来、盒马鲜生等就正是这种模式的代表。\n以人为本 # 我认为对于人来说，最重要的是做人，绝非努力或者聪明。同时，对于企业来说，最重要的是“以人为本”。\n这让我想到一个关于 app 非常有趣的现象，移动互联网时代，还在上小学以及初中的我，经常会用我当时的联想手机体验各种 app，根据我的观察，在那个 app 数量指数级增长的年代，从一个 app 的 menu 排版中就能发现窥探到 app 是否以人为本。某音乐软件堆积了一堆功能，最后把登录以及个人中心放在侧栏，导致每次登录或者检查个人相关状态需要点击两次步骤，极大增加了查找以及使用成本。还有某生活工具类型的 app，当时正直小额贷的窗口期，为了强推自己的衍生借贷产品，会将借贷页面放在较为关键的 menu 位置上，如果这个领域一旦存在竞争，那不遵从 “以人为本” 的 app 一定会被用户抛弃。“以人为本”应该让用户尽可能地减少复杂的操作同时满足或者延长用户的需求，如果本末倒置最终必然走向灭亡。\n当然，减少复杂的操作满足需求的核心还是真正的满足需求，最近我了解到国内一个融了 4 亿的创业团队做信息类型的 app，想照着上个十年今日头条的方式，刻舟求剑要打造 AI 时代的信息分发 app，founder 长篇大论说的很好听，但是我体验下来完全背离了用户的需求，我能断言它不会成为大众化，平台化的产物，结局大概率是在创业这场 “拉力赛” 中默默被淘汰，甚至都不会激起水花。\n现在经过多年的竞争，一个合格的 app 理应当是高度同质的，通常首页应该是信息流，用信息流尽可能延长用户的使用时间，其次，menu 中间的位置应该是这个 app 的核心功能，例如信息平台应该是发布内容，电商平台应该是物流以及消息，最右边的 botton 往往是个人中心的入口，如果遵从这个原则，app 基本上用户体验不会非常差。\n试吃商品从不间断 在胖东来的店里，能感受到非常强的“以人为本”的气息。对于重要的品类，往往会有大面积的试吃区域，不同于盒马鲜生的每小时定点增加，胖东来试吃是不间断的。根据我的观察，往往试吃的三四个人中就会有一个人选择购买，试吃对于培养用户习惯非常重要，就像游戏，一个从来没有完成过充值流程的人是绝无可能成为氪金玩家，也不会贡献更多的使用时长的。因此游戏公司往往会设置 6 元充值门槛，其实这个门槛几元并不重要，重要的是让用户完成一遍完整的充值流程，培养习惯。无独有偶，娱乐行业也是如此，很多平台的签约主播合同里必然会有一条，满足一定数量的最便宜的小礼物，这种 1 毛 1 块的礼物对于培养用户的消费习惯非常重要。有时重要程度远远大于巨额的礼物费用，因为它代表着广泛的潜在用户。\n此外，还有贴心的纸巾以及商品推荐，在体验服务和环境的作用下，据我观察即使是老人也会为一定的溢价买单。这是我在盒马等大型商超从来没有见过的。\n新鲜 # 我去的时间大约在 9 点多，超市里依然人潮汹涌，大家都在生熟食，蔬果柜台前挑选，价格确实在不断的下降，最低的已经打五折售卖了，要求是不卖隔夜的商品。而新鲜，恰恰是大多数顾客的核心需求。\n当日商品打折 打折卖并非首创的模式，但是在整片区域里全部打折能体现出对于”新鲜“的重视以及决心。这在一定程度上还是非常打动人的。同时也展现了供应链方面的实力。\n对比我附近的一家盒马鲜生，及时在打烊的前十分钟，店员也只是用推车收起生熟食以及蔬果，至于之后会怎么处理，完全未知。我很少买到甚至见到打折的商品，即使部分商品可能用于试吃或者免费送给会员，但是也仅占每天闭店前剩余货物的一小部分，大部分的商品究竟如何处理，我无从得知。\n认真 # 最早听说胖东来应该是有顾客和胖东来起了争执，然后胖东来两次调查，出具了数页调查报告。类似的事情还有很多，我甚至感觉每个月都有一件，胖东来每次以认真负责的态度，有时通过让小利，然后大力宣传一把，多次累计已经成为各大媒体平台的顶流。在 Reflections on Trending Topics 中我提到过，我每顿吃饭都有看公众号的习惯，以下是我在 36氪 中搜索有关胖东来的报道：\n几乎每个月都有几篇关于胖东来的报道 对于消费者来说，重要的事情还是事事有着落，件件有回应。胖东来的认真负责还是交出了一张不错的答卷。\n争议 # 去年在对于广告的思考文章 Thinking About Advertisement from an Open Source Perspective 中我就提到过胖东来：\n胖东来短时间内以极低的成本推送到了 10 亿网民的面前，其广告的价值远大于企业自身盈利能力的价值。\n胖东来的营销非常有意思，很难分辨是否有公关和广告团队在后面策划，有时候我认为他们的营销是非常顶级的，经常说一些大众常识性的话，给道德层面的行为增加福利例如假期或者补助，以一个非常低的成本，获得一次十亿级别的曝光。似乎每个月都有一次热门事件，和小米一起成为近两年热门的常客。但是宣传总是有正负两面。因为有些话题是不能碰的，很有可能会形成反向的舆论，例如前段时间的女权，房贷以及彩礼问题，自己无法解决的事情还是不要直接要求，毕竟对人有利的事情，多数是你好我好，底下一片吹捧和羡慕，但是一旦有对人不利的事情，可能再正面的人设都会被互联网骂的体无完肤。\n幸福 # 对于这一点我还是保持谨慎的态度，对于胖东来来说，能创造员工幸福的条件还是整体的营收好，才能给员工让出更多的福利。毕竟有钱说什么都是对的，但是市场商业模式并没有专利，供应链也可以在规模化中搭建，在一个竞争的市场中还是不要提前给自己造神。\n类似情况的无非是和过去几十年的 BBA，时过境迁，BBA 在新能源转型上折戟沉沙，市场份额急剧缩减，连带着大规模裁员，对于那些 BBA 员工来说，降薪或者裁员是否能保持幸福就见仁见智了。\n国内自媒体争相捧成神，结果很有可能”成也萧何败也萧何“，过度神话最后导致的绝对是一推即倒。要想在互联网长久生存，需要的还是品质和服务，至于对内员工的幸福等等方面的宣传，可以在盈利的状态下大方地做，但是最好不要大张旗鼓地说。\n当然我相信老板肯定懂这个道理，所以每次大张旗鼓地传宣可能是一些心知肚明的目的。\n总结 # 其实以上几点归根到底还是“以人为本”的体现，整体来说是一个非常好的商业闭环，让出利润，获得口碑，获得宣传，获得营收，再反哺利润，形成正向循环。当然对于商超来说，利润整体不高，但是对于整体的消费行业的促进有这不可磨灭的作用。\n"},{"id":6,"href":"/posts/langchain-and-rag-best-practices/","title":"Langchain and RAG Best Practices","section":"Blog","content":"This is a quick-start essay for LangChain and RAG which mainly refers to the Langchain chat with your data course which are taught by Harrison Chase and Andrew Ng.\nYou can check the entire code in the rag101 repository.\nLangChain and RAG best practices # Introduction # LangChain # LangChain is an Open-source developer framework for building LLM applications.\nIt components are as below:\nPrompt # Prompt Templates: used for generating model input. Output Parsers: implementations for processing generated results. Example Selectors: selecting appropriate input examples. Models # LLMs Chat Models Text Embedding Models Indexes # Document Loaders Text Splitters Vector Stores Retrievers Chains # Can be used as a building block for other chains. Provides over 20 types of application-specific chains. Agents # Supports 5 types of agents to help language models use external tools. Agent Toolkits: provides over 10 implementations, agents execute tasks through specific tools. RAG process # The whole RAG process lays on the Vector Store Loading and Retrieval-Augmented Generation.\nVector Store Loading # Load the data from different sources, split and convert them into vector embeddings.\nRetrieval-Augmented Generation # After the user\u0026rsquo;s input Query, the system will retrieve the most relevant document fragments (Relevant Splits) from the vector store. The retrieved relevant fragments will be combined into a Prompt, which will be passed along with the context to the large language model (LLM). Finally, the language model will generate an answer based on the retrieved fragments and return it to the user. Loaders # You can use loaders to deal with different kind and format of data.\nSome are public and some are proprietary. Some are structured and some are not.\nSome useful lib:\npdf: pypdf youtube audio: yt_dlp pydub web page: beautifulsoup4 For more loaders, you can check the official docs.\nYou can check the entire code here.\nPDF # Now, we can practice:\nFirst, install the lib:\npip install langchain-community pip install pypdf You can check the demo in the\nfrom langchain.document_loaders import PyPDFLoader # In fact, the langchain calls the pypdf lib to load the pdf file loader = PyPDFLoader(\u0026#34;ProbRandProc_Notes2004_JWBerkeley.pdf\u0026#34;) pages = loader.load() print(type(pages)) # \u0026lt;class \u0026#39;list\u0026#39;\u0026gt; print(len(pages)) # Print the total num of pages # Using the first page as an example page = pages[0] print(type(page)) # \u0026lt;class \u0026#39;langchain_core.documents.base.Document\u0026#39;\u0026gt; # What is inside the page: # 1. page_content # 2. meta_data: the description of the page print(page.page_content[0:500]) print(page.metadata) Web Base Loader # Also we install the lib first:\npip install beautifulsoup4 The WebBaseLoader is based on the beautifulsoup4 lib.\nfrom langchain_community.document_loaders import WebBaseLoader loader = WebBaseLoader(\u0026#34;https://zh.d2l.ai/\u0026#34;) pages = loader.load() print(pages[0].page_content[:500]) # You can also use json as the post processing # import json # convert_to_json = json.loads(pages[0].page_content) Splitters # Splitting Documents into smaller chunks. Retaining the meaningful relationships.\nWhy split? # The limitation of GPU: the GPT model with more than 1B parameters. The forward propagation cannot process such a large parameters. So the split is necessary. More efficient computation. Some fixed size of sequence. Better generalization. However, the split points may lose some information. So we split should consider the semantic.\nType of splitters # CharacterTextSplitter MarkdownHeaderTextSplitter TokenTextsplitter SentenceTransformersTokenTextSplitter RecursiveCharacterTextSplitter: Recursively tries to split by different characters to find one that works. Language: for CPP, Python, Ruby, Markdown etc NLTKTextSplitter: sentences using NLTK(Natural Language Tool Kit) SpacyTextSplitter: sentences using Spacy For more, check the docs.\nExample CharacterTextSplitter and RecursiveCharacterTextSplitter # You can check the entire code here.\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter example_text = \u0026#34;\u0026#34;\u0026#34;When writing documents, writers will use document structure to group content. \\ This can convey to the reader, which idea\u0026#39;s are related. For example, closely related ideas \\ are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n \\ Paragraphs are often delimited with a carriage return or two carriage returns. \\ Carriage returns are the \u0026#34;backslash n\u0026#34; you see embedded in this string. \\ Sentences have a period at the end, but also, have a space.\\ and words are separated by space.\u0026#34;\u0026#34;\u0026#34; c_splitter = CharacterTextSplitter( chunk_size=450, # the size of the chunk chunk_overlap=0, # the overlap of the chunk, which can be shared with the previous chunk separator = \u0026#39; \u0026#39; ) r_splitter = RecursiveCharacterTextSplitter( chunk_size=450, chunk_overlap=0, separators=[\u0026#34;\\n\\n\u0026#34;, \u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;, \u0026#34;\u0026#34;] # priority of the separators ) print(c_splitter.split_text(example_text)) # split at 450 characters print(r_splitter.split_text(example_text)) # split at first \\n\\n Vectorstores and Embeddings # Review the RAG process:\nBenefits:\nImprove the accuracy of the query. When query the similar chunks, the accuracy will be higher. Improve the efficiency of the query. Minimize the computation when query the similar chunks. Improve the coverage of the query. The chunks can cover every point of the document. Facilitate the Embeddings. Embeddings # If two sentences have similar meanings, then they will be closer in the high-dimensional semantic space.\nVector Stores # Store every chunk in a vector store. When customer query, the query will be embedded and then find the most similar vectors which means the index of these chunks, and then return the chunks.\nPractice # Embeddings # You can check the entire code here.\nFirst, install the lib:\nThe chromadb is a lightweight vector database.\npip install chromadb What we need is a good embedding model, you can select what you like. Refer to the docs.\nHere I use the ZhipuAIEmbeddings. So you should install the lib:\npip install zhipuai Here is the test code:\nfrom langchain_community.embeddings import ZhipuAIEmbeddings embed = ZhipuAIEmbeddings( model=\u0026#34;embedding-3\u0026#34;, api_key=\u0026#34;Entry your own api key\u0026#34; ) input_texts = [\u0026#34;This is a test query1.\u0026#34;, \u0026#34;This is a test query2.\u0026#34;] print(embed.embed_documents(input_texts)) Vector Stores # You can check the entire code here.\npip install langchain-chroma Then we can use the Chroma to store the embeddings.\nfrom langchain_chroma import Chroma from langchain_community.document_loaders import WebBaseLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain_community.embeddings import ZhipuAIEmbeddings # load the web page loader = WebBaseLoader(\u0026#34;https://en.d2l.ai/\u0026#34;) docs = loader.load() # split the text into chunks text_splitter = RecursiveCharacterTextSplitter( chunk_size = 1500, chunk_overlap = 150 ) splits = text_splitter.split_documents(docs) # print(len(splits)) # set the embeddings models embeddings = ZhipuAIEmbeddings( model=\u0026#34;embedding-3\u0026#34;, api_key=\u0026#34;your own api key\u0026#34; ) # set the persist directory persist_directory = r\u0026#39;.\u0026#39; # create the vector database vectordb = Chroma.from_documents( documents=splits, embedding=embeddings, persist_directory=persist_directory ) # print(vectordb._collection.count()) # query the vector database question = \u0026#34;Recurrent\u0026#34; docs = vectordb.similarity_search(question, k=3) # print(len(docs)) print(docs[0].page_content) Then you can find the chorma.sqlite3 file in the specific directory.\nRetrieval # This part is the core part of the RAG.\nLast part we have already used the similarity_search method. On top of that, we also have other methods.\nBasic semantic similarity Maximum Marginal Relevance(MMR) Metadata LLM Aided Retrieval Similarity Search # Similarity Search calculates the similarity between the query vector and all document vectors in the database to find the most relevant document.\nThe similarity measurement methods include cosine similarity and Euclidean distance, which can effectively measure the closeness of two vectors in a high-dimensional space.\nHowever, relying solely on similarity search may result in insufficient diversity, as it only focuses on the match between the query and the content, ignoring the differences between different pieces of information. In some applications, especially when it is necessary to cover multiple different aspects of information, the extended method of Maximum Marginal Relevance (MMR) can better balance relevance and diversity.\nPractice # The practice part is on the pervious part.\nMaximum Marginal Relevance (MMR) # Retrieving only the most relevant documents may overlook the diversity of information. For example, if only the most similar response is selected, the results may be very similar or even contain duplicate content. The core idea of MMR is to balance relevance and diversity, that is, to select the information most relevant to the query while ensuring that the information is diverse in content. By reducing the repetition of information between different pieces, MMR can provide a more comprehensive and diverse set of results.\nThe process of MMR is as follows:\nQuery the Vector Store: First convert the query into vectors using the embedding model. Choose the fetch_k most similar responses. Find the top k most similar vectors from the vector store. Within those responses choose the k most diverse. By calculating the similarity between each response, MMR will prefer results that are more different from each other, thus increasing the coverage of information. This process ensures that the returned results are not only \u0026ldquo;most similar\u0026rdquo;, but also \u0026ldquo;complementary\u0026rdquo;. The key parameter is the lambda which is the weight of the relevance and diversity.\nWhen lambda is close to 1, MMR will be more like the similarity search. When lambda is close to 0, MMR will be more like the random search. Practice # We can adjust the code in Vector stores part to use the MMR method. The full code is in the retrieval/mmr.py file.\n# query the vector database with MMR question = \u0026#34;How the neural network works?\u0026#34; # fetch the 8 most similar documents, and then choose the 2 most relevant documents docs_mmr = vectordb.max_marginal_relevance_search(question, fetch_k=8, k=2) print(docs_mmr[0].page_content[:100]) print(docs_mmr[1].page_content[:100]) Metadata # When our query is under some specific conditions, we can use the metadata to filter the results.\nFor example, the information such as page numbers, authors, timestamps, etc. These information can be used as filtering conditions during retrieval, thus improving the accuracy of the query.\nPractice # You can check the entire code here.\nAdd new documents from another website, and then filter the results from the specific website.\nfrom langchain_chroma import Chroma from langchain_community.document_loaders import WebBaseLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain_community.embeddings import ZhipuAIEmbeddings # load the web page loader = WebBaseLoader(\u0026#34;https://en.d2l.ai/\u0026#34;) docs = loader.load() # split the text into chunks text_splitter = RecursiveCharacterTextSplitter( chunk_size = 1500, chunk_overlap = 150 ) splits = text_splitter.split_documents(docs) # print(len(splits)) # set the embeddings models embeddings = ZhipuAIEmbeddings( model=\u0026#34;embedding-3\u0026#34;, api_key=\u0026#34;your_api_key\u0026#34; ) # set the persist directory persist_directory = r\u0026#39;.\u0026#39; # create the vector database vectordb = Chroma.from_documents( documents=splits, embedding=embeddings, persist_directory=persist_directory ) # print(vectordb._collection.count()) # add new documents from another website new_loader = WebBaseLoader(\u0026#34;https://www.deeplearning.ai/\u0026#34;) new_docs = new_loader.load() # split the text into chunks new_splits = text_splitter.split_documents(new_docs) # add to the existing vector database vectordb.add_documents(new_splits) # Get all documents all_docs = vectordb.similarity_search(\u0026#34;What is the difference between a neural network and a deep learning model?\u0026#34;, k=20) # Print the metadata of the documents for i, doc in enumerate(all_docs): print(f\u0026#34;Document {i+1} metadata: {doc.metadata}\u0026#34;) # Document 1 metadata: {\u0026#39;language\u0026#39;: \u0026#39;en\u0026#39;, \u0026#39;source\u0026#39;: \u0026#39;https://en.d2l.ai/\u0026#39;, \u0026#39;title\u0026#39;: \u0026#39;Dive into Deep Learning — Dive into Deep Learning 1.0.3 documentation\u0026#39;} # Document 2 metadata: {\u0026#39;language\u0026#39;: \u0026#39;en\u0026#39;, \u0026#39;source\u0026#39;: \u0026#39;https://en.d2l.ai/\u0026#39;, \u0026#39;title\u0026#39;: \u0026#39;Dive into Deep Learning — Dive into Deep Learning 1.0.3 documentation\u0026#39;} # Document 3 metadata: {\u0026#39;language\u0026#39;: \u0026#39;en\u0026#39;, \u0026#39;source\u0026#39;: \u0026#39;https://en.d2l.ai/\u0026#39;, \u0026#39;title\u0026#39;: \u0026#39;Dive into Deep Learning — Dive into Deep Learning 1.0.3 documentation\u0026#39;} # Document 4 metadata: {\u0026#39;description\u0026#39;: \u0026#39;DeepLearning.AI | Andrew Ng | Join over 7 million people learning how to use and build AI through our online courses. Earn certifications, level up your skills, and stay ahead of the industry.\u0026#39;, \u0026#39;language\u0026#39;: \u0026#39;en\u0026#39;, \u0026#39;source\u0026#39;: \u0026#39;https://www.deeplearning.ai/\u0026#39;, \u0026#39;title\u0026#39;: \u0026#39;DeepLearning.AI: Start or Advance Your Career in AI\u0026#39;} question = \u0026#34;how the neural network works?\u0026#34; # filter the documents from the specific website docs_meta = vectordb.similarity_search(question, k=1, filter={\u0026#34;source\u0026#34;: \u0026#34;https://www.deeplearning.ai/\u0026#34;}) print(docs_meta[0].page_content[:100]) LLM Aided Retrieval # It uses language models to automatically parse sentence semantics, extract filtering information.\nSelfQueryRetriever # LangChain provides the SelfQueryRetriever module, which can analyze the semantics of the question sentence from the language model, and extract the search term and filter conditions.\nThe search term of the vector search The filter conditions of the document metadata For example, for the question \u0026ldquo;Besides Wikipedia, which health websites are there?\u0026rdquo;, SelfQueryRetriever can infer that \u0026ldquo;Wikipedia\u0026rdquo; represents the filter condition, that is, to exclude the documents from Wikipedia.\nPractice # from langchain.llms import OpenAI from langchain.retrievers.self_query.base import SelfQueryRetriever from langchain.chains.query_constructor.base import AttributeInfo llm = OpenAI(temperature=0) metadata_field_info = [ AttributeInfo( name=\u0026#34;source\u0026#34;, # source is to tell the LLM the data is from which document description=\u0026#34;The lecture the chunk is from, should be one of `docs/loaders.pdf`, `docs/text_splitters.pdf`, or `docs/vectorstores.pdf`\u0026#34;, type=\u0026#34;string\u0026#34;, ), AttributeInfo( name=\u0026#34;page\u0026#34;, # page is to tell the LLM the data is from which page description=\u0026#34;The page from the lecture\u0026#34;, type=\u0026#34;integer\u0026#34;, ), ] document_content_description = \u0026#34;the lectures of retrieval augmentation generation\u0026#34; retriever = SelfQueryRetriever.from_llm( llm, vectordb, document_content_description, metadata_field_info, verbose=True ) question = \u0026#34;What is the main topic of second lecture?\u0026#34; Compression # When using vector retrieval to get relevant documents, directly returning the entire document fragment may lead to resource waste, as the actual relevant part is only a small part of the document. To improve this, LangChain provides a \u0026ldquo;compression\u0026rdquo; retrieval mechanism.\nIts working principle is to first use standard vector retrieval to obtain candidate documents, and then use a language model to compress these documents based on the semantic meaning of the query sentence, only retaining the relevant part of the document.\nFor example, for the query \u0026ldquo;the nutritional value of mushrooms\u0026rdquo;, the retrieval may return a long document about mushrooms. After compression, only the sentences related to \u0026ldquo;nutritional value\u0026rdquo; are extracted from the document.\nPractice # from langchain.retrievers import ContextualCompressionRetriever from langchain.retrievers.document_compressors import LLMChainExtractor def pretty_print_docs(docs): print(f\u0026#34;\\n{\u0026#39;-\u0026#39; * 100}\\n\u0026#34;.join([f\u0026#34;Document {i+1}:\\n\\n\u0026#34; + d.page_content for i, d in enumerate(docs)])) llm = OpenAI(temperature=0) # initialize the compressor compressor = LLMChainExtractor.from_llm(llm) # initialize the compression retriever compression_retriever = ContextualCompressionRetriever( base_compressor=compressor, # llm chain extractor base_retriever=vectordb.as_retriever() # vector database retriever ) # compress the source documents question = \u0026#34;What is the main topic of second lecture?\u0026#34; compressed_docs = compression_retriever.get_relevant_documents(question) pretty_print_docs(compressed_docs) Question Answering # Multiple relevant documents have been retrieved from the vector store Potentially compress the relevant splits to fit into the LLM context. The system will generate the necessary background information (System Prompt) and keep the user\u0026rsquo;s question (Human Question), and then integrate all the information into a complete context input. Send the information along with our question to an LLM to select and format an answer RetrievalQA Chain # We need to use Langchain to combine the prompts into the desired format and pass them to the large language model to generate the desired reply. This solution is better than the traditional method of inputting the question into the large language model because:\nEnhance the accuracy of the answer: By combining the retrieval results with the generation ability of the large language model, the relevance and accuracy of the answer are greatly improved. Support real-time update of the knowledge base: The retrieval process depends on the data in the vector store, which can be updated in real time according to needs, ensuring that the answer reflects the latest knowledge. Reduce the memory burden of the model: By using the information in the knowledge base as the input context, the dependence on the model\u0026rsquo;s internal parameters for storing knowledge is reduced. In addition to the RetrievalQA Chain, there are other methods, such as Map_reduce, Refine and Map_rerank.\nMap_reduce # Map_reduce method divides the documents into multiple chunks, and then passes each chunk to the language model (LLM) to generate an independent answer. After that, all the generated answers will be merged into the final answer, and the merging process (reduce) may include summarizing, voting, etc.\nThis method is suitable for the large amount of documents parallel processing, also with quick response.\nRefine # Refine method generates an initial answer from the first document chunk, and then processes each subsequent document one by one. Each block will supplement or correct the existing answer, and finally obtain an optimized and improved answer after all chunks are processed.\nThis method is suitable for the most quality answer.\nMap_rerank # Map_rerank divides the documents into multiple chunks, and then generates an independent answer for each chunk. The scoring is based on the relevance and quality of the answer. Finally, the answer with the highest score will be selected as the final output.\nThis method is suitable for the most match answer rather than combine with all the information.\nPractice # You can check the entire code here.\nFirst, install the lib:\npip install pyjwt You can use the demo to check the model performance.\nfrom langchain_community.chat_models import ChatZhipuAI from langchain_core.messages import AIMessage, HumanMessage, SystemMessage chat = ChatZhipuAI( model=\u0026#34;glm-4-flash\u0026#34;, temperature=0.5, # the temperature of the model api_key=\u0026#34;your_api_key\u0026#34; ) messages = [ AIMessage(content=\u0026#34;Hi.\u0026#34;), # AI generated message SystemMessage(content=\u0026#34;Your role is a poet.\u0026#34;), # the role of the model HumanMessage(content=\u0026#34;Write a short poem about AI in four lines.\u0026#34;), # the message from the user ] # get the answer from the model response = chat.invoke(messages) print(response.content) Then we can use the RetrievalQA chain to get the answer from the model.\nfrom langchain_community.chat_models import ChatZhipuAI from langchain_core.messages import AIMessage, HumanMessage, SystemMessage from langchain_community.embeddings import ZhipuAIEmbeddings from langchain_chroma import Chroma from langchain_community.document_loaders import WebBaseLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.chains import RetrievalQA from langchain.prompts import PromptTemplate # You can also import the PromptTemplate loader = WebBaseLoader(\u0026#34;https://en.d2l.ai/\u0026#34;) docs = loader.load() text_splitter = RecursiveCharacterTextSplitter( chunk_size = 1500, chunk_overlap = 150 ) splits = text_splitter.split_documents(docs) persist_directory = \u0026#39;.\u0026#39; # initialize the embeddings embeddings = ZhipuAIEmbeddings( model=\u0026#34;embedding-3\u0026#34;, api_key=\u0026#34;your_api_key\u0026#34; ) # initialize the vector database vectordb = Chroma.from_documents( documents=splits, embedding=embeddings, persist_directory=persist_directory ) chat = ChatZhipuAI( model=\u0026#34;glm-4-flash\u0026#34;, temperature=0.5, api_key = \u0026#34;your_api_key\u0026#34; ) # Now you can ask the question about the web to the model question = \u0026#34;What is this book about?\u0026#34; # You can also create a prompt template template = \u0026#34;\u0026#34;\u0026#34; Please answer the question based on the following context. If you don\u0026#39;t know the answer, just say you don\u0026#39;t know, don\u0026#39;t try to make up an answer. Answer in at most three sentences. Please answer as concisely as possible. Finally, always say \u0026#34;Thank you for asking!\u0026#34; Context: {context} Question: {question} Helpful answer: \u0026#34;\u0026#34;\u0026#34; QA_CHAIN_PROMPT = PromptTemplate.from_template(template) qa_chain = RetrievalQA.from_chain_type( chat, retriever=vectordb.as_retriever(), return_source_documents=True, # Return the source documents(optional) chain_type_kwargs={\u0026#34;prompt\u0026#34;: QA_CHAIN_PROMPT} # Add the prompt template to the chain ) result = qa_chain({\u0026#34;query\u0026#34;: question}) print(result[\u0026#34;result\u0026#34;]) print(result[\u0026#34;source_documents\u0026#34;][0]) # If you set return_source_documents to True, you can get the source documents Conversational Retrieval Chain # The whole process of RAG is as follows:\nConversational Retrieval Chain is a technical architecture that combines dialogue history and intelligent retrieval capabilities.\nChat History: The system will record the user\u0026rsquo;s dialogue context as an important input for subsequent question processing. Question: The user\u0026rsquo;s question is sent to the retrieval module. Retriever: The system retrieves the content related to the question from the vector database through the retriever. System \u0026amp; Human: The system integrates the user\u0026rsquo;s question and the extracted relevant information into the Prompt, providing structured input to the language model. LLM: The language model generates the answer based on the context, and then returns the answer to the user. Memory # ConversationBufferMemory is a memory module in the LangChain framework, which is used to manage the dialogue history. Its main function is to store the dialogue content between users and AI in the form of a buffer, and then return these records when needed, so that the model can generate responses in a consistent context.\nThe demo of it is as follows:\nfrom langchain.memory import ConversationBufferMemory memory = ConversationBufferMemory( memory_key=\u0026#34;chat_history\u0026#34;, # This key can be referenced in other modules (such as chains or tools). return_messages=True # whether to return the messages in list, otherwise return the messages in block. ) Besides, we also need the corresponding RA module. Then we can test the memory.\nfrom langchain.chains import ConversationalRetrievalChain retriever=vectordb.as_retriever() qa = ConversationalRetrievalChain.from_llm( chat, retriever=retriever, memory=memory ) question = \u0026#34;What is the main topic of this book?\u0026#34; result = qa.invoke({\u0026#34;question\u0026#34;: question}) print(result[\u0026#39;answer\u0026#39;]) question = \u0026#34;What is my last question?\u0026#34; result = qa.invoke({\u0026#34;question\u0026#34;: question}) print(result[\u0026#39;answer\u0026#39;]) Practice # You can check the entire code here.\nThe best practice is as follows:\nfrom langchain_community.chat_models import ChatZhipuAI from langchain_core.messages import AIMessage, HumanMessage, SystemMessage from langchain_community.embeddings import ZhipuAIEmbeddings from langchain_chroma import Chroma from langchain_community.document_loaders import WebBaseLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.memory import ConversationBufferMemory from langchain.chains import ConversationalRetrievalChain loader = WebBaseLoader(\u0026#34;https://en.d2l.ai/\u0026#34;) docs = loader.load() text_splitter = RecursiveCharacterTextSplitter( chunk_size = 1500, chunk_overlap = 150 ) splits = text_splitter.split_documents(docs) persist_directory = \u0026#39;.\u0026#39; # initialize the embeddings embeddings = ZhipuAIEmbeddings( model=\u0026#34;embedding-3\u0026#34;, api_key=\u0026#34;your_api_key\u0026#34; ) # initialize the vector database vectordb = Chroma.from_documents( documents=splits, embedding=embeddings, persist_directory=persist_directory ) chat = ChatZhipuAI( model=\u0026#34;glm-4-flash\u0026#34;, temperature=0.5, api_key = \u0026#34;your_api_key\u0026#34; ) # initialize the memory memory = ConversationBufferMemory( memory_key=\u0026#34;chat_history\u0026#34;, return_messages=True ) # create the ConversationalRetrievalChain retriever = vectordb.as_retriever() qa = ConversationalRetrievalChain.from_llm( chat, retriever=retriever, memory=memory ) # First question question = \u0026#34;What is the main topic of this book?\u0026#34; result = qa.invoke({\u0026#34;question\u0026#34;: question}) print(result[\u0026#39;answer\u0026#39;]) # Second question question = \u0026#34;Can you tell me more about it?\u0026#34; result = qa.invoke({\u0026#34;question\u0026#34;: question}) print(result[\u0026#39;answer\u0026#39;]) "},{"id":7,"href":"/posts/random-variables/","title":"Random Variables","section":"Blog","content":"This article introduces the common distribution functions both continuous and discrete. Besides, the basic knowledge of conditional distribution such as total probability and Bayes\u0026rsquo; Theorem is also introduced. The most important part are the Normal Approximation and Poisson Approximation of the Binomial Random Variable.\nFor the series of articles, you can find them in the Probability category.\nRandom variable X # A random variable X is a process of assigning a number to every outcome, while the resulting function must stisfy the following two conditions (otherwise arbitrary):\nThe set {X\u0026lt;=x) is an event for every x. The probabilities of the events(x= infinity) and (x=-infinity) equal 0. Continuous, discrete, mixed\nProb. Distribution and their Properties # Distribution Function # The probability of the event $({\\xi | X(\\xi) ≤x})$ must depend on x. Denote $(P(\\xi | X(\\xi) ≤x)=F_{X}(x) ≥0 .)$\nThe role of X is to identify the actual r.v. $F_{X}(x)$ is said to the Probability Distribution Function of X. eg. In the coin-tossing experiment, the probability of heads equals p and the probability of tails equals q. We define the random variable x such that X(h) =1, X(t)=0.\nFor the Distribution Function,\nIf x \u0026lt; 0, because the X is 0 or 1, so it is impossible, so $F_{X}(x)=P(X\\leq x)=0$. If $0\\leq x \u0026lt; 1$, $F_{X}(x)=P(X\\leq x)=P(X=0)=q$. If $x\\geq 1$, $F_{X}(x)=P(X\\leq x)=P(X=0 or X=1)=P(X=0)+P(X=1)=q+p=1$. Properties # A distribution function F(x) is nondecreasing, right-continuous and satisfies\n$P( \\infty)=1$ and $P(-\\infty)=0$ if $x1 \u0026lt; x2$, then $F(x1) \u0026lt;= F(x2)$. $P(a\u0026lt;X\u0026lt;=b)=F(b)-F(a)$ $P(X\u0026gt;a)=1-P(X\u0026lt;=a)=1-F(a)$ $P(X=a)=F(a)-F(a-0)=F(a)-lim_{x-\u0026gt;a-}F(x)$ Continuous, Discrete, (And Mixed) Types # x is said to be a continuous type if its distribution function is continuous.\nContinuous: $F(x)$ is continuous and differentiable. $[F_{X}(x-)=F_{X}(x) for all x ; P(X=x)=0.]$ Discrete: $F(x)$ is a step function. P(x=xi)=F(xi)-F(xi-0)=pi Mixed: $F(x)$ is a combination of continuous and discrete parts. Prob. Density(PDF) # $f_{X}(x)=\\frac{d F_{X}(x)}{d x}$ $f_{X}(x)\u0026gt;=0$ if $F_{X}(x)$ is continuous, then $f_{X}(x)$ will be a continuous function. if $X$ is a discrete type r.v, then its p.d.f has the general form $f_{X}(x)=\\sum_{i} p_{i} \\delta(x-x_{i})$ where $x_{i}$ represent the jump-discontinuity points in $F_{X}(x)$ $F_{X}(x)=\\int_{-\\infty}^{x} f_{x}(u) d u .$ $P{x_{1}\u0026lt;X(\\xi) \\leq x_{2}}=F_{X}\\left(x_{2}\\right)-F_{X}\\left(x_{1}\\right)=\\int_{x_{1}}^{x_{2}} f_{X}(x) d x .$ Common Distribution Functions # Parameter, shape, where to reach max, symmetrical\u0026hellip;\nContinuous Distribution # Gaussian # We say tbat x is a nonnal or Gaussian random variable if its density function is given by $f_{x}(x)=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{-(x-\\mu)^{2} / 2 \\sigma^{2}}$ This is a bell-shaped curve, symmetric around the parameter μ ,and its distribution function is given by $F_{x}(x)=\\int_{-\\infty}^{x} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{-(y-\\mu)^{2} / 2 \\sigma^{2}} d y \\triangleq G\\left(\\frac{x-\\mu}{\\sigma}\\right)$ where $G(x) \\triangleq \\int_{-\\infty}^{x} \\frac{1}{\\sqrt{2 \\pi}} e^{-y^{2} / 2} d y$\nthe notation x ～ $N(\\mu, \\sigma^{2})$ will be used to represent\nUnder very general conditions, the sum of a large number of independent random variables is approximately normally distributed.\nUniform # $$X \\sim U(a, b), a\u0026lt;b,$$\n$$ f_{X}(x)= \\begin{cases}\\frac{1}{b-a} \u0026amp; a \\leq x \\leq b \\ 0 \u0026amp; otherwise \\end{cases} $$\nExponential # Exponential: X ~ $\\varepsilon(\\lambda)$\n$$ f_{X}(x)=\\begin{cases} \\frac{1}{\\lambda} e^{-\\frac{x}{\\lambda}}, \u0026amp; x \\geq 0 \\ 0, \u0026amp; \\text{otherwise} \\end{cases} $$\nIf the occurrences of events over non-overlapping intervals are independent (eg, arrival times of telephone calls/packets or bus arrival times at a bus stop, etc), then the waiting time distribution of these events can be modeled as exponential.\nMemoryless property\n$$P{x\u0026gt;t+s | x\u0026gt;s}=\\frac{P{x\u0026gt;t+s}}{P{x\u0026gt;s}}=\\frac{e^{-(t+s)}}{e^{-s}}=e^{-t}=P{x\u0026gt;t}$$\neg. Suppose the life length of an appliance has an exponential distribution with (\\lambda=10) years. A used appliance is bought by someone. What is the probability that it will not fail in the next 5 years?\n$$P(x\u0026gt;t_{0}+5 | x\u0026gt;t_{0})=P(x\u0026gt;5)=e^{-5 / 10}=e^{-1 / 2}=0.368$$\nDiscrete Distribution # Bernoulli # Bernoulli: X is Bernoulli distributed if X takes the values 1 and 0 with P{X = 1) = p, P{X = 0) = q = 1- P\nBinomial # With n independent Bernoulli trials, let Y be the total number of favorable outcomes.\n$$ P(y=k)=\\binom{n}{k} p^{k} q^{n-k} ,p+q=1, k=0,1,2, \u0026hellip;, n $$\nPoisson # The number of occurrences of a rare event in a large number of trials. eg, the number of winning tickets among those purchased in a large lottery.\n$$P(x=k)=e^{-\\lambda} \\frac{\\lambda^{k}}{k !}, k=0,1,2, \u0026hellip;$$\n$$\\frac{p_{k-1}}{p_{k}}=\\frac{e^{-\\lambda} \\lambda^{k-1} /(k-1) !}{e^{-\\lambda} \\lambda^{k} / k !}=\\frac{k}{\\lambda}$$\nFrom the formula, the P(x=k) is the maximum when k=$\\lambda$, if the $\\lambda$ is an integer, the maximum is P(x=$\\lambda$) and P(x=$\\lambda$-1).\nGeometric # Mainly used to describe the first occurrence in the k-th trial in the series of independent trials.\n$$X \\sim g(p)$$\n$$P(X=k)=pq^k,k=0,1,2,\u0026hellip;, q=1-p.$$\nMemoryless Property\n$$P(x\u0026gt;m)=\\sum_{k=m+1}^{\\infty} P(x=k)=\\sum_{k=m+1}^{\\infty} p q^{k-1} =p q^{m}(1+q+\\cdots)=\\frac{p q^{m}}{1-q}=q^{m}$$ $$P(x\u0026gt;m+n | x\u0026gt;m)=\\frac{P(x\u0026gt;m+n)}{P(x\u0026gt;m)}=\\frac{q^{m+n}}{q^{m}}=q^{n}$$\nConclusion # Common continuous probability distributions # Distribution Description Example Normal distribution Descrbes data with values that become less probable the farther they are from the mean, with a bell-shaped probability density function. SAT scores Continuous uniform Describes data for which equal-sized intervais have equal probability. The amount of time cars wait at a red light Log-normal Describes right-skewed data. It\u0026rsquo;s the probability distribution of a random variable whose logarthm is normaly distributed. The average body weight of dfferent mammal species Exponential Describes data that has higher probablities for small values than large values.It\u0026rsquo;s the probablity distribution of time between independent events. Time between earthquakes Common discrete probability distributions # Distribution Description Example Bernoulli Describes a single trial with two possible outcomes (e.g., success/failure, yes/no). Tossing a coin Binomial Describes the number of successes in a fixed number of independent Bernoulli trials. Number of heads in 10 coin tosses Poisson Describes the number of occurrences of an event in a fixed interval of time or space. Number of phone calls received in an hour Geometric Describes the number of trials needed to get the first success in a sequence of independent Bernoulli trials. Number of coin tosses until the first head Conditional Distributions # $$P(A | B)=\\frac{P(A \\cap B)}{P(B)}, P(B) \\neq 0 .$$\nThe probaility distribution function $F_{X}(x)=P \\lbrace X(\\xi) ≤x \\rbrace$ is a function of x.\nDefine the conditional distribution of the r.v X given the event B as $$F_{X}(x | B)=P\\lbrace X(\\xi) \\leq x | B\\rbrace=\\frac{P\\lbrace (X(\\xi) \\leq x) \\cap B\\rbrace}{P(B)} .$$\n$$P\\left(x_{1}\u0026lt;X(\\xi) \\leq x_{2} | B\\right)- \\frac{P\\lbrace\\left(x_{1}\u0026lt;X(\\xi) \\leq x_{2}\\right) \\cap B\\rbrace}{P(B)} \\ =F_{X}\\left(x_{2} | B\\right)-F_{X}\\left(x_{1} | B\\right)$$\nFor the PDF:\n$$f_{x}(x | B)=\\frac{d F_{x}(x | B)}{d x},$$\n$$F_{X}(x | B)=\\int_{-\\infty}^{x} f_{X}(u | B) d u .$$\n$$P(x\u0026lt;X(\\xi)≤x|B)-f_{X}(x|B)dx.$$\nTotal Probability # $$P(x \\leq x)=P\\lbrace x \\leq x | A_{1}\\rbrace P\\left(A_{1}\\right)+\\cdots+P\\lbrace x \\leq x | A_{n}\\rbrace P\\left(A_{n}\\right)$$\n$$F(x)=F\\left(x | A_{1}\\right) P\\left(A_{1}\\right)+\\cdots+F\\left(x | A_{n}\\right) P\\left(A_{n}\\right)$$\n$$f(x)=f\\left(x | A_{1}\\right) P\\left(A_{1}\\right)+\\cdots+f\\left(x | A_{n}\\right) P\\left(A_{n}\\right)$$\nBayes\u0026rsquo; Theorem in continuous version # $$f_{X | A}(x | A)=\\frac{P(A | X=x) f_{X}(x)}{\\int_{-\\infty}^{+\\infty} P(A | X=x) f_{X}(x) d x} $$\neg. 4-19 important.\nAsymptotic Approximations for Binomial Random Variable # Binomial Random Variable # Let X represent a binomial random variable\n$$P(k_{1} ≤X ≤k_{2})=\\sum_{k=k_{1}}^{k_{2}} P_{n}(k)=\\sum_{k=k_{1}}^{k_{2}}\\binom{n}{k} p^{k} q^{n-k}$$\nAnd the coefficient $\\binom{n}{k}$ grows rapidly as n increases.\nThe Normal Approximation (DeMoivre-Laplace Theorem) # Suppose $(n \\to \\infty)$ with P held fixed. Then for k in the $\\sqrt{n p q}$ neighborhood of $n p$, we can approximate\nImportant! $$ \\binom{n}{k} p^{k} q^{n-k} \\simeq \\frac{1}{\\sqrt{2 \\pi n p q}} e^{-(k-n p)^{2} / 2 n p q} ,p+q=1 $$\nThus if $(k_{1})$ and $(k_{2})$ are within or around the neighborhood of the interval $((n p-\\sqrt{n p q}, n p+\\sqrt{n p q}),) we can approximate the summation by an integration\n$$P(k_{1} ≤X ≤k_{2})=\\int_{k_{1}}^{k_{2}} \\frac{1}{\\sqrt{2 \\pi} \\varphi p q} e^{-(x-u p)^{2} / 2 \\varphi p q} d x=\\int_{x_{1}}^{x_{2}} \\frac{1}{\\sqrt{2 \\pi}} e^{-y^{2} / 2} d y,$$\nImportant!\nwhere $x_{1}=(k_{1}-n p) / \\sqrt{n p q}$ and $x_{2}=(k_{2}-n p) / \\sqrt{n p q}$\nDefine error function $(erf(x)=\\frac{1}{\\sqrt{2 \\pi}} \\int_{0}^{x} e^{-y^{2} / 2} d y=erf(-x))$ that can be tabulated\n$$P\\left(k_{1} \\leq X \\leq k_{2}\\right)=erf\\left(x_{2}\\right)-erf\\left(x_{1}\\right) .$$\nMeaning: the evaluation of the prob (k success in n trials) is approximately the evaluation of the normal curve.\nApproximation of the P{k1\u0026lt;=X\u0026lt;=k2} # When npq \u0026raquo; 1 and k1 - np and k2 - np are of the order of $\\sqrt{npq}$, the normal approximation is good.\n$$ \\sum_{k=k_{1}}^{k_{2}}\\binom{n}{k} p^{k} q^{n-k} \\simeq G\\left(\\frac{k_{2}-n p}{\\sqrt{n p q}}\\right)-G\\left(\\frac{k_{1}-n p}{\\sqrt{n p q}}\\right) $$\nExample 4-23 Over a period of 12 hours, 180 callsare made at random.What is the probability that in a four hour interval the number of calls is between 50 and 70?\nLet p = 4/12, the probability that in a four hour interval the number of calls is between 50 and 70 is\nThe probability that k calls will occur in this interval equals\n$$ \\binom{180}{k}\\left(\\frac{1}{3}\\right)^{k}\\left(\\frac{2}{3}\\right)^{180-k} \\simeq \\frac{1}{4 \\sqrt{5 \\pi}} e^{-(k-60)^{2} / 80} $$\nand the probability that the number of calls is between 50 and 70 equals\n$$ \\sum_{k=50}^{70}\\binom{180}{k}\\left(\\frac{1}{3}\\right)^{k}\\left(\\frac{2}{3}\\right)^{180-k} \\simeq G(\\sqrt{2.5})-G(-\\sqrt{2.5}) \\simeq 0.886 $$\nThe Law of Large Numbers # if an event A with P(A)=p, occurs k times in n trials, $k \\approx np$ In fact\n$$P(k=n p) \\simeq \\frac{1}{\\sqrt{2 \\pi n p q}} \\to 0, n \\to \\infty$$\nthe approximation $(k \\approx n p)$ means that the ratio $(k / n)$ is close to p in the sense that, for any $(\\varepsilon\u0026gt;0)$ the probability that $(|k / n-p|\u0026lt;\\varepsilon)$ tends to 1 as $(n \\to \\infty)$\nPossion Approximation # For large n, the Gaussian approximation is valid only if p is fixed, i.e. if np \u0026raquo; 1, npq \u0026raquo; 1.\nWhat if np is small, or if it does not increase with n?\np to 0 as n to infinity, so that np = $(\\lambda)$ is fixed.\nRecall as $(n p=\\lambda .)$ We can obtain an excellent approximation:\n$$ \\begin{aligned} P_{n}(k) \u0026amp; =\\frac{n(n-1) \\cdots(n-k+1)}{n^{k}} \\frac{(n p)^{k}}{k !}(1-n p / n)^{n-k} =\\left(1-\\frac{1}{n}\\right)\\left(1-\\frac{2}{n}\\right) \\cdots\\left(1-\\frac{k-1}{n}\\right) \\frac{\\lambda^{k}}{k !} \\frac{(1-\\lambda / n)^{n}}{(1-\\lambda / n)^{k}} . \\end{aligned} $$\nThus The Poisson p.m.f\n$$ lim_{n \\to \\infty, p \\to 0, n p=\\lambda} P_{n}(k)=\\frac{\\lambda^{k}}{k !} e^{-\\lambda}, $$\nIn conclusion:\nIf n to infinity, p to 0, and np = $(\\lambda)$ is fixed, then\n$$ \\frac{n !}{k !(n-k) !} p^{k} q^{n-k} \\underset{n \\to \\infty}{\\to} e^{-\\lambda} \\frac{\\lambda^{k}}{k !} k=0,1,2, \u0026hellip; $$\neg. Example 4-27: A system contains 1000 components. Each component fails independently of the others and the probability of its failure in one month equals 10^-3. What the probability that in one month no failure occurs?\nn = 1000, p = 10^-3, $(\\lambda)$ = np = 1\n$$ P_{1000}(X=0)=\\frac{\\lambda^{0}}{0 !} e^{-\\lambda}=e^{-1} \\simeq 0.368 $$\neg. 4-8 The random variable x is $(N(10 ; 1))$ .Find $(f(x |(x-10)^{2}\u0026lt;4))$.\nRandom Poisson Points # $$P\\lbrace k in t_{a}\\rbrace=\\binom{n}{k} p^{k} q^{n-k} where, p=\\frac{t_{a}}{T}$$\n"},{"id":8,"href":"/posts/repeated-trials/","title":"Repeated Trials","section":"Blog","content":"This article is mainly about Bernoulli Trials and its properties.\nIf you lack the basic knowledge of probability, you can read the Probability and its Axioms first.\nIndependent Events # Independent: Event A and B are independent if P(AB) = P(A)P(B)\nMore generally, a family of events $A_1, A_2, \\cdots, A_n$ are independent if for any subset $A_{i_1}, A_{i_2}, \\cdots, A_{i_k}$ of these events,\n$$ P\\left(\\bigcap_{k=1}^{n} A_{i_{k}}\\right)=\\prod_{k=1}^{n} P\\left(A_{i_{k}}\\right) . $$\nIndependent events obviously cannot be mutually exclusive. eg, flipping a coin twice, the result of the second flip is independent of the first flip. Only the head or tail of once flip can be mutually exclusive.\nLet $A=A_{1} \\cup A_{2} \\cup A_{3} \\cup \\cdots \\cup A_{n}$, a union of n independent events.\n$$ P(\\overline{A}) = P(\\overline{A_1}\\overline{A_2}\\cdots\\overline{A_n}) = \\prod_{i = 1}^{n}P(\\overline{A_i}) = \\prod_{i = 1}^{n}(1 - P(A_i)) $$\nThus for any such A $P(A)=1-P(\\overline{A})=1-\\prod_{i=1}^{n}\\left(1-P\\left(A_{i}\\right)\\right)$\neg. Example: Three switches connected in parallel operate independently. Each switch remains closed with probability p.\n(a) Find the probability of receiving an input signal at the output.\n(b) Find the probability that switch S is open given that an input signal is received at the output.\na. Let $A_i$ be the event that switch $S_i$ is closed. Then $P(A_i) = p$. The event that an input signal is received at the output is $A = A_1 \\cup A_2 \\cup A_3$.\n$$ P(A) = 1 - P(\\overline{A}) = 1 - \\prod_{i=1}^{3}(1 - P(A_i)) = 1 - (1 - p)^3 $$\nb. We want to find $P(\\overline{A_1}|R)$.\nFrom Bayes\u0026rsquo; theorem,\n$$ P(\\overline{A_1} | R)=\\frac{P(R | \\overline{A_1}) P(\\overline{A_1})}{P(R)}=\\frac{(2 p - p^{2})(1 - p)}{3 p - 3 p^{2}+p^{3}}=\\frac{2 - 2 p + p^{2}}{3 p - 3 p^{2}+p^{3}} $$\nBecause of the symmetry of the switches, we also have\n$$ P(\\overline{A_1} | R) = P(\\overline{A_2} | R) = P(\\overline{A_3} | R) $$\nCartesian Products # If A is a subset of S1 and B is a subset of S2, then $C=A\\times B$ is the set of all pairs $(e_1,e_2)$, where $e_1\\in A$ and $e_2\\in B$.\n$$ A\\times B=(A\\times S2)\\cup(B\\times S1) $$\neg. Example 1-3: A box $B_1$ contains 10 white and 5 red balls and a box $B_2$ contains 20 white and 20 red balls.\nA ball is drawn from each box. What is the probability that the ball from $B_1$ will be white and the ball from $B_2$ red?\nLet $S_1$ be the experiment that the ball from $B_1$, $W_1$ is white and $S_2$ be the experiment that the ball from $B_2$, $R_2$ is red.\n$$ P(W_1 \\times R_2) = P(W_1)P(R_2) = \\frac{10}{15}\\times\\frac{20}{40} = \\frac{1}{3} $$\nAssuming the two experiments are independent.\nBernoulli Trials # Solution: Let $(\\Omega,F,P)$ be the probability model for a single trial. The outcome of $n$ experiments is an $n$-tuple $\\omega={\\xi_{1}, \\xi_{2}, \\cdots, \\xi_{n}} \\in \\Omega_{0}$\nThe probability of occurrence of such an o is given by\nRecall that, starting with n possible choices, the first object can be chosen n different ways, and for every such choice the second one in n-1 ways,..and the kth one n-k+1 ways, and this gives the total choices for k objects out of n to be\n$$ N=\\frac{n(n-1) \\cdots(n-k+1)}{k !}=\\frac{n !}{(n-k) ! k !}=\\binom{n}{k} $$\nthe number of combinations, or choices of n identical objects taken k at a time.\nIndependent repeated experiments, where the outcome is either a \u0026ldquo;\u0026lsquo;success\u0026rdquo; or a \u0026ldquo;failure\u0026rdquo; are characterized as Bernoulli trials, and the probability of k successes in n trials is given by\n$$ P_{n}(k)=P(A\\text{ occurs exactly }k\\text{ times in }n\\text{ trials})=\\binom{n}{k}p^{k}q^{n - k},\\quad k = 0,1,2,\\cdots,n $$\nwhere p represents the probability of \u0026ldquo;\u0026lsquo;success\u0026rdquo; in any one trial.\nLet $X_k$ = exactly k successes in n trials.\nThus $$ P\\left(X_{0} \\cup X_{1} \\cup \\cdots \\cup X_{n}\\right)=\\sum_{k=0}^{n} P\\left(X_{k}\\right)=\\sum_{k=0}^{n}\\binom{n}{k} p^{k} q^{n-k} . $$\nSolution: With (X_{i}, i=0,1,2, \\cdots, n,) 一 \u0026ldquo;exactly k occurrences of A in n trials\u0026rdquo;,mutually exclusive events. Thus\n$$ P(\\text{Occurrences of A is between } k_{1} \\text{ and } k_{2} ) = \\sum_{k=k_{1}}^{k_{2}} P\\left(X_{k}\\right)=\\sum_{k=k_{1}}^{k_{2}} \\binom{n}{k} p^{k} q^{n-k} . $$\neg. players A and B agree to play a series of games on the condition that A wins the series if he succeeds in winning m games before B wins n games. The probability of winning a single game is p for A and q=1-p for B.\nLet PA= Prob (A wins m games before B wins n games), PB= Prob(B wins n games before A wins m games\nBy the (m+n-1) games,one wins PA+PB=1\nLet $X_k =$ {A wins m games in exactly m+k games}, $k = 0,1,2,\\cdots,n-1$ M.E.\nThus (A wins)=$X_0 \\cup X_1 \\cup \\cdots \\cup X_{n-1}$\n$P(A \\text{ wins}) = P(X_0 \\cup X_1 \\cup \\cdots \\cup X_{n-1}) = \\sum_{k=0}^{n-1} P(X_k)$\nSo once A wins, he must win the last game and m-1 games in the first m+k-1 games. Thus we can list the following:\nP(A wins m-1 games in any order among the first m+k-1 games) * P(A win the last game)\n$$ P(X_k) = (\\frac{m+k-1}{m-1}) p^{m-1} q^{k} * p $$\nThe properties of $P_n(k)$ # $P_n(k) = \\binom{n}{k} p^k q^{n-k}$ $\\sum_{k=0}^{n} P_n(k) = 1$ $P_n(k) = P_n(n-k)$ $P_n(k)$ is a function of k increases until k=(n+1)p\nAs k increases, P(k) increases reaching a maximum (max) where\n$$ (n+1) p-1 \\leq k_{max } \\leq(n+1) p $$\nIf k is not an integer, the maximum $P_n(k)$ occurs at the largest integer less than k. If k is an integer, the maximum $P_n(k)$ occurs at k and k-1. The most likely number of successes in n trials, (max) satisfies\n$$ p-\\frac{q}{n} \\leq \\frac{k_{max }}{n} \\leq p+\\frac{p}{n}, $$\n$$ lim_{n \\to \\infty} \\frac{k_{m}}{n}=p $$\nBernoulli Theorem # Let A denote an event whose probability of occurrence in a single trial is p. If k denotes the number of occurrences of A in n independent trials.\nThe frequency definition of probability of an event (k/n) and its axiomatic definition (p) can be made compatible to any degree of accuracy.\n$$ P({|\\frac{k}{n}-p|\u0026gt;\\varepsilon})\u0026lt;\\frac{pq}{n\\varepsilon^{2}} $$\nSome ideas: for a given (\\varepsilon), the probability of (|\\frac{k}{n}-p|\u0026gt;\\varepsilon) decreases as n increases. When the n is very large, we can make the fractional occurrence (relative frequency) (k/n) as close to the actual probability p of the event A in a single trial. Since k max is the most likely number of occurrences of A in n trials, as n increasing to infinity, the p tends to around k max.\n"},{"id":9,"href":"/posts/probability-and-its-axioms/","title":"Probability and Its Axioms","section":"Blog","content":"This article is about the probability and its axioms.\nProbability Theory # Probability theory deals with the study of random phenomena, which under repeated experiments yield different outcomes that have certain underlying patterns about them.\nthe notion of an experiments: a set of repeatable conditions that allow any number of identical repetitions.\ncertain averages approach a constant value as the number of observations increases\nObservation deduction and prediction # definition # Laplace\u0026rsquo;s classical definition # The probability of an event A is defined a-priori without actual experimentation as $P(A)=\\frac{\\text{Number of outcomes favorable to }A}{\\text{Total number of possible outcomes}}$\nHowever, it still has some issues: eg Bertrand\u0026rsquo;s paradox.\nrelative frequency definition # $P(A)=\\lim_{n \\to \\infty} \\frac{n_A}{n}$\nWhere $n_A$ is the number of occurrences of $A$ and $n$ is the total number of trials.\nan useful example\nAmong 1 2 \u0026hellip; n, the nums p, 2p \u0026hellip; are divisible by p. Thus there are n/p such numbers between 1 and n. Hence:\n$$ P{\\text{a given number } N \\text{ is divisible by a prime } p} = \\lim_{n\\rightarrow\\infty} \\frac{n/p}{n}=\\frac{1}{p}. \\ $$\nset # equal: two sets are declared to be equal if and only if they contain exactly the same elements, $A = B \\Leftrightarrow A \\subseteq B \\land B \\supseteq A$\nsubset: $A \\subseteq B \\Leftrightarrow \\forall x \\in A, x \\in B$\nsuperset: $A \\supseteq B \\Leftrightarrow B \\subseteq A$\nIf a set consists of n elements, then the total number of its subsets equals 2^n (from empty to full)\ncardinality # |A| is a measure of how many different elements A has.\nfiniteness # some infinite sets we\u0026rsquo;ve seen\nthe set of all positive integers cartesian products of sets # $ \\begin{equation} A\\times B := {(a, b) \\mid a\\in A \\land b\\in B } \\end{equation} $\neg. {a,b}x{1,2} = {(a,1),(a,2),(b,1),(b,2)}\nfor finite A, B, |AxB|=|A||B|.\nCartesian product is not commutative\ntransitivity # $ \\text{If } C\\subseteq B \\text{ and } B\\subseteq A \\text{ then } C\\subseteq A $\nequality # $ \\text{Equality: }A = B\\text{ if }A\\subseteq B\\text{ and }B\\subseteq A $\nOperators # Union # $ A\\cup B := {x \\mid x\\in A \\text{ or } x\\in B} $\n$ \\forall A, B: (A\\cup B \\supseteq A) \\wedge (A\\cup B \\supseteq B) $\nIntersection # Formally, $\\forall A,B: A\\cap B\\ (\\text{or}=AB) \\equiv {x \\mid x\\in A \\land x\\in B}$.\n$\\forall A, B:(A \\cap B \\subseteq A) \\land (A \\cap B \\subseteq B)$\nProperties:\ncommutative: $A\\cap B = B\\cap A$ associative: $(A\\cap B)\\cap C = A\\cap (B\\cap C)$ distributive: $A\\cap (B\\cup C) = (A\\cap B)\\cup (A\\cap C)$ Disjointedness # $A$ and $B$ are disjoint if $A\\cap B = \\emptyset$\neg. the set of even numbers and the set of odd numbers are disjoint.\nInclusion-Exclusion Principle # $ |A\\cup B| = |A| + |B| - |A\\cap B| $\nSet difference # $ A-B ={x | x \\in A \\Lambda x \\notin B} $\nSet complement # The universe of discourse can itself be considered a set, call it U.\nThe complement of A, written (\\bar{A}) , is the complement of A. ie, U-A = $\\overline{A}={x | x \\notin A}$\nMutually exclusive # $A$ and $B$ are mutually exclusive(M.E.) if $A\\cap B = \\emptyset$\nA partition of $\\Omega$ is a collection of mutually exclusive subsets of $\\Omega$ such that their union is $\\Omega$.\n$A_{i} \\cap A_{j}=\\phi$, and $\\bigcup_{i=1} A_{i}=\\Omega$\nSet identities # Identity: $A\\cup \\emptyset = A$, $A\\cap U = A$ Domination: $A\\cup U = U$, $A\\cap \\emptyset = \\emptyset$ Idempotent: $A\\cup A = A$, $A\\cap A = A$ Double complement: $\\overline{\\overline{A}} = A$ Commutative: $A\\cup B = B\\cup A$, $A\\cap B = B\\cap A$ Associative: $(A\\cup B)\\cup C = A\\cup (B\\cup C)$, $(A\\cap B)\\cap C = A\\cap (B\\cap C)$ Distributive: $A\\cup (B\\cap C) = (A\\cup B)\\cap (A\\cup C)$, $A\\cap (B\\cup C) = (A\\cap B)\\cup (A\\cap C)$ De Morgan\u0026rsquo;s laws: $\\overline{A\\cup B} = \\overline{A}\\cap \\overline{B}$, $\\overline{A\\cap B} = \\overline{A}\\cup \\overline{B}$ Duality principle: $A\\cup B = \\overline{\\overline{A}\\cap \\overline{B}}$, $A\\cap B = \\overline{\\overline{A}\\cup \\overline{B}}$ If in a set identity, we replace all unions by intersections, all intersections by unions, and the sets U and $\\emptyset$ by the sets $\\emptyset$ and U, the identity will be preserved.\neg.\n$A \\cap (B \\cup C) = A \\cap B \\cup A \\cap C$ ==\u0026gt; $A \\cup B \\cap C=(A \\cup B) \\cap(A \\cup C)$\n$U \\cup A = U$ ==\u0026gt; $Q \\cap A=\\emptyset$\nmore examples: suppose A and B are not mutually exclusive, then $P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$ and $P(\\overline A B) = P(B) - P(A B)$. Field # Field: A collection of subsets of a nonempty set $\\Omega$ forms a field F if\nfull collection is $\\Omega$.\n$\\Omega \\in F$ if $A \\in F$, then $\\overline{A} \\in F$ if $A, B \\in F$, then $A \\cup B \\in F$ σ-FIELD # σ-Field (Definition): A field F is a σ-field if in addition to the three conditions of a field ((i) – (iii)), we have the following:\nFor every sequence $(A_{i}, i=1 \\to \\infty)$, For every sequence of pair wise disjoint events belonging to F, their union also belongs to F, i.e.,\n$$ A=\\bigcup_{i=1}^{\\infty} A_{i} \\in F . $$\nProbability space # S (U) or $\\sigma$ is called the certain event, its elements experimental outcomes, and its subsets events.\nImpossible event: ${\\emptyset}$ Certain event: $\\Omega$ Elementary event ${e_i}$: an event which contains only a single outcome in the sample space. The totality of all $E_i$, known a priori, constitutes a set $\\Omega$, the set of all experimental outcomes.\n$$ \\Omega={\\xi_{1}, \\xi_{2}, \\cdots, \\xi_{k}, \\cdots} $$\n$\\Omega$ has subsets $A, B, C, \\cdots$，If A is a subset of $\\Omega$, then $\\xi_i \\in A$ implies $\\xi_i \\in \\Omega$.\nProbability axioms # $P(A) \\geq 0$ $P(\\Omega) = 1$ $P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$ If $A_i$ are pair wise mutually exclusive, then $P\\left(\\bigcup_{n=1}^{\\infty} A_{n}\\right)=\\sum_{n=1}^{\\infty} P\\left(A_{n}\\right)$ Probability models # • The triplet $(\\Omega, F, P)$ form a probability model, where\n$\\Omega$ is the sample space, not empty $F$ is the field of events, a field of subsets of $\\Omega$ $P$ is the probability measure Conditional probability # $$ P(A) \\approx \\frac{N_{A}}{N}, P(B) \\approx \\frac{N_{B}}{N}, P(A B) \\approx \\frac{N_{A B}}{N} . $$\nThus the ratio $\\frac{N_{A B}}{N_{B}}=\\frac{N_{A B} / N}{N_{B} / N}=\\frac{P(A B)}{P(B)} = P(A|B)$, which is the conditional probability of $A$ given $B$.\nIf $A_i$ are pair wise disjoint, then\n$$ P(B)=\\sum_{i=1}^{n} P\\left(B A_{i}\\right)=\\sum_{i=1}^{n} P\\left(B | A_{i}\\right) P\\left(A_{i}\\right) . $$\nIf A and B are independent, then $P(A|B) = P(A)$ and $P(B|A) = P(B)$.\nBayes\u0026rsquo;s theorem # $$ P(A|B) = \\frac{P(B|A)}{P(B)} P(A) $$\nP(A) represents the a-priori probability of the event A. General version # A set of mutually exclusive events with associated a-priori probabilities $P(A_{i}), i=1 \\to n .$ With the new information B has occurred, the information about $A_i$ can be updated by the n conditional probabilities $P(B|A_{i})$\n$$ P\\left(A_{i} | B\\right)=\\frac{P\\left(B | A_{i}\\right) P\\left(A_{i}\\right)}{P(B)}=\\frac{P\\left(B | A_{i}\\right) P\\left(A_{i}\\right)}{\\sum_{i=1}^{n} P\\left(B | A_{i}\\right) P\\left(A_{i}\\right)}, $$\napplication # eg. Two boxes B1 and B2 contain 100 and 200 light bulbs respectively. The first box (B1) has 15 defective bulbs and the second has 5. Suppose a box is selected at random and one bulb is picked out.\n(a) What is the probability that it is defective?\nLet D be the event that the bulb is picked out.\nThen $P(D) = P(D|B1)P(B1) + P(D|B2)P(B2) = 0.15*0.5 + 0.025 * 0.5 = 0.0875$\n(b) If it is defective, what is the probability that it came from box B1?\nLet B1 be the event that the box is picked out.\nThen $P(B1|D) = \\frac{P(D|B1)P(B1)}{P(D)} = \\frac{0.15*0.5}{0.0875} = 0.857$\n"},{"id":10,"href":"/posts/further-understanding-of-proc/","title":"Further Understanding of Proc","section":"Blog","content":"The /proc in Linux is a kind of Pseudo file system (also known as Virtual File System), what it saves is a series of special files that store the current running state of the kernel, and the user can check the hardware info and the runnning processes via the files, even adjust some files to change the state of running kernel. The files in /proc will be refresh in RAM, so the files\u0026rsquo; size is 0 bytes.\nYou can check the official docs here.\nThe number Directory # There are many dirs named the number, which indicate the pid of running process.\ncmdline — The command line of the process. environ — The environment variables of the process. fd — The file descriptors of the process. maps — The memory maps of the process. mem — The memory usage of the process. root — The root directory of the process. status — The status of the process. Highly readable. stat — The detailed status of the process. Usually used by ps command. task — The task of the process, which saves the tid of every thread whose form is similar to pid. /proc/sys # This directory is used to change the state of the kernel. You can write the file to change the state of the kernel.\nCommon files # /proc/apm # Advanced Power Management (APM) version information and battery - related status information, typically used by the apm command.\n/proc/buddyinfo # The information about the free memory blocks.\n/proc/cmdline # This file shows the parameters passed to the kernel at the time it is started.\n/proc/cpuinfo # The information about the CPU.\n/proc/crypto # This file lists all installed cryptographic ciphers used by the Linux kernel.\n/proc/devices # This file displays the various character and block devices currently configured (not including devices whose modules are not loaded).\n/proc/iomem # This file shows the memory map of the system.\n/proc/ioports # This file shows the I/O ports that the system uses.\n/proc/loadavg # This file shows the load average of the system. The same as the output of uptime command.\ncat /proc/loadavg # 2.79 2.99 3.03 5/349 3582467 The first three columns measure CPU and IO utilization of the last 1, 5, and 15 minute periods. The fourth column shows the number of currently running processes and the total number of processes. The last column displays the last process ID used.\n/proc/meminfo # This file shows the information about the memory, which is often used by the free command.\n/proc/stat # Some of the more commonly used statistics include:\ncpu — Measures the number of jiffies (1/100 of a second for x86 systems) that the system has been in user mode, user mode with low priority (nice), system mode, idle task, I/O wait, IRQ (hardirq), and softirq respectively. The IRQ (hardirq) is the direct response to a hardware event. The IRQ takes minimal work for queuing the \u0026ldquo;heavy\u0026rdquo; work up for the softirq to execute. The softirq runs at a lower priority than the IRQ and therefore may be interrupted more frequently. The total for all CPUs is given at the top, while each individual CPU is listed below with its own statistics. The following example is a 4-way Intel Pentium Xeon configuration with multi-threading enabled, therefore showing four physical processors and four virtual processors totaling eight processors. page — The number of memory pages the system has written in and out to disk. swap — The number of swap pages the system has brought in and out. intr — The number of interrupts the system has experienced. btime — The boot time, measured in the number of seconds since January 1, 1970, otherwise known as the epoch. ctxt — The number of context switches. processes — The total number of processes. procs_running — The number of processes in the run queue. procs_blocked — The number of processes in the wait queue. softirq — The number of softirqs the system has experienced. /proc/swaps # This file shows the information about the swap.\n/proc/uptime # The first value represents the total number of seconds the system has been up. The second value is the sum of how much time each core has spent idle, in seconds. Consequently, the second value may be greater than the overall system uptime on systems with multiple cores.\n/proc/version # This file shows the version of the kernel. Also the gcc version.\nOther files # /proc/diskstats /proc/dma /proc/execdomains /proc/fb /proc/interrupts /proc/kallsyms /proc/kcore /proc/kmsg /proc/locks /proc/mdstat /proc/mounts /proc/modules /proc/partitions /proc/pci /proc/slabinfo /proc/vmstat /proc/zoneinfo "},{"id":11,"href":"/posts/the-iftop/","title":"The Iftop","section":"Blog","content":"I frequently use ifconfig and cat /proc/net/dev to examine network information. However, today, my cloud server crashed unexpectedly. I reached out to the cloud support for assistance. The engineer utilized iftop to analyze the situation. After the issue was resolved, I started to consider using this tool. Now, let\u0026rsquo;s take a closer look at this powerful utility.\nFor more info about /proc you can check another blog Further Understanding of Proc.\nThe reason why I choose the iftop is simple, if the alicloud Inc. selects the iftop, which indicates the tool is somehow suitable for general analysis and common situtation. After all, those with experience knows better.\niftop must be run with sufficient permissions to monitor all network traffic on the interface, which is root for most os.\nYou can refer the official docs: iftop\nThe display # The top part is the bar graph of the bandwidth. You can adjust it by -m parameter.\nThe main part of the display lists, for each pair of hosts, the rate at which data has been sent and received over the preceding 2, 10 and 40 second intervals. The direction of data flow is indicated by arrows, \u0026lt;= (receive) and =\u0026gt; (send).\nAt the bottom of the display, various totals are shown, including total traffic transferred (after filtering), peak traffic over the last 40s, and total transfer rates averaged over 2s, 10s and 40s.\n2.00Mb 4.00Mb 6.00Mb 8.00Mb 10.0Mb └───────────────┴───────────────┴───────────────┴───────────────┴─────────────── foo.example.com =\u0026gt; bar.example.com 1Kb 500b 100b \u0026lt;= 2Mb 2Mb 2Mb TX: cum: 43.5MB peak: 4.21Mb rates: 4.15Mb 4.08Mb 4.00Mb RX: 1.46MB 192Kb 192Kb 162Kb 136Kb TOTAL: 45.0MB 4.38Mb 4.34Mb 4.24Mb 4.13Mb The options # For more options you should check the official docs, here I will list some common usage:\niftop -nP: -n means Don\u0026rsquo;t do hostname lookups. -P means Turn on port display. iftop -B: -B means Display bandwidth rates in bytes/sec. iftop -l: -l means IPv6 addresses(default not include). iftop -m 10M specific the maximum of the bar graph. iftop -i wlan0 -f \u0026quot;dst port 22\u0026quot;: -i you can specific the network interface and -f you can specific some filters. Other filters such as dst host 10.10.8.8, src port 443, dst portrange 22-23 and gateway 10.10.8.1. The operation when running iftop # You can just press h when running iftop to check every operation.\n"},{"id":12,"href":"/posts/reflections-on-trending-topics/","title":"Reflections on Trending Topics","section":"Blog","content":"一次制造热点帖子引发的思考与总结。\n最近我在 v2ex 制造了一个热点帖子，起因是我当天中午发现我常用的 cursor 突然无法自动补全了，我先排查了网络，发现没有异常，为了方便进一步排查问题，我 logout 了我的账户，并且尝试注册一个新账户，以此来排查异常是否有关我的账户。但是在注册的时候，发现页面导向了 500 server error，账号没有办法注册，因此我推断 cursor 内部的服务 down 了。\n于是在 v2ex 先发了个帖子，估计大家都遇到了类似的问题，短时间内浏览量激增，随后我发现 cursor 官方的 forum 也有大批用户反应服务的故障，之后我在我的帖子底下同步了这一消息，结果很快就有用户来骂我标题党。我看到评论的瞬间反思了一下，确实可能标题表达地不够准确，应该把“已经”换成“现在”，强调一个暂时的状态。\n在无法继续使用 cursor 以后，我想起我的老师前几天向我推荐的 trae，于是我决定试一试，产品还不错，除了 tab 确实完成度不如 cursor 以外，其他没有发现太大的缺点。因此我想把 trae 推广给同样暂时无法使用 cursor 的人。因此重新发了一个推荐 trae 的帖子，结果超出了我的预期。\n我反思总结了这个帖子会成为 trending topic 的几个要素：\n话题紧跟时事 # 话题紧跟时事，现在辅助编程工具 cursor 已经成为了当下的 hit，就从各个流媒体平台的话题数量来看，我大概估计 cursor 在国内市场占有率应该不低，估计应该有 20%，因此 cursor 的每一个话题都可能会引起开发者的广泛关注。\n话题降低参与的门槛 # cursor 是一个辅助编程工具，因此并不需要极其深度的知识，无论是专业开发者还是相关行业的人都能参与到话题的讨论中，后续的结果也印证了这一点。\n让每一个人有参与感的宣传就是好的宣传。这也是我在日常生活中发现到的，我每顿吃饭都有看 36氪，晚点，量子位，机器之心以及IT 之家的习惯，通常越专业的报道看的人越少，评论的人也就越少。从评论区就能看到网民对哪些内容更关注，通常前面报道哪个企业哪项业务有所突破，哪家企业又获得哪轮融资，硅谷又推出的新产品新算法的突破，最近市场关注的焦点是什么，最近创新的新技术有哪些？底下往往应者了了。而读者又喜欢评论哪些内容呢？据我观察，分为两类，一类是和网民利益强相关的，例如放假安排等等，一类就是八卦闲话，哪哪车企困难，哪个人打假主播，还有乱七八糟的吃瓜内容。太阳底下没有新鲜事，通常我所喜欢的科技发展，技术突破，新产品或者某个行业新趋势，并不受到广泛人的喜欢和关注。\n这让我对平台的运营产生了思考，一个平台想要良好的发展，重要的是把握内容的度。如果一个内容分发者广泛发一些乱七八糟的内容，一定会让独立思考或者专业的人反感。但是一个平台如果只分发专业深度的内容，上限又非常低，不太会有广泛的受众。\n选到了合适的时间 # 当时正是周五下午，并且即将到来的是春节的假期，因此可能很多人可能工作不忙，论坛的在线人数也间接印证了这一点，比平时能有 20% 的涨幅。因此在此时发布往往会有一个好的曝光，潜在能参与讨论的人也相应会增长。\n这也是我跟自媒体的朋友学到的，通常作为流媒体的消费者，观众可能不太会意识到其中的规律。通常一个生产者在不同的时候发布会潜在有一个不同的浏览量差异。据我观察多数自媒体账号都会选择在工作日的午饭或者晚饭期间发布，如果错过了就在晚上九点左右发布，一般不会过早也不会过晚，可能是统计或者预测对应用户行为的结果。因为往往这个时候用户浏览流媒体等平台的可能性更大。\n好的标题决定上限 # 标题起的能引起人的关注，一个好的标题和封面会给浏览量带来巨大的效应，它的权重甚至有时能超过内容。在移动互联网时代，观众所经常被吸引的图文内容或者视频内容，经常被人总结为图文或者视频的“hook”，起初看到这样的内容我还觉得很好笑，什么都能总结成学问，但是后来认真分析意识到，不可否认这就是移动互联网时代的学问。因此一个好的标题会将内容分发的上限在无形中提高。\n因此如果一个标题总是能够吸引一定的曝光，一定有其对应的原因，本着“好奇”与“对比”的原则，我起的帖子标题如下：尝试了 trae 后，我为 cursor 续费的心有点动摇了\u0026hellip;\n通过这一次实践，我也终于知道为什么通常发布会（例如手机，汽车）都会有一个对标的产品了，一个对标的产品通常会解决以下几个问题：\n快速解决受众认知上的问题，例如 su7 对标 model 3，将一个新产品和行业标杆对比，天然解决了受众对产品类型，产品定位，产品价值的潜在认知流程。反之，如果是一个完全新独一无二的产品，可能会有一个非常漫长的解决用户认知的问题。 通过对比其优势，在不同维度给受众以心理暗示，最后使产品有非常强的竞争力。这也是为什么科技公司大模型都喜欢打榜的原因。 合适的内容支撑 # 除了一个吸引人的标题，也应该有一个适合的内容作为话题引起的支撑，我简要地对比了 trae 和 cursor 的优缺点，让用户有引起讨论的入口，给用户一些基本的材料，从而让用户能更好地结合自己的经历或者见解有所表达。\n为什么是合适的？\n很多人都对广告有抵触心理，因此好的广告不能让人意识到它是广告。通常内容都是夸赞，可能引起的效果完全相反。大众总是有各种的“逆反”心理，你要是宣传的越完美，就越有网民拿着放大镜挑问题，然后在底下大骂特骂，有时这种评判的声音可以完全盖过正面的宣传。而读者或者观众只会注意并且相信这些批评的声音（通常认为产品好的人并不会费时费力地为你写出称赞的评价），最终 The road to hell is paved with good intentions，结果与本意往往相反。因此好的宣传内容一定是合适的，以一种相对理性的方式表达，并最好能引导受众讨论，往往讨论里一来一回有积极有消极，有起有伏，更能让受众留步驻足，有时你甚至需要主动提出自己的缺点，给受众调侃的入口，从而引起有来有回的讨论，这相比全面赞扬更能激发受众的好奇尝试与探索。\n总结 # 一次关于媒体平台的初步实践，发出去的当时我预料帖子可能会成为交流讨论的 trending topic，结果规模远超出预期，截止目前也就是一天以后，帖子获得 6k 多次点击，4.5k 注册用户的查看，这对一个通常在线只有 2k 左右的平台应该算是一个不错的曝光了，这也印证了我在年初的思考，Thinking About Advertisement from an Open Source Perspective 广告是一个持续不断曝光的过程。一个科学的方法收到了不错的反馈。\n听说当晚 trae 官方 Community 里的不少人都是看我的帖子被推荐过来的，trae 能不能给我发点广告费 ;D\n"},{"id":13,"href":"/posts/cpu-can-only-see-the-threads/","title":"CPU can only see the threads","section":"Blog","content":"In python, due to the GIL (Global Interpreter Lock), which is a mutex and ensures only one thread can execute at a time, so the multiple threads parallel execution is not supported under the CPython interpreter. But what about the multiple processes? What is the difference between them? How to choose the right method? Do you know the coroutine? Let\u0026rsquo;s explore it together.\nPrerequisite # To begin with, you should know the basic conceptions:\nProcess: The process is the resource allocation unit. Thread: The thread is the minimum unit of CPU scheduling. For every process, the actually execution unit is the main thread in the process. Hence, even in different processes, the CPU can only see the threads.\nThe core of the computer is the number of physical cores that can be parallelized at the same time (the CPU can only see the threads). Due to the hyper-threading technology, the actual number of threads that can be parallelized is usually twice the number of physical cores, which is also the number of cores seen by the operating system. We only care about the number of threads that can be parallelized, so the number of cores mentioned later is the number of cores seen by the operating system, and the core refers to the core after the hyper-threading technology (not the physical core).\nIf the computer has multiple CPU cores, and the total number of threads in the computer is less than the number of cores, then the threads can be parallelized and run on different cores. If it is single-core multi-threading, then multi-threading is not parallel, but concurrent, that is, to balance the load, the CPU scheduler will switch different threads on a single core. If it is multi-core multi-threading, and the number of threads is greater than the number of cores, some threads will keep switching, and execute concurrently, but the maximum number of parallel execution is actually the number of cores in the current process, so blindly increasing the number of threads will not make your program faster, but will add extra overhead to your program. Process # The process is the resource allocation unit: An process has its own independent running space (including the text region, data region, and stack region). Every process has its own independent memory space, which ensures the isolation of memory address spaces between processes. Process contains the procedure, data set and PCB(Process Control Block). The related resources will be recorded in the PCB which indicates the resources are occupied. A process is the scheduling unit for seizing the processor. The switching of processes requires saving and restoring the CPU state. Multiple processes cannot communicate with each other directly. They need a inter-process communication (IPC) mechanism to communicate with each other, which there will be a pipe(in parent-child process) or named pipe or signal or message queue or shared memory(the most efficient way) or socket(even can be used in different machines). Cons:\nIf changes the processes frequently, it will consume more resources. Around GB level. Thread # Thread is a lightweight process, which is minimum unit of CPU scheduling. Multiple threads share the same memory space of the process. So the threads can communicate with each other directly. The scheduling of threads is smaller than process. The stack is KB level. Thread is not related to resource allocation. It belongs to a process and shares the resources of the process with other threads in the process. Threads consist only of the related stack (system stack or user stack), registers, and the thread control block (TCB). Registers can be used to store local variables within a thread, but cannot store variables related to other threads. Other comparsion # The process is independent of each other, if one process is crashed, it will not affect other processes. But the thread is not independent of each other, if one thread is crashed, it will affect the whole process crashed. The process uses memory addresses that can be locked, that is, when one thread uses some shared memory, other threads must wait for it to finish before they can use this memory. (mutex) Choose # For the CPU intensive tasks, recommend to use multiple processes. For the IO intensive tasks, recommend to use multiple threads. eg, scratch the web, when IO is blocking in system call, the thread can release the GIL and let CPU run other threads. Common usage: For the Webserver, which needs to create or close the connection frequently, better use the multiple thread. For the strong relationship between the data, better use the multiple threads. Context switching # The context switching contains the following three types:\nProcess context switching: Switching between different processes. Task scheduling adopts a preemptive method of time slice rotation for process scheduling. Thread context switching: Switching between different threads. User mode and kernel mode context switching: Switching between user mode and kernel mode. (when user program needs to call hardware devices, the kernel needs to switch the user program to the system call) The context switching contains the following steps:\nSwitch page to use new address space (only for process switching) Switch kernel stack and hardware context: The main difference between thread context switching and process context switching is that the virtual memory space of thread switching is the same, but the process switching is different (so the process switching is most expensive). The most cost is the switch of register content. How to just the bottle neck of context switching? If CPU is running at full load, it should meet the following distribution:\nUser Time: 65%～70% System Time: 30%～35%(if too high, it means the context switching is too frequent) Idle: 0%～5% Multiple threads # In other languages, multiple threads can run in multiple cores. But in python, multiple threads can only run in one core at the same time.\nIn python, there is a GIL (Global Interpreter Lock) which only allows one thread to execute in the CPU. And the python has different interpreters, such as:\nCPython: the official implementation which is written in C. Jython: compile the python code to java bytecode and run it in JVM. IronPython: .NET platform. PyPy: RPython. Only the CPython has the GIL, and the process of releasing the lock is time-comsuming. So in the multiple cores the performance of the Python is not good.\nThe official docs: This lock is necessary mainly because CPython’s memory management is not thread-safe. (However, since the GIL exists, other features have grown to depend on the guarantees that it enforces.)\nIn other words, if there is no GIL, probably the garbage collection mechanism will directly recycle the variables of the code that is being executed by multiple threads, which will cause the program to crash. So the GIL is added in the CPython to ganruntee the security of thread\u0026rsquo;s level. Only the thread obtains the GIL first, then to obtains the CPython interpreter.\nWhy only CPython has the GIL? # The Python is interpret language, so it cannot know the global of whole code. Only has the GIL can access to the CPython.\nBut Java is a hybrid language, so in the Jpython there is no GIL.\nCompiled languages: the compiler make the code to the binary bytescode which can run on the machine directly. But you should compile it into different platform binary executable. Some common languages: C, C++, go, Pascal, Object-C and Swift.\nInterpreted language: First, translate it into intermediate code, and then let the interpreter interpret and run the intermediate code. The source program is translated into machine code during the running process. One sentence is translated and then executed until the end. It has good platform compatibility and can run in any environment, provided that an interpreter (virtual machine) is installed. Some common languages: JavaScript, Python, Erlang, PHP, Perl, Ruby.\nHybrid language: In C#, instead of directly compiling into machine code during compilation, it compiles into intermediate code. The.NET platform provides a Common Language Runtime (CLR) to run the intermediate code. The CLR is similar to the Java Virtual Machine (JVM). After C# code is compiled into Intermediate Language (IL) code, it is saved in a DLL. When the program runs for the first time, the Just-In-Time (JIT) compiler compiles the IL code into machine code and caches it in memory, and subsequent executions can directly use this cached machine code. As for Java, it can be considered a compiled language. All Java code needs to be compiled; a .java file is useless without compilation. Regarding the efficiency of the JVM, there are optimization techniques such as JIT and Ahead-Of-Time (AOT). Java can also be considered an interpreted language. Since Java code cannot run directly after compilation, it is interpreted and run on the JVM, so it is interpreted.\nWhat is thread security? # The thread security is mainly about the memory security.\nIn the memory space of each process, there is a special common area, usually called the heap (memory). All threads within the process can access this area, which is the potential cause of problems. In the absence of a protection mechanism, the heap memory space is an unsafe area for multithreading. This is because the data you put into it may be \u0026ldquo;adjusted\u0026rdquo; by other threads.\nThe multiple thread in python # So in the python, we can use the threading module to create the thread.\n# directly usage import threading def run(n): print(\u0026#34;current task：\u0026#34;, n) if __name__ == \u0026#34;__main__\u0026#34;: t1 = threading.Thread(target=run, args=(\u0026#34;thread 1\u0026#34;,)) t2 = threading.Thread(target=run, args=(\u0026#34;thread 2\u0026#34;,)) t1.start() t2.start() # customize class usage import threading class MyThread(threading.Thread): def __init__(self, name): super(MyThread, self).__init__() # initialize the parent class self.name = name def run(self): print(\u0026#34;current task：\u0026#34;, name) if __name__ == \u0026#34;__main__\u0026#34;: t1 = MyThread(\u0026#34;thread 1\u0026#34;) t2 = MyThread(\u0026#34;thread 2\u0026#34;) t1.start() t2.start() # Let the main thread wait for the sub-thread to complete if __name__ == \u0026#34;__main__\u0026#34;: t1 = threading.Thread(target=count, args=(\u0026#34;100000\u0026#34;,)) t2 = threading.Thread(target=count, args=(\u0026#34;100000\u0026#34;,)) t1.start() t2.start() # use .join() to wait for the sub-thread to complete # if the t1 is very time-comsuming, you can set the timeout # t1.join(timeout=5) to prevent the t2.join() from being blocked t1.join() t2.join() # The Daemon thread is opposite to join(), it will ensure the sub-thread follows the main thread t1.setDaemon(True) t1.start() Timer\ndef show(): print(\u0026#34;Pyhton\u0026#34;) t = threading.Timer(1, show) # set the thread start 1 second later t.start() But how to let each thread use their own data? Use the threading.local() to create the thread-local storage.\nimport threading # Create the global object local_school = threading.local() def process_student(): std = local_school.student print(\u0026#39;Hello, %s (in %s)\u0026#39; % (std, threading.current_thread().name)) def process_thread(name): # Bound the object separately local_school.student = name process_student() t1 = threading.Thread(target= process_thread, args=(\u0026#39;Alice\u0026#39;,), name=\u0026#39;Thread-A\u0026#39;) t2 = threading.Thread(target= process_thread, args=(\u0026#39;Bob\u0026#39;,), name=\u0026#39;Thread-B\u0026#39;) t1.start() t2.start() t1.join() t2.join() Thread Synchronization and Lock # # create the Lock mutex = threading.Lock() # Allow the the thread obtain the lock again when it already holds the lock # Same methods as the Lock Reentrant = threading.RLock() # obtain the lock mutex.acquire([timeout]) # release the lock mutex.release() # allow multiple threads to obtain the lock semaphore = threading.BoundedSemaphore(5) # acquire the lock semaphore.acquire([timeout]) # release the lock semaphore.release() The multiple process in python # The usage is similar to the thread.\n# similar usage, omit here from multiprocessing import Process def show(name): print(\u0026#34;Process name is \u0026#34; + name) if __name__ == \u0026#34;__main__\u0026#34;: proc = Process(target=show, args=(\u0026#39;subprocess\u0026#39;,)) proc.start() proc.join() Queue # from multiprocessing import Process, Queue def put_in_queue(queue): queue.put(\u0026#39;Queue\u0026#39;) # put() means put the data into the queue if __name__ == \u0026#39;__main__\u0026#39;: queue = Queue() pro = Process(target=put_in_queue, args=(queue,)) pro.start() print(queue.get()) # get() means get the data from queue pro.join() # Common usage Queue.qsize() Queue.empty() Queue.full() Queue.get([block[, timeout]]) Queue.get_nowait() # == Queue.get(False) Queue.put() Queue.put_nowait(item) # == Queue.put(item, False) Queue.task_done() # send a signal to the queue Queue.join() # wait until the queue is empty(similar to thread.join()) Pipe # The essence is communication rather than share the data. Just likes the socket.\nMain use: send() and recv()\nfrom multiprocessing import Process, Pipe def show(conn): conn.send(\u0026#39;Pipe\u0026#39;) conn.close() if __name__ == \u0026#39;__main__\u0026#39;: parent_conn, child_conn = Pipe() pro = Process(target=show, args=(child_conn,)) pro.start() print(parent_conn.recv()) pro.join() Manager # https://docs.python.org/3.6/library/multiprocessing.html#managers\nPool # apply(): synchronous(serial) blocking apply_async(): asynchronous(parallel) not blocking terminate(): terminate the pool close(): wait for the pool to complete then close the pool join(): must add after close() or terminate() #coding: utf-8 import multiprocessing import time def func(msg): print(\u0026#34;msg:\u0026#34;, msg) time.sleep(3) print(\u0026#34;end\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: # the pool is 3 # only 3 processes is running at the same time pool = multiprocessing.Pool(processes = 3) for i in range(5): msg = \u0026#34;hello %d\u0026#34; %(i) # non-blocking, the sub-process will not affect the main process execution, main process will run to pool.join() pool.apply_async(func, (msg, )) # blocking, first execute the sub-process, then execute the main process # pool.apply(func, (msg, )) print(\u0026#34;=================\u0026#34;) # before calling join, you must call close function, otherwise it will be wrong. pool.close() # after calling close, there will be no new process added to the pool # join function will wait for all sub-processes to complete # !important: if not join, there will be 5 zombie processes pool.join() print(\u0026#34;Sub-process(es) done.\u0026#34;) Coroutine # When async operations share the resources, the coroutine which defined by user based on time slice is better than threads based on Preemptively. The coroutine has its own register context and stack, which is more lightweight than the thread, the stack is KB level. In python, the coroutine is single thread, which means there only be one coroutine running at the same time, there won\u0026rsquo;t be conflict in variable. So the coroutine is safe.\nimport asyncio balance = 0 async def change_it_without_lock(n): global balance balance = balance + n # Besides the time slice, you can also use the `await` to surround the \u0026#34;right\u0026#34; actively. await asyncio.sleep(1) balance = balance - n print(balance) # if you call the `change_it_without_lock()` function directly, the return is a coroutine object. # so if you want to run it directly, you should use the `await` or `asyncio.run(function)` loop = asyncio.get_event_loop() # get the event loop res = loop.run_until_complete( asyncio.gather(change_it_without_lock(10), change_it_without_lock(8), change_it_without_lock(2), change_it_without_lock(7))) print(balance) # 17 # 9 # 7 # 0 # 0 # As we can see above, the process is not consistent, but the result is consistent. # that is easy to understand, the 10 is +10 but await the sleep, then the 8 is running, and the same until 10+8+2+7=27, then the 10 sleep is over, then -10, print 17, and recursively 9 7 0, finaly is 0. All in all, if the coroutine is not use await to surround the \u0026ldquo;right\u0026rdquo; actively, the coroutine is safe. If you use await, the result will be consistent. If you want to make the process consistent, you can only use the Lock. But in this situation, the async will be sync.\nReference # https://stackoverflow.com/questions/49090416/why-is-the-python-interpreter-not-thread-safe\nhttps://www.cnblogs.com/lizexiong/p/17141988.html\nhttps://liaoxuefeng.com/books/python/process-thread/process-manager/index.html\nhttps://www.cnblogs.com/v3ucn/p/16530665.html\nhttps://zhuanlan.zhihu.com/p/82123111\n"},{"id":14,"href":"/posts/go-common-test/","title":"Go Common Test","section":"Blog","content":"The test methods in Go mainly include three types: unit test, benchmark test, and example test.\nFirst of all:\nYou should import the testing package first. In the package directory, all source code files with the suffix *_test.go are part of the go test test, and will not be compiled into the final executable file by go build. Unit Test # The basic format/signature is:\nfunc TestXxx(t *testing.T) { // ... } Put the _test.go and .go in the same dir, and run go test -v to execute the test detailed, the go test -cover to check the coverage of the test.\nBenchmark test # The benchmark test will not be excuted by default, you need to use go test -bench=Xxx to execute the benchmark test.\nThe basic format/signature is:\nfunc BenchmarkXxx(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { // ... } } The b.N is the number of times the test is run, and the b.N is automatically adjusted by the go test tool.\nBenchmarkSplit-8 10000000 215 ns/op The -8 means the test is run on the 8 GOMAXPROCS The 10000000 means the test is run 10000000 times. The 215 ns/op means the test takes 215 nanoseconds average per operation. go test -bench=Xxx -benchmem can check the memory allocation and the number of allocations per operation.\nBenchmarkSplit-8 10000000 215 ns/op 112 B/op 3 allocs/op The 112 B/op means the test allocates 112 bytes per operation. The 3 allocs/op means the test allocates 3 times per operation. Performance comparison # // note: the n is the parameter of Fib not the b.N, the b.N is the number of times the test is run. it will be automatically adjusted by the go test tool. func benchmarkFib(b *testing.B, n int) { for i := 0; i \u0026lt; b.N; i++ { Fib(n) } } func BenchmarkFib1(b *testing.B) { benchmarkFib(b, 1) } func BenchmarkFib2(b *testing.B) { benchmarkFib(b, 2) } func BenchmarkFib3(b *testing.B) { benchmarkFib(b, 3) } go test -bench=. will execute the benchmark test for all functions with the prefix Benchmark.\nfunc BenchmarkSplit(b *testing.B) { time.Sleep(5 * time.Second) // some time-consuming operations b.ResetTimer() // reset the timer(ignore the time above) for i := 0; i \u0026lt; b.N; i++ { // some time-consuming operations } } // you can also use this method func BenchmarkSplit(b *testing.B) { b.StopTimer() // some time-consuming operations b.StartTimer() for i := 0; i \u0026lt; b.N; i++ { // some time-consuming operations } } Parallel test # Signature:\nfunc BenchmarkSplitParallel(b *testing.B) { // b.SetParallelism(1) // set the number of CPU to use b.RunParallel(func(pb *testing.PB) { for pb.Next() { // some time-consuming operations } }) } Example test # Signature:\nfunc ExampleName() { // No parameters and no return } "},{"id":15,"href":"/posts/go-concurrency-and-parallelism/","title":"Go Concurrency and Parallelism","section":"Blog","content":" Process and Thread # The process is the program execution process in the operating system, and it is an independent unit for resource allocation and scheduling.\nThe thread is the execution entity of the process, the basic unit for CPU scheduling and distribution, and it is a smaller unit that can run independently.\nA process can create and cancel multiple threads; multiple threads in the same process can execute concurrently.\nConcurrency and Parallelism # Concurrency is multiple threads running on the same CPU core.\nParallelism is multiple threads running on multiple CPU cores.\nGoroutine # A thread can run multiple goroutines, and a goroutine is a lightweight thread, which is managed by the Go runtime, independent stack space, shared heap space, scheduling controlled by the user, essentially similar to user-level threads. The cost of routine is KB level which is much lower than thread MB level.\ngoroutine is just a super \u0026ldquo;thread pool\u0026rdquo; implemented by the official. The 4~5KB stack memory occupied by each instance and the greatly reduced creation and destruction overhead due to the implementation mechanism are the fundamental reason for Go\u0026rsquo;s high concurrency.\nA goroutine must correspond to a function, you just need to add the go keyword before the function.\nfunc hello() { fmt.Println(\u0026#34;Hello Goroutine!\u0026#34;) } func main() { go hello() // use goroutine fmt.Println(\u0026#34;main goroutine done!\u0026#34;) } // output: // main goroutine done! // why? the main goroutine life cycle is over, the program will exit. // this only for the main goroutine, so for the other goroutine, if return, the sub-routine can still running. Use sync.WaitGroup to wait all goroutine done.\nvar wg sync.WaitGroup func hello(i int) { defer wg.Done() // goroutine done, add -1 fmt.Println(\u0026#34;Hello Goroutine!\u0026#34;, i) } func main() { for i := 0; i \u0026lt; 10; i++ { wg.Add(1) // add a goroutine go hello(i) } wg.Wait() // wait all goroutine done } "},{"id":16,"href":"/posts/go-cheatsheet/","title":"Go Cheatsheet","section":"Blog","content":" syntax # The uppercase of the first letter of const, var, func, type is exported. And the lowercase is private. The { cannot be as a separate line. Not need to use ; to end the statement.(Not recommend) // or /* */ can be used to comment. the string can be connected by + fmt.Printf is used to print the string format. dependency # go get # go env # GOROOT=\u0026#39;/usr/local/go\u0026#39; // the standard library # GOPATH=\u0026#39;/Users/username/go\u0026#39; // the project lib or three-party lib go get will download the three-party lib to the GOPATH/src directory. The go search GOROOT first, then search GOPATH. But the denpendency only can keep one version, which will cause some conflict.\ngo vendor # Save all the denpendency in the vendor directory of the project, which make the project more redundant. The go will search the vendor first. This still cannot solve the problem of version.\ngo module # It was introduced in go1.11, which can manage the denpendency more flexibly.\ngo mod init first initialize the project denpendency. go mod tidy write the using denpendency to the go.mod file and remove the unused denpendency in the file. go mod download or go build or go test, the three-party lib will be downloaded to the GOPATH/pkg/mod directory. indirect means the sub-denpendency. +incompatible means compatible with the old version without the go.mod file.\nIf the project different package needs the same denpendency\u0026rsquo;s different version, it will choose the minimum compatible version.\ndenpendency distribution # Normally use the GOPROXY='https://proxy.golang.org,direct' search the proxy first, if not found, then search the original source.\ndata type # Int float..: default is 0 String: default is \u0026quot;\u0026quot; Bool: default is false Below are all nil.\nvar a *int var a []int var a map[string] int var a chan int var a func(string) int var a error var # // case1 var identifier1, identifier2 type // usually used in global variable var ( vname1 v_type1 vname2 v_type2 ) // case2 only can be used in function intVal := 1 // you cannot declare the variable twice // which is equivalent to var intVal int = 1 var intVal int intVal =1 // case3 auto var v1name, v2name = value1, value2 The int, float, bool and string are basic value type, using these types of variables directly save the value in memory, if use such as i = j, then actually copy the value of j to i in memory. \u0026amp;i is used to get the address of i in memory. When the i is local variable, the address is in the stack, when the i is global variable, the address is in the heap.\nThe complex type is a pointer, slice, map, channel, function, interface, struct, they are usually used as reference type. The reference type usually save the address of the value in memory, if use such as i = j, then actually copy the address of j to i in memory. So if j changes reference value, i will also change.\nYou cannot declare a var but not use it in function. Except the global variable.\nYou can use a, b = b, a to swap the value of a and b.(Must be the same type)\nUse _ to drop value, such as _, b = a, b\nconst # // type only boolen, string, number const identifier [type] = value You can use the build-in function to operate the const.\nimport \u0026#34;unsafe\u0026#34; const ( a = \u0026#34;abc\u0026#34; b = len(a) c = unsafe.Sizeof(a) ) iota # special const, default is 0, every time add one const, the iota will add one.\nAnd in the const block, if you don\u0026rsquo;t assign the value obviously, it will inherit the last value.\nfunc main() { const ( a = iota //0 b //1 c //2 d = \u0026#34;ha\u0026#34; //\u0026#34;ha\u0026#34; iota += 1 e //\u0026#34;ha\u0026#34; iota += 1 f = 100 //100 iota +=1 g //100 iota +=1 h = iota //7 i //8 ) } operator # \u0026amp;: get the address of the variable.\npointer variable: get the value of the address. package main import \u0026#34;fmt\u0026#34; func main() { var a int = 4 var b int32 var c float32 var ptr *int ptr = \u0026amp;a fmt.Printf(\u0026#34;a is %d\\n\u0026#34;, a); // a is 4 fmt.Printf(\u0026#34;*ptr is %d\\n\u0026#34;, *ptr); // *ptr is 4 } if # if num := 9; num \u0026lt; 0 { fmt.Println(num, \u0026#34;is negative\u0026#34;) } else if num \u0026lt; 10 { fmt.Println(num, \u0026#34;has 1 digit\u0026#34;) } else { fmt.Println(num, \u0026#34;has multiple digits\u0026#34;) } switch # switch var1 { case val1: // do something case val2: // do something default: // do something } switch x.(type){ case type: statement(s); case type: statement(s); default: // optional statement(s); } // the fallthrough will execute the next case forcefully. func main() { switch { case false: fmt.Println(\u0026#34;1\u0026#34;) fallthrough case true: fmt.Println(\u0026#34;2\u0026#34;) fallthrough case false: fmt.Println(\u0026#34;3\u0026#34;) fallthrough case true: fmt.Println(\u0026#34;4\u0026#34;) case false: fmt.Println(\u0026#34;5\u0026#34;) fallthrough default: fmt.Println(\u0026#34;6\u0026#34;) } } // 2 3 4 select # select only can be used in channel, each case must be a channel operation, either send or receive.\nselect { case \u0026lt;-ch1: // do something case value := \u0026lt;- channel2: // do something case channel3 \u0026lt;- value: // do something default: // do something } select statement will listen to all the channel operations, once one channel is ready, then execute the corresponding code block. If multiple channels are ready, then the select statement will randomly select one channel to execute. If all channels are not ready, then execute the code in the default block. if there is no default, select will block until one channel is ready. Go will not re-evaluate the channel or value.\nfor # for n := 0; n \u0026lt; 5; n++ { if n%2 == 0 { continue } fmt.Println(n) } // infinite loop for { fmt.Println(\u0026#34;loop\u0026#34;) break } array # It is fixed length, which is not common used.\nvar twoD [2][3]int for i := 0; i \u0026lt; 2; i++ { for j := 0; j \u0026lt; 3; j++ { twoD[i][j] = i + j } } slice # func main() { s := make([]string, 3) s[0] = \u0026#34;a\u0026#34; s[1] = \u0026#34;b\u0026#34; s[2] = \u0026#34;c\u0026#34; fmt.Println(\u0026#34;get:\u0026#34;, s[2]) // c fmt.Println(\u0026#34;len:\u0026#34;, len(s)) // 3 s = append(s, \u0026#34;d\u0026#34;) // must be received back s = append(s, \u0026#34;e\u0026#34;, \u0026#34;f\u0026#34;) fmt.Println(s) // [a b c d e f] c := make([]string, len(s)) copy(c, s) fmt.Println(c) // [a b c d e f] fmt.Println(s[2:5]) // [c d e] fmt.Println(s[:5]) // [a b c d e] fmt.Println(s[2:]) // [c d e f] good := []string{\u0026#34;g\u0026#34;, \u0026#34;o\u0026#34;, \u0026#34;o\u0026#34;, \u0026#34;d\u0026#34;} fmt.Println(good) // [g o o d] } map # func main() { m := make(map[string]int) m[\u0026#34;one\u0026#34;] = 1 m[\u0026#34;two\u0026#34;] = 2 fmt.Println(m) // map[one:1 two:2] fmt.Println(len(m)) // 2 fmt.Println(m[\u0026#34;one\u0026#34;]) // 1 fmt.Println(m[\u0026#34;unknow\u0026#34;]) // 0 r, ok := m[\u0026#34;unknow\u0026#34;] fmt.Println(r, ok) // 0 false delete(m, \u0026#34;one\u0026#34;) m2 := map[string]int{\u0026#34;one\u0026#34;: 1, \u0026#34;two\u0026#34;: 2} var m3 = map[string]int{\u0026#34;one\u0026#34;: 1, \u0026#34;two\u0026#34;: 2} fmt.Println(m2, m3) } range # func main() { nums := []int{2, 3, 4} sum := 0 for i, num := range nums { sum += num if num == 2 { fmt.Println(\u0026#34;index:\u0026#34;, i, \u0026#34;num:\u0026#34;, num) // index: 0 num: 2 } } fmt.Println(sum) // 9 m := map[string]string{\u0026#34;a\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;b\u0026#34;: \u0026#34;B\u0026#34;} for k, v := range m { fmt.Println(k, v) // b 8; a A } for k := range m { fmt.Println(\u0026#34;key\u0026#34;, k) // key a; key b } } func # not support nested not support overload not support default parameter return can only be received by multiple values x, _ cannot use a container to receive // func name(parameter-list) (return-list) func add(a int, b int) int { return a + b } func add2(a, b int) int { return a + b } func exists(m map[string]string, k string) (v string, ok bool) { v, ok = m[k] return v, ok } func main() { res := add(1, 2) fmt.Println(res) // 3 v, ok := exists(map[string]string{\u0026#34;a\u0026#34;: \u0026#34;A\u0026#34;}, \u0026#34;a\u0026#34;) fmt.Println(v, ok) // A True } func myfunc(args ...interface{}) { // pass any type of parameter } func add(a int, b int, args…int) int { //2 parameters or more // the args is a slice you can use it via args[index] } //eg. func test(s string, n ...int) string { var x int for _, i := range n { x += i } return fmt.Sprintf(s, x) } func main() { s := []int{1, 2, 3} res := test(\u0026#34;sum: %d\u0026#34;, s...) // slice... (expand slice) println(res) } lambda function # In go, the lambda function is a closure.\ngetSqrt := func(a float64) float64 { return math.Sqrt(a) } fmt.Println(getSqrt(4)) The function is a first-class citizen in the language. Generally, a function returns another function, which can reference the local variables of the outer function, forming a closure.\nUsually, a closure is implemented through a structure, which stores a function and an associated context environment. But in Go, the anonymous function is a closure, which can directly reference the local variables of the outer function.\nclosure # package main import \u0026#34;fmt\u0026#34; func add(base int) func(int) int { return func(i int) int { base += i return base } } func main() { tmp1 := add(10) // base is 10; // tmp1 is returned func(i) // in fact the tmp1 is a `FuncVal` object, FuncVal { func_address, closure_var_pointer ... } which contains: // 1. the anonymous function address // 2. the closure object pointer, which points to the closure object that contains the required external variables. // the function call usually involves passing parameters to the function and jumping to the function\u0026#39;s entry address for execution. // **then FuncVal object is obtained, and the anonymous function address and the closure object pointer are obtained**, completing the function call and correctly accessing the required external variables. // the closure is indirectly expanded the life cycle of function. fmt.Println(tmp1(1), tmp1(2)) // 11 13 // this tmp2 is a new entity tmp2 := add(100) fmt.Println(tmp2(1), tmp2(2)) // 101 103 } defer # The keyword defer is used to register a deferred call. These calls are executed just before the return. Therefore, it can be used for resource cleanup. Multiple defer statements are executed in a FILO manner. Even if is a panic. The variables in the defer statement are determined when the defer statement is declared. Often used in:\nClose file handle Release lock resource Release database connection // defer in closure package main import \u0026#34;fmt\u0026#34; func main() { var whatever [5]struct{} for i := range whatever { defer func() { fmt.Println(i) }() } } Each time a \u0026ldquo;defer\u0026rdquo; statement executes, the function value and parameters to the call are evaluated as usualand saved anew but the actual function is not invoked.\npackage main func test() { x, y := 10, 20 defer func(i int) { println(\u0026#34;defer:\u0026#34;, i, y) // y is a closure reference, will be executed until the test function return }(x) // (x) means calling the function right now, x is copied to i, but the function is deferred, so it will be executed after the test function return x += 10 y += 100 println(\u0026#34;x =\u0026#34;, x, \u0026#34;y =\u0026#34;, y) } func main() { test() } // x = 20 y = 120 // defer: 10 120 pointer # func add2(n int) { n += 2 } func add2ptr(n *int) { *n += 2 } func main() { n := 5 add2(n) fmt.Println(n) // 5 add2ptr(\u0026amp;n) fmt.Println(n) // 7 } struct # type user struct { name string password string } func main() { // 1 a := user{name: \u0026#34;wang\u0026#34;, password: \u0026#34;1024\u0026#34;} // 2 b := user{\u0026#34;wang\u0026#34;, \u0026#34;1024\u0026#34;} // 3 c := user{name: \u0026#34;wang\u0026#34;} c.password = \u0026#34;1024\u0026#34; // 4 var d user d.name = \u0026#34;wang\u0026#34; d.password = \u0026#34;1024\u0026#34; fmt.Println(a, b, c, d) // {wang 1024} {wang 1024} {wang 1024} {wang 1024} fmt.Println(checkPassword(a, \u0026#34;haha\u0026#34;)) // false fmt.Println(checkPassword2(\u0026amp;a, \u0026#34;haha\u0026#34;)) // false } func checkPassword(u user, password string) bool { return u.password == password } // if use pointer, you can make some changes and avoid some big structure cost func checkPassword2(u *user, password string) bool { return u.password == password } // the structure method func (u user) checkPassword(password string) bool { return u.password == password } // You can change the value via pointer func (u *user) resetPassword(password string) { u.password = password } func main() { a := user{name: \u0026#34;wang\u0026#34;, password: \u0026#34;1024\u0026#34;} a.resetPassword(\u0026#34;2048\u0026#34;) fmt.Println(a.checkPassword(\u0026#34;2048\u0026#34;)) // true } exception # panic throw the error, recover catch the error.\nfunc main() { defer func() { if err := recover(); err != nil { fmt.Println(err) } }() var ch chan int = make(chan int, 10) close(ch) ch \u0026lt;- 1 } // send on closed channel error # Besides, panic causes a fatal system error, and can also return an error object to represent the status of the function call.\nimport ( \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; ) type user struct { name string password string } func findUser(users []user, name string) (v *user, err error) { for _, u := range users { if u.name == name { return \u0026amp;u, nil } } return nil, errors.New(\u0026#34;not found\u0026#34;) } func main() { if u, err := findUser([]user{{\u0026#34;wang\u0026#34;, \u0026#34;1024\u0026#34;}}, \u0026#34;li\u0026#34;); err != nil { fmt.Println(err) // not found return } else { fmt.Println(u.name) } } string # import ( \u0026#34;fmt\u0026#34; \u0026#34;strings\u0026#34; ) func main() { a := \u0026#34;hello\u0026#34; fmt.Println(strings.Contains(a, \u0026#34;ll\u0026#34;)) // true fmt.Println(strings.Count(a, \u0026#34;l\u0026#34;)) // 2 fmt.Println(strings.HasPrefix(a, \u0026#34;he\u0026#34;)) // true fmt.Println(strings.HasSuffix(a, \u0026#34;llo\u0026#34;)) // true fmt.Println(strings.Index(a, \u0026#34;ll\u0026#34;)) // 2 fmt.Println(strings.Join([]string{\u0026#34;he\u0026#34;, \u0026#34;llo\u0026#34;}, \u0026#34;-\u0026#34;)) // he-llo fmt.Println(strings.Repeat(a, 2)) // hellohello fmt.Println(strings.Replace(a, \u0026#34;e\u0026#34;, \u0026#34;E\u0026#34;, -1)) // hEllo fmt.Println(strings.Split(\u0026#34;a-b-c\u0026#34;, \u0026#34;-\u0026#34;)) // [a b c] fmt.Println(strings.ToLower(a)) // hello fmt.Println(strings.ToUpper(a)) // HELLO fmt.Println(len(a)) // 5 b := \u0026#34;你好\u0026#34; fmt.Println(len(b)) // 6 } fmt # type point struct { x, y int } func main() { s := \u0026#34;hello\u0026#34; n := 123 p := point{1, 2} // %v: value fmt.Printf(\u0026#34;s=%v\\n\u0026#34;, s) // s=hello fmt.Printf(\u0026#34;n=%v\\n\u0026#34;, n) // n=123 fmt.Printf(\u0026#34;p=%v\\n\u0026#34;, p) // p={1 2} // %+v: value with field name fmt.Printf(\u0026#34;p=%+v\\n\u0026#34;, p) // p={x:1 y:2} // more detail fmt.Printf(\u0026#34;p=%#v\\n\u0026#34;, p) // p=main.point{x:1, y:2} f := 3.141592653 fmt.Println(f) // 3.141592653 fmt.Printf(\u0026#34;%.2f\\n\u0026#34;, f) // 3.14 } json # import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; ) type userInfo struct { // you should make sure the uppercase of field name Name string Age int `json:\u0026#34;age\u0026#34;` Hobby []string } func main() { a := userInfo{Name: \u0026#34;wang\u0026#34;, Age: 18, Hobby: []string{\u0026#34;Golang\u0026#34;, \u0026#34;TypeScript\u0026#34;}} // Marshal: convert struct to json buf, err := json.Marshal(a) if err != nil { panic(err) } // hex code fmt.Println(buf) // [123 34 78 97...] // convert to string fmt.Println(string(buf)) // {\u0026#34;Name\u0026#34;:\u0026#34;wang\u0026#34;,\u0026#34;age\u0026#34;:18,\u0026#34;Hobby\u0026#34;:[\u0026#34;Golang\u0026#34;,\u0026#34;TypeScript\u0026#34;]} buf, err = json.MarshalIndent(a, \u0026#34;\u0026#34;, \u0026#34;\\t\u0026#34;) if err != nil { panic(err) } fmt.Println(string(buf)) var b userInfo err = json.Unmarshal(buf, \u0026amp;b) if err != nil { panic(err) } fmt.Printf(\u0026#34;%#v\\n\u0026#34;, b) // main.userInfo{Name:\u0026#34;wang\u0026#34;, Age:18, Hobby:[]string{\u0026#34;Golang\u0026#34;, \u0026#34;TypeScript\u0026#34;}} } time # import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { now := time.Now() fmt.Println(now) // 2022-03-27 18:04:59.433297 +0800 CST m=+0.000087933 t := time.Date(2022, 3, 27, 1, 25, 36, 0, time.UTC) t2 := time.Date(2022, 3, 27, 2, 30, 36, 0, time.UTC) fmt.Println(t) // 2022-03-27 01:25:36 +0000 UTC fmt.Println(t.Year(), t.Month(), t.Day(), t.Hour(), t.Minute()) // 2022 March 27 1 25 fmt.Println(t.Format(\u0026#34;2006-01-02 15:04:05\u0026#34;)) // 2022-03-27 01:25:36 diff := t2.Sub(t) fmt.Println(diff) // 1h5m0s fmt.Println(diff.Minutes(), diff.Seconds()) // 65 3900 t3, err := time.Parse(\u0026#34;2006-01-02 15:04:05\u0026#34;, \u0026#34;2022-03-27 01:25:36\u0026#34;) if err != nil { panic(err) } fmt.Println(t3 == t) // true fmt.Println(now.Unix()) // 1648738080 } strconv # import ( \u0026#34;fmt\u0026#34; \u0026#34;strconv\u0026#34; ) func main() { f, _ := strconv.ParseFloat(\u0026#34;1.234\u0026#34;, 64) fmt.Println(f) // 1.234 // str base precision n, _ := strconv.ParseInt(\u0026#34;111\u0026#34;, 10, 64) fmt.Println(n) // 111 n, _ = strconv.ParseInt(\u0026#34;0x1000\u0026#34;, 0, 64) fmt.Println(n) // 4096 // convert quickly n2, _ := strconv.Atoi(\u0026#34;123\u0026#34;) fmt.Println(n2) // 123 n2, err := strconv.Atoi(\u0026#34;AAA\u0026#34;) // invalid fmt.Println(n2, err) // 0 strconv.Atoi: parsing \u0026#34;AAA\u0026#34;: invalid syntax } os # import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;os/exec\u0026#34; ) func main() { // go run example/20-env/main.go a b c d fmt.Println(os.Args) // [/var/folders/8p/n34xxfnx38dg8bv_x8l62t_m0000gn/T/go-build3406981276/b001/exe/main a b c d] fmt.Println(os.Getenv(\u0026#34;PATH\u0026#34;)) // /usr/local/go/bin... fmt.Println(os.Setenv(\u0026#34;AA\u0026#34;, \u0026#34;BB\u0026#34;)) buf, err := exec.Command(\u0026#34;grep\u0026#34;, \u0026#34;127.0.0.1\u0026#34;, \u0026#34;/etc/hosts\u0026#34;).CombinedOutput() if err != nil { panic(err) } fmt.Println(string(buf)) // 127.0.0.1 localhost } "},{"id":17,"href":"/posts/video-technology-101/","title":"Video Technology 101","section":"Blog","content":"Do you really understand the video technology? What is the frame rate, resolution, bit rate and their relationships? Do you know the parameters p and K in the terminology 1080p and 4K? Do you know how to describe the video quality? What is the Blu-ray Disc on earth? What the difference between the codec such as H.264/AVC, H.265/HEVC, AV1? You may notice the Apple ProRes in apple, so do you really understand what it for? Do you know the HDR and Dolby Vision? So why there are so many tailers such as .mp4, .mkv, .flv, etc?\nNow this Video Technology 101 will introduce them to you.\nAt the beginning, you should know the Gib（Gibibyte, 1Gib = 2^30 Bytes）and GB（Gigabyte, 1GB = 10^9 Bytes）is not the same thing.\nGB is normal used for the storage device capacity such as hard disk, SSD, and the software package size. Gib is normal used for the RAM, file system capacity and other storage that need to be represented in binary. Bandwidth # Generally, the unit of bandwidth is bps (or b/s), which means bits per second, indicating how many bits of information are transmitted per second. It is the abbreviation of bit per second. Normally, the bandwidth provided by the ISP is Mbps, so the true bandwidth you need to divided 8 convert to MBps which is usually used. The bandwidth is mainly focus on the transmission field.\nFrame # We often call the number of frames in 1 second as frame rate, which is usually represented by fps (Frames Per Second). So in the video field, the frame rate is normally 24fps、25fps、29.97fps and 30fps. Nowdays, 99 percent of films are still using 24fps.\nThe refresh rate of the screen # The refresh rate of the screen is the number of times the screen is refreshed per second. It is usually represented by Hz (Hertz). So in the video field, the refresh rate of the screen is normally 60Hz、75Hz、120Hz and 144Hz. If the frame rate is higher than the refresh rate, the redundant frames will be discarded.\nSo the frame rate of the phone is 30fps, the frame rate of the TV is 25/30 (commonly used), 50/60fps (motion lens), and the frame rate of the high-speed camera is 120/240fps (slow lens).\nThey are different concepts, the frame rate is focus on the video, but the refresh rate is focus on the device. Even your device has 120 Hz, but the playing video is 24fps, the display is still 24fps.\nThis reminds me of my childhood experience, when I was a child, I always played the CF game, every time the display didn\u0026rsquo;t fit, I would try to close the vertical synchronization technology to improve the smoothness. The vertical synchronization technology is used to make the display follows the video same frame rate.\nResolution # The resolution of a video/screen is the number of pixels in the horizontal and vertical directions.\nP or i? # eg. If the resolution of a video is 1920x1080, it means that the video has 1920 pixels in the horizontal direction and 1080 pixels in the vertical direction(In the 16:9 aspect ratio, 1080/9*16=1920). So it total has 1920 * 1080 = 2073600 pixels, whose resolution is 2073600.\nAnd the traditional we call this resolution as 1080p, which means the vertical resolution is 1080 and the p means progressive scan.\nThe difference between progressive scan(1080p) and interlaced scan(1080i):\nInterlaced scan: Each frame contains half of the lines making up the entire image: the first containing only the odd lines, and the second frame containing only the even lines. The two frames will be displayed in alternating modes, allowing the human brain to reconstruct the entire image into one without realizing that it’s made up of two frames. That really save the bandwidth, which means if the fps is 60, actually the interlaced scan only creates a total of 30 full frames. The monitor only displays half of what it recorded by the camera itself. Progressive scan: The pixels are displayed in a single frame. While the human eye can’t usually detect the 1080i technology, 1080p has it clear advantages when it comes to scenes with lots of motion. In this case, an object in fast motion can have moved from the first field to the second, creating an almost blurry or glitchy image for the viewer.\nCompare the 1080i and 720p source: https://youtu.be/Avvh0iH2xSg Refer to the industry standard:\n480p SD（Standard Definition） 720p HD（High Definition） 1080p HD or FHD （Full High Definition） 2160p 4k UHD（Ultra High Definition） 4320p 8k UHD K # Now we can talk about the K, differ from the P and i in the vertical direction, the K is used to describe the number of horizontal pixel lines in the cinema level. In this case, you can just call the 1920x1080 as 1.9K or 2K. The 3840x2160 is the 4K of TV standard（Actually name is UHD: Ultra High Definition）, and the 4096×2160 is also the 4K in the cinema.\nNormally, there is not the fixed resolution of 4K or 2K, it just about the horizontal pixels lines.\nAspect Ratio # The aspect ratio of a video is the ratio of the width to the height. The normal is 16:9 or 4:3. For the resolution below 480p(SD), the aspect ratio normally is 4:3. The resolution is (480 / 3 * 4 = 640 ) 640x480.\nBit Rate / Data Rate # It is also called the data rate, which is more focus on the data compared to the bit rate on the binary data.\nThe size of data produced by the encoder per second, in kbps. For example, 100kbps means that the encoder generates 100kb of data per second. Most of the time, we don\u0026rsquo;t use the data size to describe it because every video duration is different. So we use the data rate which means the size of data produced by the encoder per second or the size of data in this video per second(1 Mbps = 1000 kbps). We can just estimate the data rate of a 2-hours film whose size is 35G and its bit rate is 35×1024×1024×8/2/3600=40777kbps=41Mbps. The bit rate is higher, the video image will be more detailed.\nThe data rate can be also categorized into Variable Bit-Rate (VBR) and Constant Bit-Rate (CBR).\nVBR: The data rate is not fixed, it will change according to the content of the video. Often used in the video website. CBR: The data rate is fixed, the common scenario is the streaming or TV broadcast. There is a question: A video file size is 10 M, the playback duration is 2 minutes 21 seconds, a 10 Mbps network can shared by how many people? Answer is in the end.\nRelationship between frame rate, resolution, and bit rate # How to describe the video quality # The image quality is usually described by PSNR (Peak signal-to-noise ratio), which means the ratio of the maximum signal to the noise. The image compressed the PSNR near 50dB means the image is very clear, and keep beyond 35 dB is acceptable.\nFor the video quality, we can use the VMAF (Video Multi-Method Assessment Fusion) from netflix to describe it.\nUnderstand the relationship # Different kinds of video requires different data rate. eg, the animation video has many blocks color, so it requires less than sport.\nSo when we center around the data rate:\nWhen the data rate is flexible, if the frame rate is higher, the frames in every second will be more, so the encoder will need more data to encode the frames. The video size will be larger.\nWhen the data rate is fixed, if the resolution is higher, the number of pixels will be more, but the encoder generates data is fixed, so the video will be more blurry. If the resolution is lower, the video will be more clear.\nYou can just refer to the netflix\u0026rsquo;s tech blog:\nAt each resolution, the quality of the encode monotonically increases with the bitrate, but the curve starts flattening out (eg A and B) when the bitrate goes above some threshold. This is because every resolution has an upper limit in the perceptual quality it can produce. When a video gets downsampled to a low resolution for encoding and later upsampled to full resolution for display, its high frequency components get lost in the process.\nOn the other hand, a high-resolution encode may produce a quality lower than the one produced by encoding at the same bitrate but at a lower resolution (see C and D). This is because encoding more pixels with lower precision can produce a worse picture than encoding less pixels at higher precision combined with upsampling and interpolation. Furthermore, at very low bitrates the encoding overhead associated with every fixed-size coding block starts to dominate in the bitrate consumption, leaving very few bits for encoding the actual signal. Encoding at high resolution at insufficient bitrate would produce encoding artifacts such as blocking, ringing and contouring. Encoding artifacts are typically more annoying and visible than blurring introduced by downscaling (before the encode) then upsampling at the member’s device.\nFrom netflix\u0026amp;rsquo;s tech blog If we collect all these regions from all the resolutions available, they collectively form a boundary called convex hull. Ideally, we want to operate exactly at the convex hull, but due to practical constraints, we would like to select bitrate-resolution pairs that are as close to the convex hull as possible.\nSo on the video website, if your device does not support higher resolution, but you have the higher bandwidth, the video website will offer you more higher bit rate video to improve your experience.\nBlu-ray Disc # As we can see in many video website, there is always a option named Blu-ray Disc. In fact, it means uses the blue laser to read and write, it is just a kind of storage, not a format of video resolution. Which is the successor of the DVD(720×480) and VCD(320×240).\nIn 2015, the Blu-ray Disc Association launched Ultra HD Blu-ray, with a capacity of up to 100GB and support for 4K UHD video at a resolution of 3840×2160 via H.265/HEVC.\nAnd in 2016, the Blu-ray Disc Association launched Ultra HD Blu-ray standard, which requires the HDR10, and can set the Dolby Vision as the optional.\nEncode is efficient # But if a video has a high resolution and frame rate and data rate, the video size will be very large. When you play it via internet, which is very waste of bandwidth. At this time, you will need to compress the video through some encode methods.\nThe x axis is data rate If you use different encode method, you will get a different data rate. But the cost is compression and poor compatibility.\nSo what is the codec? # Nowadays, we can shot the image with the camera, and then they are encoded via encoder and saved in your device, finally you can play it via the decoder, through the decoder, the compressed video data is output as non-compressed color data, such as YUV420P, RGB, etc.\nThe codec is the abbreviation of the encoder and decoder.\nAnd the common used tailers .mp4, .mkv, .flv have nothing to do with the encode. You can just treat them as the container. Every container has a lot of different encoded of video, audio, subtitle and other data.\nH.264/AVC # H.264 is the most common codec for video compression, it was released in 2003 by MPEG (Moving Picture Experts Group). It is the successor of the MPEG-2. It has a lot of different names, such as:\nH.264 AVC(Advanced Video Coding) MPEG-4 Part 10 H.265/HEVC # H.265 HEVC（High Efficiency Video Coding） MPEG-H Part 2 H.265 is the successor of the H.264. It is the most common codec for video compression. The main advantage is that it can compress the video more efficiently, which means the same quality video can be compressed to a smaller size as well as save the 50% of the bit rate at the same quality of video. Besides, the best part of H.265 is that it can support the resolution beyond 4K, eg 8K. eg, the Dolby Vision is almost codec in H.265.\nBut it needs the patent fee.\nH.266/VVC # It was released in 2020, the compression efficiency of H.266/VVC is 50% higher than H.265/HEVC.\nVP8/9 # They are created by Google and free of charge.\nAV1 # The AV1(AOMedia Video 1) is released in 2018 by Alliance for Open Media which is composed of Apple、Amazon、Cisco、Google、Intel、Microsoft、Mozilla and Netflix, and it can be more efficient about 20% than VP9.\nApple ProRes # Different from the codec before, Apple ProRes is the Intra-compression, not the Inter-compression.\nAs we can see, the intra-compression will save a lot of frame, which is larger than the inter-compression. But it has a lot of advantages:\nYou don\u0026rsquo;t need to compute other frames, so it has lower requirements for the device. You can do more editing on the video. For better understanding, you should know the IPB frames in inter-compression. In the video, every frame is a picture. But when saving them, we should consider compressing them.\nI: Intra-coded frame, which means the frame is independent, only use the current frame to encode. P: Predictive-coded frame, which means the frame is used the previous I or P frame to predict the current frame. B: Bi-directional predictive-coded frame, which means the frame is used the previous I or P frame and next P frame to predict the current frame. In the video encoding sequence, GOP (Group of picture) refers to the distance between two I frames, and Reference (Reference period) refers to the distance between two P frames. The number of bytes occupied by an I frame is greater than that of a P frame, and the number of bytes occupied by a P frame is greater than that of a B frame.\nSo in the same bit rate, the larger the GOP value, the more P and B frames, the more bytes occupied by each I, P, and B frames, and the better the image quality. The larger the Reference value, the more B frames, the same is true.\nIn a GOP, the largest frame is the I frame, so relatively speaking, the larger the gop_size is, the better the image quality will be, but in the decoding end, it must start from the first I frame received to correctly decode the original image, otherwise it will not be able to decode correctly. In the technique of improving video quality, there is also a technique of using more B frames. Generally, the compression rate of I is 7 (similar to JPG), P is 20, and B can reach 50. It can be seen that using B frames can save a lot of space, and the saved space can be used to save more I frames, so that better image quality can be provided at the same bit rate. So we should set the size of gop_size according to different business scenarios to get better video quality.\nNormally, there is 1 P frame after the I frame, and 2～3 B frames between the I and P frames, and 2～3 B frames between the two P frames. B frames transmit the difference information between it and the I or P frame, or the difference information between the P frame and the subsequent P frame or I frame, or the difference information between it and the front and rear I, P frames or P, P frames average value. When the main content changes more, the frame value between two I frames is smaller; when the main content changes less, the interval of I frames can be set larger.\nSo the order of encoding and display is not the same: The upper is encoding order, the lower is display order Color Depth # The color depth is the number of bits used to represent the color of a pixel. In the RGB, the common color depth is 8-bit, 10-bit, 12-bit.\nIn the 8-bit color depth, the color is represented by 256 levels, which means the color is represented by 256 different shades of gray. So the total number of colors is 256^3 = 16777216.\nBesides, the engineers find that the human eyes are sensitive to the brightness, it is not necessary to store all the color signals. We can allocate more bandwidth to the black-white signal (referred to as \u0026ldquo;brightness\u0026rdquo;) and allocate less bandwidth to the color signal (referred to as \u0026ldquo;chroma\u0026rdquo;). So, there is YUV. The Y means Brightness Luma, UV means Chroma.\nFor more info about images, I will introduce them in the article Image Technology 101.\nHDR # The HDR is an emerging concept in the video field. It is the abbreviation of High Dynamic Range. Compared with the SDR(Standard Dynamic Range), the HDR can display more color ranges and lightness.\nHDR also has a lot of different standards, such as:\nHDR10 HLG HDR10+ Dolby Vision HDR10 # HDR10 is an open standard announced by the American Consumer Technology Association in 2015. It does not require any copyright fees, and the \u0026ldquo;10\u0026rdquo; comes from the 10-bit color depth. In addition to the color depth, HDR10 recommends using a wide color gamut Rec.2020, PQ (SMTPE ST2084) absolute display, and static data processing. It can support the 1000 nit.\nThe HDR10 is mainly designed for the streaming media, which is not suitable for the film.\nHLG # But for a non HDR device such as TV, if it displays the HDR10 video, it will be very dark. The maximum brightness is 300 nit, so the HLG is designed for this. The NHK and BBC are the main supporters of the HLG, which can let the device display the 1000 nit differentiatedly.\nDolby Vision # The Dolby Vision is created by the Dolby lab, it uses the 12-bit color depth, and the color gamut is Rec.2020. It can support the 10000 nit.\nBesides, the dynamic metadata is also supported, which can adjust the color and brightness partially, emphasizing the contrast progressively.\nHDR10+ # HDR10+ is created by Samsung, which is also use the dynamic metadata.\nVideo Format # Note: the video format is different from the video codec.\nVideo format is the way to package the video data, audio data, subtitle data, etc. into a file. The file format is the video format, if a video file is packaged in a certain format, then its file suffix name will usually reflect it. The normal video format is:\nAVI (.avi) ASF (.asf) WMV (.wmv) QuickTime (.mov) MPEG (.mpg / .mpeg) MP4 (.mp4) m2ts (.m2ts / .mts ) Matroska (.mkv / .mks / .mka ) TS FLV AVI # Audio Video Interleaved, which is nearly a outdated technology, the file structure is divided into header, body and index 3 parts. The image data and sound data are stored in the body, and the index can be used to find the position of the image data and sound data. AVI itself only provides this framework, the internal image data and sound data format can be arbitrary encoding form. Because the index is placed at the end of the file, it is not possible to play the network media. For example, if you download the AVI file from the network, if it is not downloaded completely, it is difficult to play normally.\nFLV # FLV is an Adobe-released packaging format that can be used for both live streaming and on-demand scenarios. Its packaging format is extremely simple, with each FLVTAG being an independent entity.\nThe FLV packaging format consists of a file header (9 bytes) and many tags (FLV body). Each tag contains audio and video data, and each tag has a preTagSize field that indicates the size of the previous tag. The structure of FLV is shown below.\nAnd the tags can be divided into 3 types:\nVideo: Video Tag Audio: Audio Tag Script: Script Tag(Metadata Tag) A normal FLV file consists of a header, a script tag, and several video and audio tags.\nAnd the header is composed of 9 bytes, the first 3 bytes are the file type, always \u0026ldquo;FLV\u0026rdquo;, which is (0x46 F 0x4C L 0x56 V). The 4th byte is the version number. The 5th byte is the stream information, the last bit if is 0x01 means video exists, the last bit is 0x04 means audio exists, 0x01 | 0x04（0x05） means both video and audio exist, others are 0. The last 4 bytes represent the length of the FLV header 3+1+1+4=9.\nM3U8 # M3U8 is a common streaming media format, mainly in the form of a file list, supporting both live streaming and on-demand.\n#EXTM3U // m3u8 header #EXT-X-VERSION:3 #EXT-X-TARGETDURATION:4 #EXT-X-MEDIA-SEQUENCE:0 # if not EXT-X-ENDLIST the sequences will always be played from the last 3 sequences. #EXTINF:3.760000, out0.ts #EXTINF:1.880000, out1.ts #EXTINF:1.760000, out2.ts #EXTINF:1.040000, out3.ts #EXTINF:1.560000, out4.ts FFmpeg has built-in HLS packaging parameters, using the HLS format can perform HLS packaging.\nMP4 # MP4 actually represents MPEG-4 Part 14. It is only the 14th part of the MPEG standard. It is mainly based on the ISO/IEC standard. MP4 mainly aims to achieve fast forward and fast backward, and the effect of downloading while playing. It is the most common video format in the industry.\nMP4 file format is a very open container, almost able to describe all media structures. The media description and media data in the MP4 file are separated, and the organization of media data is also very free, not necessarily in chronological order, and media data can be directly referenced from other files. At the same time, MP4 also supports streaming media. MP4 is widely used for packaging h.264 video and AAC audio.\nMP4 data is all in the box（atom in QuickTime）, which means that the MP4 file is composed of several boxes, each box has a type and length, and can be understood as a data object block.\nA box can contain another box, this box is called container box.\nA MP4 file will have a ftyp type box as the flag of MP4 format and contain some information about the file; Then there will be a moov type box (Movie Box), which is a container box, the sub-box contains the media metadata information. moov in the MP4 file is also unique, moov will contain 1 mvhd and several trak. mvhd is the header box, which usually appears as the first sub-box of moov. mvhd defines the characteristics of the entire movie, usually containing media-independent information such as playback duration, creation time, etc. The media data of the MP4 file is contained in the mdat type box (Midia Data Box), this type of box is also a container box, can have multiple, can also be none (when the media data is all referenced from other files), the media data structure is described by metadata. The normal MP4 file playback requires that the ftyp and moov boxes are loaded completely, and some frames of the mdat box are downloaded, before it can start playing.\nOn top of that, the fMP4 can be fragmented, which means that the fMP4 does not need a moov Box to initialize, it can just contains some tracks, the metadata is in the moof box.\nTS # The Transport Stream file is a streaming media format, which is widely used in the live streaming field. It is a container format that can be used for both live streaming and on-demand. The suffix of the TS stream is .ts, .mpg or .mpeg. The HLS protocol is based on the TS format.\nThe difference between fMP4 and ts # The ts file does not provide information about the duration, so you cannot perform seek operations on the ts file. fMP4 provides information about the duration, so you can seek to a specific position. MPEG2-TS is a format that requires the video stream to be independently decodable from any segment.\nStreaming video format # We often call the flv, rmvb, mov, asf as the streaming video format. That is because the streaming file can be decoded while it is being transmitted, and it does not need the whole file to start. The characteristics are that there is file header information (this is not necessary) and the middle is packaged, which can be parsed directly by packet, and the file can be of any size, without the need to pass through the index packet. FLV, MPEG, RMVB, etc. can be parsed directly by packet, while MP4, AVI must rely on the index table, and the start must be fixed, if the index table is at the end, it is not possible to parse.\nBonus:\nThe streaming process is as follows:\nThe host uses rtmp to push the stream, and then pushes it to the cdn. The cdn supports the audience to use http-flv, hls, rtmp three ways to pull the stream. The common live streaming app uses http-flv. These protocols are like containers, they carry the packaging, rtmp and http-flv carry flv, hls carries m3u8 and ts. m3u8, ts inside are audio and video. The data size will be TS \u0026gt; MP4 \u0026gt; FLV.\nThe answer of the question # A video file size is 10 M, the playback duration is 2 minutes 21 seconds, a 10 Mbps network can shared by how many people?\nIt bit rate is 10 * 1024 * 1024 * 8 / (2 * 60 + 21) = 594936.73759 bps\n10* 1024 * 1024 / 594936.73759 = 17.215 people\nSo the answer is 17 people.\nRecommend the bit rate of the video:\nbitrate = w * h * fps * factor(the streaming on phone is 0.08)\neg. 720 X 480 25fps the recommend bit rate is 675Kb/s ( 720 * 480 * 25 * 0.08 )/1024 = 675Kb/s ，so the 100M bandwidth can shared by 100/0.675≈148 people.\nHow about the device? # For the more info about device, such as nit, ppi, refresh rate, etc. I will write a new article about it.\nReference # https://tagarno.com/blog/1080p-vs-1080-on-digital-microscopes/\nhttps://www-file.huawei.com/-/media/corporate/pdf/ilab/30-cn.pdf\nhttps://netflixtechblog.com/per-title-encode-optimization-7e99442b62a2\nhttps://netflixtechblog.com/optimized-shot-based-encodes-for-4k-now-streaming-47b516b10bbb\nhttps://netflixtechblog.com/optimized-shot-based-encodes-now-streaming-4b9464204830\nhttps://sspai.com/post/66001\nhttps://sspai.com/post/58012\nhttps://en.wikipedia.org/wiki/Video_Multimethod_Assessment_Fusion\nhttps://www.zhihu.com/question/265520537\nhttps://github.com/CharonChui/AndroidNote/tree/master/VideoDevelopment\nhttps://www.easemob.com/news/3614\nhttp://www.52im.net/thread-235-1-1.html\n"},{"id":18,"href":"/posts/the-main-kind-of-message-queue/","title":"The Main Kind of Message Queue","section":"Blog","content":"Message queues are a form of asynchronous service-to-service communication. They are important in enhancing a system\u0026rsquo;s scalability, reliability, and maintainability.\nNote: this article is reprint from the article : The System Design Cheat Sheet: Message Queues - ActiveMQ, RabbitMQ, Kafka, ZeroMQ, the author is Aleksandr Gavrilenko who I really appreciate.\nThe list of key features:\nAsynchronous Communication: Allows different parts of a system to communicate without needing to respond immediately, leading to more efficient use of resources. Decoupling of Services: Enables services to operate independently, reducing the system\u0026rsquo;s complexity and enhancing maintainability and scalability. Load Balancing: Distributes messages evenly across different services or workers, helping to manage workload and improve system performance. Order Preservation: Some message queues can ensure that messages are processed in the order they are sent, which is crucial for specific applications. Scalability: Facilitates easy scaling of applications by adding more consumers or resources to handle increased message flow. Rate Limiting and Throttling: Controls the rate at which messages are processed, which is important for managing resources and preventing system overloads. Fan-out Capability: Message queues often include a fan-out mechanism, which allows a single message to be delivered to multiple consumers or services simultaneously. Data Persistence: Offers the ability to store messages on disk or in memory until they are successfully processed, ensuring data is not lost in case of system failures. Message Filtering and Routing: Allows messages to be routed or filtered based on specific criteria or content, enabling more targeted and efficient processing. Components # In the context of message queues, the concepts of producers, consumers, and messages form the core of how these systems operate.\nProducer is an application or service responsible for creating and sending messages to the message queue. It does not need to be aware of who will process the message or when it will be processed. Consumer is an application or service that retrieves and processes messages from the queue. It acts on the data sent by producers. Messages are the data packets sent from producers to consumers. They can vary in size and format, ranging from simple text strings to complex data structures like JSON or XML. Message Broker is a middleware tool that facilitates communication between different applications or services by receiving messages from a sender and routing them to the appropriate receiver. It typically provides features like message queuing, routing, transformation, and delivery assurance. Messaging Models # Globally, there are two types of messaging: Point-to-Point and Publish-Subscribe.\nPoint-to-Point # Messages sent by a producer are placed in a queue and are consumed by a single consumer. It ensures that each message is processed only once by one receiver.\nPublish-Subscribe # Messages are published to a specific topic rather than a queue. Multiple consumers can subscribe to a topic and receive messages broadcast to that topic.\nHowever, some messaging protocols and the brokers that support them use an additional Exchange component for routing. In that case, messages are published to an exchange in the broker first. The Exchange, acting as the routing agent, forwards these messages to the appropriate queue using its routing rules.\nThe following exchange operating modes are distinguished:\nDirect Exchange # The message is routed to the queues whose binding key matches the message\u0026rsquo;s routing key.\nTopic Exchange # The topic exchange will perform a wildcard match between the routing key and the routing pattern specified in the binding to publish messages to the queue.\nFanout exchange # A message sent to a fan-out exchange is copied and forwarded to all the queues bound to the exchange.\nHeader exchange # Header exchanges will use the message header attributes for routing.\nDead letter # Dead letter queues collect messages that couldn’t be processed successfully for various reasons like processing errors, message expiration, or delivery issues.\nProtocols # Message brokers are responsible for delivering messages from producers to consumers. They use specific protocols that define the rules and formats for messaging.\nThe most popular protocols in this domain are:\nAMQP (Advanced Message Queuing Protocol) # A binary protocol designed for message-oriented middleware with robustness, security, and interoperability. Ideal for complex and reliable enterprise messaging systems.\nUse Cases: Enterprise applications, financial systems, and business processes Messaging Model: point-to-point, publish-subscribe Security: TLS/SSL, SASL, PLAIN Addressing: Uses exchange and queue-based addressing with routing capabilities Architecture: Broker-based MQTT (Message Queuing Telemetry Transport) # A lightweight, publish-subscribe network protocol optimized for high-latency or unreliable networks, ideal for IoT scenarios.\nUse Cases: IoT devices, home automation, mobile messaging applications Messaging Model: publish-subscribe Security: TLS/SSL, SASL, PLAIN Addressing: It uses topic-based addressing where messages are published to topics Architecture: Broker-based JMS (Java Message Service) # A Java-based messaging standard offers interfaces for point-to-point and publish-subscribe messaging patterns in Java applications.\nUse Cases: Enterprise Java applications, integration of multiple Java-based systems Messaging Model: point-to-point, publish-subscribe Security: Relies on the underlying Java EE security model Addressing: Uses JNDI for locating queues and topics Architecture: It is often implemented on top of enterprise service buses or application servers. STOMP (Simple Text Oriented Messaging Protocol) # A simple, text-based protocol that is easy to implement, suitable for scenarios where advanced messaging features are not a priority.\nUse Cases: Rapid development environments and simple messaging applications Messaging Model: point-to-point, publish-subscribe Security: PLAIN; relies on the underlying transport protocol for encryption Addressing: Frame-based with headers for destination, content type, etc. Architecture: Broker-based Kafka Protocol # Associated with Apache Kafka, a distributed streaming platform capable of handling high-throughput data streams.\nUse Cases: Real-time analytics, data pipelines, stream processing applications Messaging Model: publish-subscribe Security: SSL/TLS, SASL Addressing: Topic-based with partitioning for scalability. Architecture: Distributed system architecture with brokers and coordination. ZMTP (ZeroMQ Message Transport Protocol) # The underlying protocol for ZeroMQ is a high-performance asynchronous messaging library for building scalable, distributed applications.\nUse Cases: High-throughput, low-latency applications, microservices architecture. Messaging Model: request-reply, publish-subscribe, pipeline, exclusive pair, etc. Security: PLAIN, CurveZMQ, and ZAP Addressing: Flexible addressing using sockets Architecture: Library-based, enabling a brokerless design or various brokered configurations Brokers # ActiveMQ RabbitMQ Kafka ZeroMQ Written in Java Erlang Scala C++ Cross-platform yes yes yes yes Opensource yes yes yes yes Multiple languages yes yes yes yes Protocols AMQP, AUTO, MQTT, OpenWire, REST, RSS and Atom, Stomp, WSIF, WS Notification, XMPP, WebSocket AMQP, STOMP, MQTT, HTTP Binary over TCP TCP, UDP, inproc, PGM, IPC, TIPC, NORM, SOCKS5 QoS at-least-once at-most-once at-least-once at-most-once at-least-once at-most-once exactly-once at-least-once at-most-once Message patterns Queue, Pub-Sub Queue, Pub-Sub, RPC Pub-Sub Request-Reply, Pub-Sub, Push-Pull, Dealer and Router, Pair, Exclusive Pair, etc Persistence Disk, DB Mem, Disk Disk - ActiveMQ # Apache ActiveMQ is an open-source, multi-protocol, Java-based message broker designed by Apache. Architecture Features:\nMulti-Protocol Support: ActiveMQ supports a wide range of messaging protocols, including AMQP, MQTT, OpenWire, STOMP, and JMS (Java Message Service), making it highly adaptable to different client requirements. JMS Provider: As a JMS provider, ActiveMQ complies with the JMS API, which allows loose coupling, asynchronous communication, and reliability for Java applications. Broker-Based Architecture: ActiveMQ uses a broker architecture, where a central broker handles message routing, delivery, and queuing. Pluggable Persistence and Storage: Offers options for message persistence, including database storage (for durability) and file-system storage, supporting both high-performance and high-durability scenarios. Clustering and Load Balancing: Supports clustering and load balancing. Client-Side Acknowledgements: Provides different options for message acknowledgments, enhancing message reliability. Scenarios: # Enterprise Integration: Ideal for integrating different systems within an enterprise, mainly where Java-based or multiple protocols are used. Asynchronous Communication: Useful in scenarios where decoupling system components is essential, like in microservices architecture. Distributed Computing: Facilitates message communication in distributed systems, ensuring data consistency and reliability. IoT Communication: Can be used in IoT setups, especially where MQTT is preferred. Pros: # Versatility in Protocol Support: One of the key strengths of ActiveMQ is its support for multiple protocols, offering flexibility in various environments. Reliability and Durability: Provides reliable message delivery and durable storage. Clustering and High Availability: Supports clustering for load balancing and high availability. JMS Support: Comprehensive support for the JMS API makes it a strong candidate for Java-based systems. Cons: # Performance: While robust, ActiveMQ may not match the performance of some newer message brokers, especially in scenarios with extremely high throughput requirements. Complex Configuration: Can be difficult to configure and manage, especially in clustered setups. Resource Usage: Might require significant resources, particularly under heavy load, for optimal performance. Management and Monitoring: While it offers management tools, they might be less comprehensive and user-friendly than those of some newer brokers. RabbitMQ # RabbitMQ is an open-source message broker software known as a message-oriented middleware. It\u0026rsquo;s written in Erlang and is built on the Open Telecom Platform framework for clustering and failover. RabbitMQ is widely used for handling asynchronous processing, enabling communication between distributed systems through various messaging protocols, primarily AMQP (Advanced Message Queuing Protocol).\nSupport for Multiple Messaging Protocols: While RabbitMQ is primarily known for AMQP, it also supports MQTT, STOMP, and other protocols through plugins. Producer-Consumer Model: It follows the standard producer-consumer pattern, where producers send messages and consumers receive them, with RabbitMQ acting as the broker. Exchange-Queue Binding: Messages in RabbitMQ are published to exchanges, which are then routed to bound queues based on routing keys and patterns. Durable and Transient Messaging: Supports durable (persistent on disk) and transient (in-memory) messages. Clustering and High Availability: RabbitMQ can be clustered for high availability and scalability, distributing queues among multiple nodes. Flexible Routing: Offers several exchange types (like direct, topic, fanout, and headers) for diverse routing logic. Pluggable Authentication and Authorization: Supports pluggable authentication modules, including LDAP. Scenarios: # Asynchronous Processing: Ideal for decoupling heavy processing tasks in web applications, ensuring responsive user interfaces. Inter-Service Communication: Used in a microservices architecture for communicating between services. Task Queues: Well-suited for handling background tasks like sending emails or processing images. Distributed Systems: Facilitates message communication in distributed systems, maintaining consistency and reliability. Pros: # Reliability: RabbitMQ is known for its reliability and ability to ensure message delivery. Flexible Routing Capabilities: Its routing capabilities are more advanced than those of many message brokers. Scalability and High Availability: Supports scalable clustering, which is crucial for large-scale applications. Wide Protocol Support: The ability to support multiple messaging protocols increases adaptability. Management Interface: Comes with a user-friendly management interface, which simplifies monitoring and managing message flows. Cons: # Memory Usage: It can be memory-intensive, especially under heavy load, requiring proper monitoring and tuning. Erlang Dependency: Being built on Erlang, it introduces an additional technology stack that teams might need to familiarize themselves with. Performance Under High Load: While generally performant, performance tuning might be necessary under extremely high loads or in complex routing scenarios. Kafka # Apache Kafka is an open-source stream-processing software platform developed by LinkedIn and later donated to the Apache Software Foundation. It\u0026rsquo;s designed to handle high volumes of data and enable real-time data processing. Kafka is a distributed, partitioned, and replicated commit log service.\nArchitecture Features:\nProducer-Consumer Model: Kafka operates on a producer-consumer model. Producers publish messages to Kafka topics, and consumers subscribe to those topics to read the messages. Topics and Partitions: Data in Kafka is categorized into topics. Each topic can be split into partitions, allowing for parallel data processing. Partitions also enable Kafka to scale horizontally. Distributed System: Kafka runs as a cluster on one or more servers, and the Kafka cluster stores streams of records in categories called topics. Replication: Kafka replicates data across multiple nodes (brokers) to ensure fault tolerance. If a node fails, data can be retrieved from other nodes. Zookeeper Coordination: Kafka uses ZooKeeper for cluster management and coordination, ensuring consistency across the cluster. Commit Log Storage: Kafka stores all data as a sequence of records (or a commit log), providing durable message storage. Scenarios # Real-Time Data Processing: Ideal for real-time analytics and monitoring systems where quick data processing is crucial. Event Sourcing: Suitable for recording the sequence of events in applications. Log Aggregation: Effective for collecting and processing logs from multiple services. Stream Processing: Can be used for complex stream processing tasks like aggregating data streams or real-time filtering. Integration with Big Data Technologies: Often used with big data tools for data processing and analysis. Pros # High Throughput: Can handle high volumes of data and many simultaneous transactions. Scalability: Easily scalable both horizontally and vertically. Durability and Reliability: Provides durable storage of messages. Fault Tolerance: High fault tolerance due to data replication. Flexibility: Can be used for a wide range of use cases, from messaging systems to activity tracking and log aggregation. Cons # Complexity: Setup and management can be complex, especially for large clusters. Resource Intensive: Can be resource-intensive, requiring a good amount of memory and CPU. Dependency on ZooKeeper: Relies on ZooKeeper for coordination, adding an extra component to manage. Latency: While fast, it may not be suitable for use cases requiring extremely low latency. ZeroMQ # ZeroMQ (ØMQ, 0MQ, or ZMQ) is a high-performance asynchronous messaging library for distributed or concurrent applications. It\u0026rsquo;s not a message broker but a library that abstracts socket communication into a message-oriented middleware, making it easier to implement complex communication patterns in a scalable way. Developed in C++, ZeroMQ can be used in various programming languages through bindings.\nArchitecture Features:\nSocket-Based Communication: ZeroMQ uses sockets that abstract away the complexity of low-level network programming. These sockets can be used in patterns like publish-subscribe, request-reply, and fan-out. Brokerless Design: Unlike traditional message brokers, ZeroMQ is brokerless, allowing direct communication between endpoints without requiring a central message broker. Scalable Multithreading: Provides a way to manage multiple threads with socket-based communication, facilitating scalable I/O bound operations. Asynchronous I/O: Supports non-blocking, asynchronous I/O operations, which is critical for building responsive, high-performance applications. Language Agnostic: Offers bindings for multiple programming languages, making it accessible from different technology stacks. Scenarios # Microservices: Ideal for inter-service communication in a microservices architecture. High-Performance Computing: Used in parallel processing systems where performance is critical. Distributed Systems: Suitable for scenarios requiring complex, distributed messaging patterns without the overhead of a broker. Real-time Communication:** Effective in systems needing low-latency, real-time data exchange**. Pros # High Performance: ZeroMQ is designed for high throughput and low latency, making it suitable for performance-critical applications. Flexibility in Messaging Patterns: Supports various messaging patterns, providing flexibility for different communication scenarios. Reduced Complexity: The brokerless architecture simplifies deployment and reduces system complexity. Scalability: Facilitates easy scaling of applications with its efficient handling of multiple connections. Lightweight: Less resource-intensive compared to traditional messaging brokers. Cons # No Built-in Durability or Message Persistence: Lacks built-in message durability or persistence support, which must be handled externally. Requires Explicit Management of Connections: Developers need to manage connections, retries, and error handling, which can add complexity to application logic. Lack of a Central Broker: While this can be an advantage, it also means needing more centralized management, monitoring, and control over the messaging system. "},{"id":19,"href":"/posts/the-method-to-manage-traffic/","title":"The Method to Manage Traffic","section":"Blog","content":"Do you know the basic method to manage the traffic? There are four methods: Load Balancers, Reverse Proxies, Forward Proxies, and API Gateways. And they have different features and usage scenarios.\nNote: this article is reprint from the article : The System Design Cheat Sheet: Load Balancer, Reverse Proxy, Forward Proxy, API Gateway, the author is Aleksandr Gavrilenko who I really appreciate.\nLoad Balancers # A load balancer is a specialized network device or software application designed to optimize the distribution of incoming network traffic across multiple servers or resources, which prevents any single server from becoming a bottleneck. Load balancers achieve this by employing various algorithms to route incoming requests to the most appropriate server intelligently.\nLoad balancers can also be used within a data center to balance traffic between different components of an application, such as microservices.\nTypes # Based on specific needs, load balancing can be performed at the network/transport and application layer of the OSI layers:\nLayer 4\nOperate at the Transport layer of the OSI model, dealing primarily with TCP and UDP packets. This load balancers route traffic based on source and destination IP addresses and ports. They are relatively simple, fast, and effective for routing user requests to available servers without inspecting the content of the packets. Used when needed: High-speed data routing Simple load distribution based on IP and port Layer 7\nOperate at the Application layer and can inspect the data packets\u0026rsquo; content. This allows them to make more intelligent routing decisions based on HTTP headers, cookies, or application-specific data. Used when needed: SSL termination Content-based routing Application-level decisions like directing users to a specific version of a web page Layer 2/3\nThough less common, some load balancers operate at the Data Link (Layer 2) and Network (Layer 3) levels. These load balancers are generally used in specialized scenarios requiring packet-level routing. Used when needed:\nMAC address-based routing (Layer 2) IP-based routing without port considerations (Layer 3) GSLB(Global Server Load Balancer) # Global Server Load Balancing, is designed to distribute user traffic across multiple geographically dispersed data centers. GSLB is primarily based on the Domain Name System (DNS). Based on many factors, the DNS server returns the IP address of the most suitable data center.\nUse Cases # DNS round-robin: Distributes traffic between all data centers in multiple locations. Geolocation-based DNS: Detect users\u0026rsquo; locations and route traffic to the nearest data center to lower latency. Failover: Send all traffic to a primary data center, but redirect traffic to a secondary data center if the primary becomes inaccessible. Popular Solutions: F5 BIG-IP DNS, Citrix ADC, AWS Route 53, Cloudflare Load Balancer\nLocal Load Balancers # A Local Load Balancer operates within a single data center or cloud region, primarily focusing on distributing incoming traffic among local servers. Its main goal is to optimize resource utilization, maximize throughput, and minimize response time.\nPopular Solutions: HAProxy, NGINX Load Balancer, AWS Elastic Load Balancer (ELB), F5 BIG-IP Local Traffic Manager (LTM)\nLoad-balancing algorithms # Different algorithms offer various advantages and trade-offs, making them more or less suitable for particular scenarios.\nRound Robin # Distributes incoming requests sequentially and evenly across all available servers cyclically.\nSticky Round Robin # A hybrid approach that combines Round Robin distribution with session persistence, ensuring that once a user session is established, it remains on the assigned server.\nWeighted Round Robin # Similar to Round Robin, each server is assigned a weighted score, affecting the distribution of requests. Servers with higher weights receive a larger share of the incoming requests.\nIP/URL Hash # This algorithm hashes the client\u0026rsquo;s IP address to determine the server for routing the request, ensuring session persistence by always directing a specific client\u0026rsquo;s requests to the same server.\nDynamic Algorithms # Least Time # Requests are redirected to the server with the fastest average response time, balancing server load and user experience.\nLeast Connections # Requests are redirected to the server with the fewest active connections, requiring additional computation by the load balancer to identify less-busy servers.\nReverse Proxy # The reverse proxy is a server that sits between clients and a web server, directing incoming requests to appropriate backend servers. The key difference between a reverse proxy and a load balancer is their primary focus. While both can distribute traffic across multiple servers, a load balancer is designed explicitly for this purpose and usually offers more advanced distribution algorithms. A reverse proxy, on the other hand, provides a broader range of functionalities, such as:\nBackend Anonymity: Backend servers remain hidden from the external network, protecting against potential vulnerabilities. DDoS Mitigation: Many reverse proxies have built-in features to shield backend servers from distributed denial-of-service attacks, such as IP deny listing and client connection rate limiting. SSL Offloading: Handles the decryption of incoming requests and encryption of server responses, relieving backend servers from these computationally intensive tasks. Data Compression: Reduces the size of server responses for faster data transfer. Response Caching: Serves previously cached responses to identical requests, improving speed and reducing server load. Direct Serving of Static Content: Manages the delivery of static files like HTML, CSS, JavaScript, images, and videos directly to the client. URL/Content Rewriting: Modifies the URL or content before forwarding requests to the backend servers. Reverse proxies can be helpful even with just one web server or application server.\nPopular Solutions: Nginx, Apache HTTP Server (mod_proxy), HAProxy, Squid, Azure Application Gateway\nForward Proxy # A Forward Proxy is a server that sits between client devices and the Internet, acting as an intermediary for outgoing requests from the client. A forward proxy accepts connections from computers on a private network and forwards those requests to the public internet. It is the single exit point for subnet users accessing resources outside their private network.\nThe key difference between a forward proxy and a reverse proxy lies in their primary roles and whom they serve.\nA forward proxy primarily serves the client\u0026rsquo;s needs, helping it access blocked or restricted content and providing anonymity. A reverse proxy, on the other hand, is installed on the server side and manages incoming requests to the server. A forward proxy is client-focused and provides functions like：\nClients Anonymity: A forward proxy conceals the client\u0026rsquo;s original IP address, adding an extra layer of security during internet access. Access Management: Organizations can employ forward proxies to limit access to specific resources, safeguarding sensitive information. Caching: By caching commonly accessed resources, forward proxies can enhance client internet response times. Traffic Control: Forward proxies can manage and control network traffic flow, optimizing bandwidth usage. Logging: Forward proxies can record all outgoing requests and responses, aiding in monitoring and auditing. Popular Solutions: Squid, Tinyproxy, CCProxy, WinGate\nAPI Gateway # An API Gateway is a centralized entry point that manages and routes API requests from client applications to appropriate backend services. It acts as a layer of abstraction between the client and multiple backend services, streamlining their interaction. It is a crucial component in modern architecture, especially in microservices-based systems. API gateways offer various functionalities like:\nAn API Gateway is a centralized entry point that manages and routes API requests from client applications to appropriate backend services. It acts as a layer of abstraction between the client and multiple backend services, streamlining their interaction. It is a crucial component in modern architecture, especially in microservices-based systems. API gateways offer various functionalities like:\nRouting: Directs client-originating API requests to the suitable backend service or microservice, guided by established rules and settings. Authentication and Authorization: Manages user credentials to ensure only approved clients can access services. This includes verification of API keys, tokens, or other forms of identification. Rate Limiting and Throttling: Safeguards backend services by enforcing client request rate limits or throttling based on pre-configured policies. Load Balancing Caching Request and Response Transformation: Alters incoming and outgoing data, such as data format conversions or header modifications, to maintain compatibility between clients and backend services. Monitoring Request and Response Validation Circuit Breaking: Implements a circuit breaker pattern to prevent a single service failure from compromising the entire system. It monitors service health and can switch to a backup service if needed. Service Discovery: Identifies available microservices and their locations, allowing clients to interact with them without knowing their specific addresses. Enhanced Security: Enforces robust authentication and access control measures, bolstering the system\u0026rsquo;s overall security against unauthorized access. Popular Solutions: Kong, Amazon API Gateway, Apigee, Azure API Gateway, MuleSoft Anypoint Platform\nConclusion # Load Balancers primarily focus on distributing incoming traffic across multiple servers to ensure no single server is overwhelmed. They are essential for scalability and high availability but are generally agnostic to the type of content being served.\nReverse Proxies sit in front of web servers and direct client requests to the appropriate backend server. They are server-facing and are often used for caching, SSL termination, and load distribution within an internal network.\nForward Proxies act as intermediaries between clients and servers, often filtering requests, providing anonymity for users, or bypassing geo-restrictions. They are client-facing and are generally used to control outbound traffic.\nAPI Gateways, on the other hand, are specialized types of reverse proxies tailored for API traffic. They offer advanced functionalities like request routing, API composition, rate limiting, and security features such as authentication and authorization.\n"},{"id":20,"href":"/posts/the-different-kind-of-api-design/","title":"The Different Kind of API Design","section":"Blog","content":"In the daily life, we often hear about different kind of API, such as REST API, GraphQL API, WebSocket, Webhook, RPC and gRPC even the SOAP, so do you know the difference between them? How to choose the right API for your project?\nNote: this article is reprint from the article : The System Design Cheat Sheet: API Styles - REST, GraphQL, WebSocket, Webhook, RPC/gRPC, SOAP, the author is Aleksandr Gavrilenko who I really appreciate.\nREST(Representational State Transfer): the most used style that uses standard methods and HTTP protocols. It\u0026rsquo;s based on principles like statelessness, client-server architecture, and cacheability. It\u0026rsquo;s often used between front-end clients and back-end services. GraphQL: a query language for APIs and a runtime for fulfilling those queries with your existing data. GraphQL provides a complete and understandable description of the data in your API, gives clients the power to ask for exactly what they need and nothing more, reducing over-fetching. WebSocket: a protocol allowing two-way communication over TCP. Clients use web sockets to get real-time updates from back-end systems. Webhook: a way to send data from one application to another. It\u0026rsquo;s often used to send data about specific events from a back-end service to a front-end client. It is a user-defined HTTP callback. RPC (gRPC): a protocol that one service can use to request a procedure/method from a service located on another computer in a network. Usually, It\u0026rsquo;s designed for low-latency, high-speed communication. SOAP: a protocol for exchanging structured information to implement web services. It relies on XML and is known for its robustness and security features, currently considered a legacy protocol. REST API # REST API is the most used style that uses standard methods and HTTP protocols. Its stateless nature and use of standard HTTP methods make it a popular choice for building web-based APIs.\nFormat: XML, JSON, HTML, plain text\nTransport protocol: HTTP/HTTPS\nKey Concepts and Characteristics # Resource # In REST, everything is a resource. A resource is an object with a type, associated data, relationships to other resources, and a set of methods that operate on it. Resources are identified by their URIs (typically a URL).\nCRUD Operations # REST services often map directly to CRUD (Create, Read, Update, Delete) operations on resources.\nHTTP Methods # REST systems use standard HTTP methods:\nGET: Retrieve a resource. POST: Create a new resource. PUT/PATCH: Update an existing resource. DELETE: Remove a resource. Status Codes # REST APIs use standard HTTP status codes to indicate the success or failure of an API request:\nFor more status codes, you can refer to the RFC manual.\n2xx - Acknowledge and Success\n200 - OK 201 - Created 202 - Accepted 3xx - Redirection\n301 - Moved Permanently 302 - Found 303 - See Other 4xx - Client Error\n400 - Bad Request 401 - Unauthorized 403 - Forbidden 404 - Not Found 405 - Method Not Allowed 5xx - Server Error\n500 - Internal Server Error 501 - Not Implemented 502 - Bad Gateway 503 - Service Unavailable 504 - Gateway Timeout Stateless # Each request from a client to a server must contain all the information needed to understand and process the request. The server should not store anything about the client\u0026rsquo;s state between requests.\nClient-Server Architecture # REST is based on the client-server model. The client is responsible for the user interface and experience, while the server is responsible for processing requests, handling business logic, and storing data.\nCacheable # Responses from the server can be cached by the client. The server must indicate whether a response is cacheable or not.\nLayered System # A client cannot ordinarily tell whether it is connected directly to the end server or an intermediary. Intermediary servers can improve system scalability by enabling load balancing and providing shared caches.\nHATEOAS # Hypermedia As The Engine Of Application Stat is a REST web service principle that enables clients to interact with and navigate through a web application entirely based on the hypermedia provided dynamically by the server in its responses, promoting loose coupling and discoverability.\nUse Cases # Web Services: Many web services expose their functionality via REST APIs, allowing third-party developers to integrate and extend their services.\nMobile Applications: Mobile apps often communicate with backend servers using REST APIs to fetch and send data.\nSingle Page Applications (SPAs): SPAs use REST APIs to dynamically load content without requiring a full page refresh.\nIntegration Between Systems: Systems within an organization can communicate and share data using REST APIs.\nExamples # Request\nGET “/user/42” Response\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34; \u0026#34;links\u0026#34;: { \u0026#34;role\u0026#34;: \u0026#34;/user/42/role\u0026#34; } } GraphQL API # It offers a more flexible, robust, and efficient approach to building APIs, especially in complex systems or when the frontend needs high flexibility. It shifts some of the responsibility from the server to the client, allowing the client to specify its data requirements.\nFormat: JSON\nTransport protocol: HTTP/HTTPS\nKey Concepts and Characteristics # Query Language for APIs # It allows clients to request the data they need, making it possible to get all required information in a single request.\nType System # GraphQL APIs are organized in terms of types and fields, not endpoints. It uses a strong type system to define the capabilities of an API. All the types exposed in an API are written down in a schema using the GraphQL Schema Definition Language (SDL).\nSingle Endpoint # Unlike REST, where you might have multiple endpoints for different resources, in GraphQL, you typically expose a single endpoint that expresses the complete set of capabilities of the service.\nResolvers # On the server side, resolvers gather the data described in a query.\nReal-time Updates with Subscriptions # GraphQL supports real-time updates through subscriptions, allowing clients to receive updates when data changes.\nIntrospective # A GraphQL server can be queried for the types it supports. This creates a strong contract between client and server, allowing for tooling and better validation.\nUse Cases # Flexible Frontends: For applications (especially mobile) with crucial bandwidth, you want to minimize the data fetched from the server.\nAggregating Microservices: A GraphQL layer can be introduced to aggregate the data from these services into a unified API if you have multiple microservices.\nReal-time Applications: With its subscription system, GraphQL can be an excellent fit for applications that need real-time data, like chat applications, live sports updates, etc.\nVersion-Free APIs: With REST, you often need to version your APIs once changes are introduced. With GraphQL, clients only request the data required, so adding new fields or types doesn\u0026rsquo;t create breaking changes.\nExamples # Request\nGET “/graphql?query=user(id:42){ name role { id name } }” Response\n{ \u0026#34;data\u0026#34;: { \u0026#34;user\u0026#34;: { \u0026#34;id\u0026#34;: 42, \u0026#34;name\u0026#34;: \u0026#34;Alex\u0026#34;, \u0026#34;role\u0026#34;: { \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;admin\u0026#34; } } } } WebSocket # WebSockets provide a full-duplex communication channel over a single, long-lived connection, allowing real-time data exchange between a client and a server. This makes it ideal for interactive and high-performance web applications.\nThe Websocket establish a connection by the Upgrade header in the HTTP request. The non-encrypted ws:// and encrypted wss://. Because it uses HTTP handshake, it uses the same port: ws is 80 (HTTP), wss is 443 (HTTPS).\nFormat: Binary\nTransport protocol: TCP\nKey Concepts and Characteristics # Persistent Connection # Unlike the traditional request-response model, WebSockets provide a full-duplex communication channel that remains open, allowing for real-time data exchange.\nUpgrade Handshake # WebSockets start as an HTTP request, which is then upgraded to a WebSocket connection if the server supports it. This is done via the Upgrade header.\nFrames # Once the connection is established, data is transmitted as frames. Both text and binary data can be sent through these frames.\nLow Latency # WebSockets allow for direct communication between the client and server without the overhead of opening a new connection for each exchange. This results in faster data exchange.\nBidirectional # Both the client and server can send messages to each other independently.\nLess Overhead # After the initial connection, data frames require fewer bytes to send, leading to less overhead and better performance than repeatedly establishing HTTP connections.\nProtocols and Extensions # WebSockets support subprotocols and extensions, allowing for standardized and custom protocols on top of the base WebSocket protocol.\nUse Cases # Online Gaming: Real-time multiplayer games where players\u0026rsquo; actions must be immediately reflected to other players.\nCollaborative Tools: Applications like Google Docs, where multiple users can edit a document simultaneously and see each other\u0026rsquo;s changes in real-time.\nFinancial Applications: Stock trading platforms where stock prices need to be updated in real-time.\nNotifications: Any application where users need to receive real-time notifications, such as social media platforms or messaging apps.\nLive Feeds: News websites or social media platforms where new posts or updates are streamed live to users.\nExamples # Request\nGET /chat HTTP/1.1 // 请求行 Host: server.example.com Upgrade: websocket // required Connection: Upgrade // required Origin: http://example.com Sec-WebSocket-Key: x3JJHMbDL1EzLkh9GBhXDw== Sec-WebSocket-Protocol: chat, superchat Sec-WebSocket-Version: 13 Response\nHTTP/1.1 101 Switching Protocols Upgrade: websocket // required Connection: Upgrade // required Sec-WebSocket-Accept: s3pPLMBiTxaQ9kYGzzhZRbK+xOo= // required，the encoded Sec-WebSocket-Key Sec-WebSocket-Protocol: chat The handshaking process is not the TCP handshaking, but the upgrade process from HTTP/1.1 to WebSocket. After the handshaking process, the next is the frames of the websocket.\nWebhook # Webhook is a user-defined HTTP callback triggered by specific web application events, allowing real-time data updates and integrations between different systems.\nFormat: XML, JSON, plain text\nTransport protocol: HTTP/HTTPS\nKey Concepts and Characteristics # Event-Driven # Webhooks are typically used to denote that an event has occurred. Instead of requesting data at regular intervals, webhooks provide data as it happens, turning the traditional request-response model on its head.\nCallback Mechanism # Webhooks are essentially a user-defined callback mechanism. When a specific event occurs, the source site makes an HTTP callback to the URI provided by the target site, which will then take a specific action.\nPayload # When the webhook is triggered, the source site will send data (payload) to the target site. This data is typically in the form of JSON or XML.\nReal-time # Webhooks allow applications to get real-time data, making them highly responsive.\nCustomizable # Users or developers can typically define what specific events they want to be notified about.\nSecurity # Since webhooks involve making callbacks to user-defined HTTP endpoints, they can pose security challenges. It\u0026rsquo;s crucial to ensure that the endpoint is secure, the data is validated, and possibly encrypted.\nUse Cases # Continuous Integration and Deployment (CI/CD): Triggering builds and deployments when code is pushed, or a pull request is merged.\nContent Management Systems (CMS): Notifying downstream systems when content is updated, published, or deleted.\nPayment Gateways: Informing e-commerce platforms about transaction outcomes, such as successful payments, failed transactions, or refunds.\nSocial Media Integrations: Receiving notifications about new posts, mentions, or other relevant events on social media platforms.\nIoT (Internet of Things): Devices or sensors can trigger webhooks to notify other systems or services about specific events or data readings.\nExamples # Request\nGET “https://external-site/webhooks?url=http://site/service-h/api\u0026amp;name=name” Response\n{ \u0026#34;webhook_id\u0026#34;: 12 } RPC/gRPC # RPC (Remote Procedure Call) is a protocol that allows a program to execute a procedure or subroutine in another address space, enabling seamless communication and data exchange between distributed systems.\ngRPC (Google RPC) is a modern, open-source framework built on top of RPC that uses HTTP/2 for transport and Protocol Buffers as the interface description language, providing features like authentication, load balancing, and more to facilitate efficient and robust communication between microservices.\nRPC # Format: JSON, XML, Protobuf, Thrift, FlatBuffers\nTransport protocol: Various\nKey Concepts and Characteristics # Definition # RPC allows a program to cause a procedure (subroutine) to execute in another address space (commonly on another computer on a shared network). It\u0026rsquo;s like calling a function performed on a different machine than the caller\u0026rsquo;s.\nStubs # In the context of RPC, stubs are pieces of code generated by tools that allow local and remote procedure calls to appear the same. The client has a stub that looks like the remote procedure, and the server has a stub that unpacks arguments, calls the actual procedure, and then packs the results to send back.\nSynchronous by default # Traditional RPC calls are blocking, meaning the client sends a request to the server and gets blocked waiting for a response from the server.\nLanguage Neutral # Many RPC systems allow different client and server implementations to communicate regardless of the language they\u0026rsquo;re written in.\nTight Coupling # RPC often requires the client and server to know the procedure being called, its parameters, and its return type.\nUse Cases # Distributed Systems: RPC is commonly used in distributed systems where parts of a system are spread across different machines or networks but need to communicate as if they\u0026rsquo;re local.\nNetwork File Systems: NFS (Network File System) is an example of RPCs performing file operations remotely.、\nExamples # Request\n{ \u0026#34;method\u0026#34;: \u0026#34;addUser\u0026#34;, \u0026#34;params\u0026#34;: [ \u0026#34;Alex\u0026#34; ] } Response\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;name\u0026#34;: \u0026#34;Alex\u0026#34;, \u0026#34;error\u0026#34;: null } gRPC # Format: Protobuf\nTransport protocol: HTTP/2\nKey Concepts and Characteristics # Definition # gRPC is an open-source RPC framework developed by Google. It uses HTTP/2 for transport, Protocol Buffers (Protobuf) as the interface description language, and provides authentication, load balancing features, and more.\nProtocol Buffers # This is a language-neutral, platform-neutral, extensible mechanism for serializing structured data. With gRPC, you define service methods and message types using Protobuf.\nPerformance # gRPC is designed for low latency and high throughput communication. HTTP/2 allows for multiplexing multiple calls over a single connection and reduces overhead.\nStreaming # gRPC supports streaming requests and responses, allowing for more complex use cases like long-lived connections, real-time updates, etc.\nDeadlines/Timeouts # gRPC allows clients to specify how long they will wait for an RPC to complete. The server can check this and decide whether to complete the operation or abort if it will likely take too long.\nPluggable # gRPC is designed to support pluggable authentication, load balancing, retries, etc.\nLanguage Neutral # Like RPC, gRPC is language agnostic. However, with Protobuf and the gRPC tooling, generating client and server code in multiple languages is easy.\nUse Cases # Microservices: gRPC is commonly used in microservices architectures due to its performance characteristics and ability to define service contracts easily.\nReal-time Applications: Given its support for streaming, gRPC is suitable for real-time applications where servers push data to clients in real-time.\nMobile Clients: gRPC\u0026rsquo;s performance benefits and streaming capabilities make it a good fit for mobile clients communicating with backend services.\nExamples # Request\nmessage User { int id = 1 string name = 2 } service UserService { rpc AddUser(User) returns (User); } SOAP # SOAP, which stands for Simple Object Access Protocol, is a protocol for exchanging structured information to implement web services in computer networks. It\u0026rsquo;s an XML-based protocol that allows programs running on disparate operating systems to communicate with each other.\nFormat: XML\nTransport protocol: HTTP/HTTPS, JMS, SMTP, and more\nKey Concepts and Characteristics # XML-Based # SOAP messages are formatted in XML and contain the following elements:\nEnvelope: The root element of a SOAP message that defines the XML document as a SOAP message. Header: Contains any optional attributes of the message used in processing the message, either at an intermediary point or the ultimate end-point. Body: Contains the XML data comprising the message being sent. Fault: An optional Fault element that provides information about errors while processing the message. Neutrality # SOAP can be used with any programming model and is not tied to a specific one.\nIndependence # It can run on any operating system and in any language.\nStateless # Each request from a client to a server must contain all the information needed to understand and process the request.\nBuilt-in Error Handling # The Fault element in a SOAP message is used for error reporting.\nStandardized # Operates based on well-defined standards, including the SOAP specification itself, as well as related standards like WS-ReliableMessaging for ensuring message delivery, WS-Security for message security, and more.\nUse Cases # Enterprise Applications: SOAP is often used in enterprise settings due to its robustness, extensibility, and ability to traverse firewalls and proxies.\nWeb Services: Many web services, especially older ones, use SOAP. This includes services offered by major companies like Microsoft and IBM.\nFinancial Transactions: SOAP\u0026rsquo;s built-in security and extensibility make it a good choice for financial transactions, where data integrity and security are paramount.\nTelecommunications: Telecom companies might use SOAP for processes like billing, where different systems must communicate reliably.\nExample # Request\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;soap:Envelope\u0026gt; \u0026lt;soap:Body\u0026gt; \u0026lt;m:AddUserRequest\u0026gt; \u0026lt;m:Name\u0026gt;Alex\u0026lt;/m:Name\u0026gt; \u0026lt;/m:AddUserRequest\u0026gt; \u0026lt;/soap:Body\u0026gt; \u0026lt;/soap:Envelope\u0026gt; Response\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;soap:Envelope\u0026gt; \u0026lt;soap:Body\u0026gt; \u0026lt;m:AddUserResponse\u0026gt; \u0026lt;m:Id\u0026gt;42\u0026lt;/m:Id\u0026gt; \u0026lt;m:Name\u0026gt;Alex\u0026lt;/m:Name\u0026gt; \u0026lt;/m:AddUserResponse\u0026gt; \u0026lt;/soap:Body\u0026gt; \u0026lt;/soap:Envelope\u0026gt; "},{"id":21,"href":"/posts/the-encode-and-decode-in-python/","title":"The Encode and Decode in Python","section":"Blog","content":"Do you really know the encode and decode in Python?\nThe encode and decode in Python are used to convert between strings and bytes. That we all know that the string in the computer storage and communication in the network is in the form of byte sequence, not the unicode.\nEncode # So the encode is used to transform the string to the byte sequence. And when you call the encode function, you need to specify the encoding type, such as utf-8, gbk, gb2312, etc. And python will use the encoding type to transform every character in the string to the corresponding byte sequence.\ns = \u0026#34;你好，世界\u0026#34; encoded_s = s.encode(\u0026#39;utf-8\u0026#39;) print(encoded_s) # b\u0026#39;\\xe4\\xbd\\xa0\\xe5\\xa5\\xbd\\xef\\xbc\\x8c\\xe4\\xb8\\x96\\xe7\\x95\\x8c\u0026#39; # the b is the prefix of the byte sequence. Decode # And the decode is the function of byte sequence. It transform the byte sequence to the string. And you should all use the same encoding type to transform the byte sequence to the string.\nb = b\u0026#39;\\xe4\\xbd\\xa0\\xe5\\xa5\\xbd\\xef\\xbc\\x8c\\xe4\\xb8\\x96\\xe7\\x95\\x8c\u0026#39; decoded_b = b.decode(\u0026#39;utf-8\u0026#39;) print(decoded_b) # 你好，世界 "},{"id":22,"href":"/posts/about-the-systemd/","title":"About the Systemd","section":"Blog","content":"It has been a long time since the linux used init to manage the startup process, such as sudo /etc/init.d/apache2 start or service apache2 start, but the init is serial. To address this issue, the systemd was born. The d is the abbreviation of daemon, which means the systemd is a daemon manager. The systemd substitutes the initd and becomes the default main process PID 1.\nYou can check the version systemctl --version.\nsystemctl # sudo systemctl reboot sudo systemctl poweroff sudo systemctl suspend hostnamectl # Look up the host info, Architecture, Hardware, Kernel, Operating System, etc.\nYou can also query via uname -a.\ntimedatectl # Query the timezone.\nloginctl # Manage the login session.\nloginctl list-sessions loginctl list-users loginctl show-user Unit # There are 12 types of units:\nService unit Target unit which is a group of units Device Unit Mount Unit Automount Unit Path Unit Scope Unit: not started by systemd Slice Unit: process group Snapshot Unit: Systemd snapshot, can switch back to a snapshot Socket Unit: process communication socket Swap Unit: swap file Timer Unit You can also query them:\nsystemctl list-units \u0026ndash;all systemctl list-units \u0026ndash;all \u0026ndash;state=inactive systemctl list-units \u0026ndash;type=service systemctl status # systemctl status bluetooth.service systemctl about service # systemctl start systemctl stop systemctl restart systemctl reload systemctl enable systemctl disable systemctl show service Unit config # The unit config file is located at /etc/systemd/system/. But most of them are the symbolic links to the real config file in /usr/lib/systemd/system/.\nThe systemctl enable command will create a symbolic link to the real config file in /etc/systemd/system/(if you config start on boot in the unit file, it will start on boot) And the systemctl disable command will remove the symbolic link. Such as\nsudo systemctl enable clamd@scan.service # which is equivalent to $ sudo ln -s \u0026#39;/usr/lib/systemd/system/clamd@scan.service\u0026#39; \u0026#39;/etc/systemd/system/multi-user.target.wants/clamd@scan.service\u0026#39; You can list all the config files:\nsystemctl list-unit-files # the tail is the kind of unit, such as service(default), socket, etc. There are four status of the unit:\nenabled: the unit is enabled disabled: the unit is disabled static: the unit is static, which only served as other unit\u0026rsquo;s dependency masked: the unit is banned to be enabled Adjust file # systemctl cat atd.service can show the specific unit file.\nThe detail you can refer to the official document.\nOnce you adjust the unit file, you need to reload the systemd and restart the service:\nsudo systemctl daemon-reload sudo systemctl restart httpd.service Target # The target is a group of units, once the target is enabled, the units in the target will be enabled.\njournalctl # You can check the kernel log and the service log by only journalctl. The config file is /etc/systemd/journald.conf.\nsudo journalctl sudo journalctl -k # kernel log sudo journalctl \u0026ndash;since yesterday sudo journalctl -f # follow the log sudo journalctl _PID=1 # the log of the specific process sudo journalctl -u # the log of the specific service Reference\nThe systemd document systemd-tutorial-commands "},{"id":23,"href":"/posts/the-tips-about-dockerfile/","title":"The Tips About Dockerfile","section":"Blog","content":"Normally, we often write a Dockerfile in the current directory.\nThe Dockerfile is a configuration file that describes how to build the image. You can refer to the official documentation for more details. If you list more than one CMD, only the last one takes effect. So if you have multiple commands to run, you better write them in a script file. Docker is not the VMware, there is no systemd in the container. Its startup program is the container application process. The container exists for the main process. Once the main process exits, the container loses its meaning of existence and thus exits. So when you execute multiple commands and if they are blocking, you better write the previous commands in nohup and the last command in the blocking command. (never use the command such as CMD service nginx start, the CMD only will execute as CMD [ \u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;service nginx start\u0026quot;], when the sh is executed, the container will exit, the correct way is run it directly CMD [\u0026quot;nginx\u0026quot;, \u0026quot;-g\u0026quot;, \u0026quot;daemon off;\u0026quot;]) Then, run the following command to build the image:\ndocker build -t my_image:1.0 .: the -t means tag, the . means the current directory(actually, it is the context of the dockerfile, but considering many people only use the same method, so here call it current). Besides, you can also build from .tar.gz file.\nYou can just refer to the official docs, but there is only one command that you should pay attention to: ENTRYPOINT.\nIf you want to tell the difference between CMD and ENTRYPOINT, you should first understand the shell pattern and the exec pattern.\nexec pattern # The feature of exec pattern is that it will not pass the command through the shell. So the environment variables such as $HOME will not be passed.\nCMD [ \u0026#34;echo\u0026#34;, \u0026#34;$HOME\u0026#34; ] ... run docker run ... ... output: $HOME But use the exec to run the shell you can get the correct result.\nCMD [ \u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo\u0026#34;, \u0026#34;$HOME\u0026#34; ] ... run docker run ... ... output: /root shell pattern # The shell pattern will execute the command via /bin/sh -c \u0026quot;task command\u0026quot;, which means the no.1 process is not the task process but the bash process.\nCMD top ... run docker run ... PID1 /bin/sh -c top PID7 top CMD # There are three ways to use CMD.\nCMD [\u0026quot;executable\u0026quot;,\u0026quot;param1\u0026quot;,\u0026quot;param2\u0026quot;] (exec pattern) CMD [\u0026quot;param1\u0026quot;,\u0026quot;param2\u0026quot;] (provide the entrypoint parameters) CMD command param1 param2 (shell pattern) == CMD [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;command param1 param2\u0026quot;] The both pattern of CMD command will be overwritten by the command in the end of the docker run command. The overwrite command will also run in the same pattern.\n# Example 1 CMD echo \u0026#34;hello\u0026#34; # docker run my_image:1.0 top # top # Example 2 ENTRYPOINT [\u0026#34;/bin/echo\u0026#34;, \u0026#34;Hello,\u0026#34;] CMD [\u0026#34;world!\u0026#34;] # docker run myimage \u0026#34;GPT3\u0026#34; # Hello, GPT3 ENTRYPOINT # There are two ways to use ENTRYPOINT.\nENTRYPOINT [\u0026quot;executable\u0026quot;,\u0026quot;param1\u0026quot;,\u0026quot;param2\u0026quot;] (exec pattern) ENTRYPOINT command param1 param2 (shell pattern) In exec pattern, the ENTRYPOINT command will not be overwritten by the command in the end of the docker run command. Such as docker run my_image:1.0 -c. The -c will not overwrite the ENTRYPOINT [ \u0026quot;top\u0026quot;, \u0026quot;-b\u0026quot; ], but it will be added to the ENTRYPOINT command as ENTRYPOINT [ \u0026quot;top\u0026quot;, \u0026quot;-b\u0026quot;, \u0026quot;-c\u0026quot; ].\nIn shell pattern, your custom command will be ignored by the ENTRYPOINT command. Such as docker run my_image:1.0 -c. The -c will be ignored by the ENTRYPOINT top, and the command will still be top.\nYou can also overwrite the ENTRYPOINT command by using the --entrypoint xxx follow the docker run command.\nYou can choose CMD or ENTRYPOINT according to your specific needs.\nIf you want to create an image with default behavior, and allow users to override the default behavior, you can use CMD. If you want to create an image that always executes a specific command, and allows users to pass parameters, you can use ENTRYPOINT. At the same time, CMD and ENTRYPOINT can also be used together, in which case the parameters specified in CMD will be used as the default parameters for the command specified by ENTRYPOINT. conclusion # shellmode: The main process(pid 1) is the /bin/sh process, which can resolve the environment variables. execmode: The main process is the command you specify, and the environment variables will not be resolved. If ENTRYPOINT uses shellmode, the default CMD instruction will be ignored. If ENTRYPOINT uses execmode, the content specified in default CMD instruction will be appended as parameters for the command specified by ENTRYPOINT. If ENTRYPOINT uses execmode, the default CMD instruction should also use exec mode. "},{"id":24,"href":"/posts/docker-cheatsheet/","title":"Docker Cheatsheet","section":"Blog","content":"This is a cheatsheet of docker.\nNote: the docs will be updated from time to time.\nDeamon # docker info systemctl start | stop | restart | status | enable docker This command to operate the docker daemon. For more details, you can refer to the systemd. docker system df query the disk usage of the docker images # docker push \u0026lt;username\u0026gt;/\u0026lt;image_name\u0026gt; docker images docker pull | inspect | rmi ubuntu:20.04 normally, the image name is composed by registry/username/repository:tag, if there is no username, the default is library, which is the official repository. if there is no registry, the default is docker.io, which is the official registry. docker create -it ubuntu:20.04 create a container by the image docker tag image_name:tag new_image_name:new_tag docker export/import and docker save/load： export/import will discard history and metadata information, only saving the snapshot state of the container at the time docker export -o xxx.tar CONTAINER docker import xxx.tar image_name:tag save/load will save the complete record, with a larger volume docker save -o xxx.tar image_name:tag docker load -i xxx.tar docker hub # docker login -u \u0026lt;username\u0026gt; docker search \u0026lt;image_name\u0026gt; docker push \u0026lt;username\u0026gt;/\u0026lt;image_name\u0026gt; dangling image: if the image is updated by official, and the tag is allocated to the new image, the old image will be called dangling image. Only display \u0026lt;none\u0026gt; in the docker images command. containers # docker ps -a list all containers docker ps list running containers docker stats search all the containers resource usage (CPU, memory, storage, network) docker rename CONTAINER1 CONTAINER2 docker start | stop | restart | rm | top | inspect | kill ｜ port | history CONTAINER docker run -itd ubuntu:20.04 search and run a container (-i means stdio -t means pseudo-tty -d means detach) == pull + create + start eg. docker run -p 20000:22 --name my_docker_server -itd docker_images:1.0 For the -itd effection, you can refer to this blog. docker attach CONTAINER ⌃ + p and ⌃ + q which can detach the container ⌃+d which can close and exit the container（exit; then the container will be stopped） docker logs -f CONTAINER -f means follow, you can see the logs in real time docker cp xxx CONTAINER:xxx docker exec CONTAINER COMMAND eg docker exec -it container_name bash docker exec -it container_name /bin/bash if your garrison program is sshd(which is not accept input) not the bash, then you should use this command to substitute it. (Recommend, exit; the container will not be stopped) You can also use docker run -it container_name /bin/bash specific the shell to enter the container. docker update CONTAINER --memory 500MB docker container prune remove all stopped containers docker commit container_name image_name:tag Package the container as an image.(Not recommend use to build image, can use as a snapshot of the container) Docker volume # The volume will not be deleted when the container is deleted. Volume can be shared between containers.\ndocker volume create VOLUME create a volume docker run --mount source=VOLUME,target=/webapp option to specify the volume docker volume ls list all volumes docker volume rm VOLUME remove the volume docker volume prune remove all unused volumes docker volume inspect VOLUME inspect the volume docker rm -v CONTAINER remove the container and the volume Besides, you can also mount the host directory to the container. The host directory path must be absolute path.\ndocker run -d -P \\ --name web \\ --mount type=bind,source=/src/webapp,target=/opt/webapp[,readonly] \\ training/webapp \\ python app.py For the detail of build docker image, you can refer to the tips about dockerfile.\n"},{"id":25,"href":"/posts/docker-101/","title":"Docker 101","section":"Blog","content":"Docker is a practical tool for everyday use, and like Git, you can learn it in just 30 minutes.\nDocker 101 # Why docker # Traditionally, it is believed that after the completion of software coding development/testing, the output is a program or executable binary bytecode, such as java. In order to enable these programs to execute smoothly, the development team also has to prepare complete deployment files and a running environment, so that the operation and maintenance team can deploy the application. The emergence of Docker enables the packaging, from bottom to top, of the system environment required to run the application, excluding the operating system kernel, through images.\nDocker Concepts # Docker itself is a container runtime carrier or a management engine. Docker is an open source project based on the Go. The main goal of Docker is \u0026ldquo;Build, Ship and Run Any App, Anywhere\u0026rdquo;, addressing the issues of software containers regarding the running environment and configuration.\nimage # A Docker Image is a read-only template. Many containers can be created from the image.\nYou can imagine the image as the class and the container as the instance.\ncontainer # Containers can be regarded as a simplified Linux environment, including root user privileges, process space, user space, network space, etc., as well as the applications running within them.\nrepository # A repository is a centralized place for storing image files. It is similar to a Maven repository, which is where various jar packages are stored, and a GitHub repository, where various Git projects are stored. The official registry provided by Docker, Inc. is called Docker Hub。\nDocker Workflow # Docker is a Client-Server structured system. The Docker daemon runs on the host machine and can be accessed from the client via a Socket connection. The daemon receives commands from the client and manages the containers running on the host, similar to MySQL.\nDocker is a C/S mode architecture. The backend is a loosely coupled architecture, with numerous modules separated and each performing its own functions.\nThe basic process of running Docker is as follows:\nUsers use the Docker Client to establish communication with the Docker Daemon and send requests to the latter. The Docker Daemon, as the main part of the Docker architecture, first provides the Docker Server function so that it can accept requests from the Docker Client. The Docker Engine executes a series of internal tasks in Docker, and each task exists in the form of a Job. During the running of a Job, when a container image is needed, the image is downloaded from the Docker Registry, and the downloaded image is stored in the form of a Graph through the image management driver, Graph driver. When creating a network environment for Docker, the Docker container network environment is created and configured through the network management driver, Network driver. When operations such as restricting the running resources of a Docker container or executing user instructions are required, it is completed through the Exec driver. Libcontainer is an independent container management package. Both the Network driver and the Exec driver use Libcontainer to implement specific operations on containers. Docker components # Before we talk about Docker, we need to understand some basic concepts about linux.\nbootfs(boot file system): contains the kernel and the bootloader. The bootloader is used to load the OS kernel into memory and start it. Then the bootfs will be unloaded and release some memory space. rootfs(root file system): contains the OS kernel and the root directory. The rootfs is used to store the OS kernel and the root directory, such as /dev, /proc, /bin, /etc, /lib, /usr, and /tmp etc. When starting the system, the rootfs will be mounted as read-only. After the system is started, the rootfs will be mounted as read-write. UnionFS: (below from wiki) It allows files and directories of separate file systems, known as branches, to be transparently overlaid, forming a single coherent file system. Contents of directories which have the same path within the merged branches will be seen together in a single merged directory, within the new, virtual filesystem. When mounting branches, the priority of one branch over the other is specified. So when both branches contain a file with the same name, one gets priority over the other. The different branches may be either read-only or read/write file systems, so that writes to the virtual, merged copy are directed to a specific real file system. This allows a file system to appear as writable, but without actually allowing writes to change the file system, also known as copy-on-write, which means that the modification of the read-only file system can be saved to the writable file system. The startup process of a computer:\nPOST (Power-On Self-Test), this process is mainly executed by the computer\u0026rsquo;s BIOS (Basic Input/Output System) or UEFI (Unified Extensible Firmware Interface). BIOS/UEFI checks whether the computer\u0026rsquo;s hardware, such as memory, hard disk, CPU, etc., is working properly. After completing POST, the next task of BIOS/UEFI is to find and load the boot loader. The boot loader（such as GRUB - Grand Unified Bootloader） is responsible for loading the kernel(vmlinuz in the /boot) and the rootfs. The kernel init: Device test and driver loading, the memory paging. And then init the first process PID 1, which is init or systemd. Then other services will be started, such as GNOME Display Manager, etc. For the PID 1 and the systemd, you can refer my blog the systemd. So the docker is based on layers. When docker run a container and do some changes, it will just add the writable layer on the other layers and this layer is so-called container.\nRethinking: what the advantage of layers?\nAnswer: Some images may require the same dependencies, thus they can use the shared layers to save the storage space. This can be observed in the pulling of images, if the layer which is necessary for the image already exists, the images will use the cached layer directly.\nThe maximum number of UnionFS layers is 127.\nIn the dockerfile, every RUN command will create a new layer. So you should use\nRUN xxxx \u0026amp;\u0026amp; xxxx \\ \u0026amp;\u0026amp; xxxx to reduce the number of layers. And in the end, you should also clean the cache to make this layer as small as possible.\nCompare # Traditional Virtual Machine Technology:\nA virtual machine (VM) is a solution that includes an environment installation. It can run one operating system within another operating system. A hypervisor (such as VMware) virtualizes a set of OS. It virtualizes a set of hardware, on which a complete operating system runs, and within that system, the required application processes are executed. Docker:\nThe application processes inside a container run directly on the host\u0026rsquo;s kernel. The container does not have its own kernel and does not perform hardware virtualization. It directly uses the hardware resources of the physical machine, isolating the processes. Each container is isolated from one another, each having its own file system. Processes in different containers do not affect each other, allowing for the distinction of computing resources. Docker is kernel-level virtualization, which does not require reloading an operating system kernel like a virtual machine. This avoids the time-consuming and resource-intensive process of seeking and loading the operating system kernel. IMPORTANT: Don\u0026rsquo;t make the docker Virtualization!\nWhat it means? Indeed, the docker is stateless, which means that it only run its service process, and you can check it logs through docker logs. The docker should only running for its service process, every time the service is down(the docker also quits), the K8S can compose a new service docker to start. You are not supposed to keep the docker running when the service process is down. That is not suitable for debug and maintainance.\nThe pros are:\nThe docker can easily receive the SIGTERM of docker stop or api to quit elegantly.(The SIGTERM will be sent to the main process pid 1, so don\u0026rsquo;t docker run with bash or use \u0026amp; to run the main process in the background which will not receive the SIGTERM) If the service is down, you can easily check it from outsides. You can easily adjust the configuration if it is amounted from host to the images. You can deal with the logs via docker logs to check the service logs. You can use k8s to face the some pressures in a decoupled way. Docker Installation # Docker is not a universal container tool. It depends on an existing and running Linux kernel environment.\nDocker essentially creates an isolated file environment within a running Linux system. As a result, its execution efficiency is nearly equivalent to that of the deployed Linux host.\nTherefore, Docker must be deployed on a system with a Linux kernel. If other systems want to deploy Docker, they must install a virtual Linux environment. For example, in Windows, you should run Docker on your VMware Linux images.\nThe process you can refer to the official docs: https://docs.docker.com/engine/install/\nIn this process, you run docker run hello-world which will first search the image locally, if not found, it will search the image from the Docker Hub. And docker image pull it automatically, and then run it.\nPrequisite # Add current user to docker group # To avoid having to use the sudo command every time you use the docker command, you can add the current user to the docker group created during installation (refer to the official documentation).\nAliyun mirror acceleration # Only domestic developers refer:\nhttps://help.aliyun.com/zh/acr/user-guide/accelerate-the-pulls-of-docker-official-images\nmkdir -p /etc/docker\nvim /etc/docker/daemon.json\ndon\u0026rsquo;t forget restart\nsystemctl daemon-reload systemctl restart docker Now you have known the basic of docker, for more commands, you can refer to the docker-cheatsheet.\n"},{"id":26,"href":"/posts/the-instance-class-static-magic-method-in-python/","title":"The Instance Class Static Magic Method in Python","section":"Blog","content":"So what is the difference between the Instance method, the Class method and the Static method?\nInstance method # The normal method is defined with self as the first parameter. And it can only be called by the instance.\nClass method # The class method is mainly about the class.\nYou have to use the @classmethod to sign your type. So you can easily use the variable inside the class via cls.variable you cannot call the init method because they are called only the instance is created. You can call the function through ClassName.method_name(others not recommend) Static method # The static method is as a normal function. Only has the name related to the class. So that it cannot access the class variable and instance variable.\nYou can call the function via ClassName.method_name\nMagic method # And there is a special method called the magic method. The magic method is the method that has double underscores at the beginning and end of the method name. Such as __str__, __init__, etc.\nThe normal methods need to be called, but the magic method is called automatically when some events happen. So if you want to customize your class, you can override the magic method.\nThe most common operators, for loops, and class operations are all run on the magic method.\nNow I will introduce some common magic methods:\ninit # The __init__ method is the constructor of the class. It is used to initialize the instance of the class. And it will be called automatically when the instance is created.\nnew # It will also be called automatically when the instance is created. But it is the first method to be called, which is prior to __init__.\ndel # The __del__ method is the destructor of the class. It is used to destroy the instance of the class. And it will be called automatically when the instance is destroyed. (But there still exists some problems, if the interpreter is terminated, but the instance is not destroyed, the destructor will not be called.)\nMost of the magic methods are not commonly used. So I will not introduce them in detail. For more information, you can refer to this article.\nPrivate method # Besides, there is another special method called the private method. The private method is the method that has two underscores at the beginning of the method name. Such as __method_name.\nThe private method is used to hide the method from the outside. So that the method cannot be called by the outside, if you try that, you will only get an AttributeError：\u0026lsquo;xxx\u0026rsquo; object has no attribute \u0026lsquo;__attribute_name\u0026rsquo;.\nThe private method and attribute can be accessed by the instance internally, so you can use the public method to access them indirectly.\nIn deed, there is no real private method in Python, it just converts the method name to _ClassName__method_name() or _ClassName__attribute_name. You can use the dir() function to see the method and attribute inside the instance.\n"},{"id":27,"href":"/posts/the-review-and-plan-for-bilive/","title":"The Review and Plan for Bilive","section":"Blog","content":"从 12 月 10 号正式上线生产版本的 bilive 到现在，已经过去了一个月了。目前 bilive 的 star 数已经突破了 300，实际用户数应该在 20 ～ 30 之间。当然 star 的数据本身没有什么意义，我真正追求的是使用者，但是 star 数据能够反映出项目实际浏览量，与其他工具类项目相比，我对于 bilive 的定位更像是一个产品，如何让这个产品积累足够多的用户和反馈，是我目前主要面临的问题。\n项目的复盘 # 对于一个产品类型的项目，能在短暂的一个月里获得 300 的 star 数，如果放在工具类型的项目我认为是值得称赞的，但是对于一个产品类型的项目，我认为是一个正常的现象，复盘一下，它增长的原因可能在以下几个方面：\n实用：目前 b 站主要是年轻人居多，其中实际的开发者更是不计其数，因此一个实用的产品能减少很多理解的难度，加上 b 站录播员不计其数，因此对于目标用户来说项目是一个非常实用的工具。 门槛低：我在开发的时候尽可能地降低了项目的硬件门槛，针对性地将 ffmpeg 的压制，弹幕的渲染对照最低的配置进行了优化，像我宣传的一样，一台十年前的电脑也可以轻松运行，降低了用户的成本。 收入：项目的投稿还会存在一定的收益，一个随手能做并且完全不用监守的工具，还能有收入的项目，我相信很多人都会愿意去尝试。 详细的文档：我写了一个非常详细的文档，包括了项目的安装，使用，以及一些常见的问题，尽可能地降低用户的上手难度。 宣传：我认为宣传是一个不可缺少的环节，对于一个强绑定的工具，很难做到向当初字节宠物短视频一样的快速冷启动，因此我必须保持着一定频率的宣传，让项目保持在持续的曝光里。在之前的文章 Thinking About Advertisement from an Open Source Perspective 中我提到过这个项目推广的基本策略，在实际的用户积累中，我将项目主要在 linux.do 以及 v2ex 上进行了初步的宣传。这些渠道主要以垂直的开发者居多，在部署方面不会产生较大的问题。 创新：市面录播的工具很多，竞品也有很多，通过对于竞品的分析，我能大概预估出这个项目的规模至少是 k 级别的增长，针对于及时的用户反馈，我尽可能从各个角度满足用户的需求，从压榨硬件的性能到增加字幕，定制化的弹幕参数以及优化流程做了很多创新。 合规：对于内容平台，一个不可忽视的问题就是版权以及违规问题，在开始项目之前我就针对 b 站的版权情况做了详细的调查。 首先，b 站对于自己的生态内的主播都有内容相关的所有权，因此对于录播稿件，主播除非涉及到个人隐私或者法律引战的部分会审核下架，其他类型不会被投诉下架。 其次，b 站对于稿件审核较为宽松，很多时候总是能见到一些原版视频后面添一段不相关的片段或者直接在视频上贴一段评论就能上传成功，因为 b 站判断稿件是否重复或者违规条件卡的非常严格，只要不是完全一模一样，基本上不会被判定稿件违规。所以我针对弹幕的渲染以及识别字幕的方式很大程度上保证了视频内容的独一无二，保证了内容的合规性。 最后，最重要的还是 b 站的官方政策，在前两个月推出了官方在直播流中剪辑的方式，鼓励二创作者边看边剪，因此 bilive 的工具很大程度上和 b 站官方的步调保持一致，都是对于整体生态的一种补充。 面临问题以及计划 # 目前宣传以垂直的开发者居多，如果想要获得更加广泛的反馈以及积累用户，宣传是必不可少的。对于我这个应用的产品，短期的目标是获得 1k 次有效浏览，积累 100 个种子用户。我对于有效浏览的定义是用户至少仔细观看了项目的主页内容并且留下印象，在以后有相关需求时能够想到曾经看到过这个项目。由于 bilive 主要还是面向中文的观众，因此我在 Thinking About Advertisement from an Open Source Perspective 中提到的当时尤雨溪的推广路径很大程度上走不通，最近在 1000UserGuide 这个 repository 中发现了整理好的一些适合独立开发者和创业者推广产品的渠道，我大概分析了一下。目前还没有启用的宣传途径有： 阮一峰等开源作者以及自媒体的曝光。我认为 KOL 的作用还是非常大的，KOL 拥有最垂直类型的 followers，内容生态上一个简短的文章或者视频既不会让受众走马观花，也不会像重度的介绍以及文档让用户产生抵触的心里，对于能够留下印象的有效推广来说是一个不错的选择。 导航站：由于种种原因，我个人对于导航站不是特别看好。不过顺手能提交的我应该还会尝试。 常见的产品推广平台，例如 product hunt，即刻等。在这些平台上产品经理非常活跃，有时能收获另一个看待项目的视角。 个人影响力的曝光：我目前在一些博客平台上还有一定的 followers 基础，短时间内大规模推广应该会有一定的效果。 主流媒体平台的推广，例如知乎，b 站，小红书等，这一步更加偏向大众，等待稳定的生产版本发布后，会在这些平台进行推广。 2025 年 2 月更新，最近提交 bilive 到了 github 上一些开源项目收录的仓库。\n某些模块的自制与优化：目前上传部分采用的 biliup-rs 的模块，由于只是 cli 调用，少了很多优化内部调度或者异步的可能，因此我目前在做 python 重写工具并把它开源成 pip 库，方便更多人的使用的同时也能尽量避免下游工具故障引起的整个项目的阻塞。 平台的广度受限：b 站市场受众还是不如几个大型的流媒体以及直播平台广泛，因此产品的用户上限较其他平台的竞品来说会小很多。 语言的限制：目前 b 站主要还是针对中文用户，因此对于非中文用户也不会有使用的价值，对于产品的多样性来说，也会有一定的限制。 更加规范化的结构：由于最早项目是我在服务器上写的 shell 流程，后来用 python 重构，虽然在重构的过程中已经重写了很多设计，但是很多地方还是不合理，例如日志模块，从最早的 shell 版本之后我都一直将运行的结果重定向到日志文件中，这本身是存在问题的，如果很长时间内没有报错以及中断，日志文件会随着时间的增长越来越大，变得难以排查和维护。对于以后的 docker 将服务独立出来乃至 k8s 进行管理，没有良好的日志设计都是非常致命的。后期会考虑使用 logging 模块对于日志进行管理，并且将日志改为基于时间的分段，无需累计单次运行的日志内容，运行过程中基于此刻的日期进行日志的分割与输出，方便定位和维护，同时还能将日志留存在控制台，方便后期 docker 的状态监控。这意味着代码一定要重构，并且重构不止一点，目前项目结构设计过于冗余的坏处就是，开发者看起来不清楚，我之前听过很多开发者谈论自己的项目，说想要设计得复杂，让其他人看起来不能直接用，这种观点是非常错误的。我一直认为，开源项目只有足够清晰，让开发者更容易上手，才会有更多的人一起做，才会真正促进项目的发展，大型的开源项目是无法通过几个人就能完成的，就像一家两个人的公司过于集中的股权是无法上市的。目前我的项目问题就非常严重，我深知项目能快速积累用户是因为功能有创新，有意思，而并非从代码乃至项目结构上能吸引到更多的用户以及开发者。一个清晰规范的结构以及一份详细的说明以及开发文档，是项目长期发展的基础。 多平台的支持：根据收到的实际反馈，使用的用户大部分以 windows 为主，还有部分的 NAS 跑 docker 的用户，真正使用 linux 服务器的用户还是少之又少。虽然 windows 用户也能通过 wsl 进行部署，但是存在一定的部署门槛。纯 windows 系统的部署也会存在一些基础库的问题，例如 fcntl 关于锁的库主要操作的是 linux 的底层函数，因此不支持 windows。未来我会考虑针对 windows 平台进行针对性的适配。 "},{"id":28,"href":"/posts/the-overview-of-security/","title":"The Overview of Security","section":"Blog","content":"In today\u0026rsquo;s digital age, security is paramount. As we increasingly rely on technology for communication, commerce, and data storage, understanding the fundamentals of security becomes essential. This article provides an overview of key security concepts, including encryption, digest algorithms, and digital signatures. By exploring these topics, we aim to equip you with the knowledge to protect your digital assets and ensure the integrity and confidentiality of your information. Whether you\u0026rsquo;re a tech enthusiast or a professional in the field, this guide will offer valuable insights into the mechanisms that safeguard our digital world.\nEncryption # Symmetric Encryption # The same key is used for both encryption and decryption. The key is normally short. And the efficiency is high, so it is widely used in https communication and network transmission.\nThe most common symmetric encryption algorithm is AES, DES, 3DES, DESX, Blowfish, RC6.\nAsymmetric Encryption # The public key and the private key are used for encryption and decryption. The public key is widely distributed, while the private key is kept secret. Only the holder of the private key can decrypt/encrypt the data encrypted/decrypted by the public key.\nCons: The efficiency is low, it will take a long time to encrypt/decrypt the data. So it is only suitable for small data encryption. e.g. https communication pre-shared key、CA certificate、login authentication.\nThe most common asymmetric encryption algorithm is RSA, ECC, DSA, ECDSA, Diffie-Hellman.\nThe symmetric encryption can not be used for signing, only the asymmetric encryption can be used for signing. Verify file consistency will use the Information Digest Algorithm. Digest Algorithm # The digest algorithm is usually called as the hash algorithm.\nIt is a one-way encryption algorithm, which means that the original data can not be restored from the encrypted data. It will generate a fixed length of data, which is called the digest. The digest is unique for the original data, and the same original data will generate the same digest(in the same hash algorithm). Avoid collision. Normally can be considered as the compression of the original data. Above all, so it can be used for file integrity verification.\nMD series # MD2、MD4、MD5, and the MD5 is the most common one as well as secure and fast.\nMD5（Message Digest Algorithm）: Generally generate a 128-bit hash value(16 bytes), and the output is a 32-character hexadecimal string. Normally generate the .md5 or .md5sum file to make sure the file is not modified.\nSHA series # SHA (Secure Hash Algorithm): The length of SHA is longer than MD, so it is more secure to avoid collision. But the speed is lower than MD.\nSHA1 generate a 20 bytes(160 bits) hash value, SHA224 generate a 28 bytes (224 bits) hash value, SHA256 generate a 32 bytes(256 bits) hash value, SHA384 generate a 48 bytes(384 bits) hash value, SHA512 generate a 64 bytes(512 bits) hash value.\nDigital Signature # Normally, the digital signature will use the hash algorithm to generate the digest, and then use the asymmetric encryption algorithm to sign the digest.\nThe process is as follows:\nSender:\nUse the hash algorithm to generate the digest of the original data. Use the sender\u0026rsquo;s private key to encrypt the digest. Send the original data and the encrypted digest to the receiver. Receiver:\nUse the hash algorithm to generate the digest of the received data. Use the sender\u0026rsquo;s public key to decrypt the digest. Compare the decrypted digest with the received digest. If they are the same, it means that the data is not modified. encrypt(hash(plaintext), private_key_a) =============== (plaintext, signature) ===============\u0026gt; hash(plaintext) == decrypt(signature, public_key_a) ? However, this only resolve the Imitation and Tampering problem. But can not resolve the tapping problem. Because the text in the network is plaintext, and anyone who has the public key can decrypt the text. How about another way?\nencrypt(plaintext, public_key_b) =============== ciphertext ===============\u0026gt; decrypt(ciphertext, private_key_b) This way can resolve the tapping and Tampering problem(Because the three party can not get the receiver\u0026rsquo;s private key). But it can not resolve the Imitation problem.\nSo we can combine the two ways to make the data more secure. That is first use the public key to encrypt the plaintext, and then use the private key to sign the hash of the plaintext.\nencrypt(plaintext, public_key_b), encrypt(hash(plaintext), private_key_a) =============== (ciphertext, signature) ===============\u0026gt; plaintext = decrypt(ciphertext, private_key_b), hash(plaintext) == decrypt(signature, public_key_a) ? However, this process still has some problems:\nThe performance is low, because the asymmetric encryption is slow. If the distribution of public key is not secure, the attacker can temper the public key to his own public key, the tunnel will be intercepted by the attacker. For the performance issue # We can also use the symmetric encryption to encrypt the plaintext, but how to distribute the symmetric key is another problem. In deed, we often use the asymmetric encryption to encrypt the symmetric key. And then use the symmetric encryption to encrypt the plaintext and transfer.\nFor the public key distribution issue # To make sure the public key is correct, the public key will be signed by the CA. And CA will sign the public key with its private key and send them both (Digital Certificate) to the receiver. Then the receiver can use the CA\u0026rsquo;s public key to decrypt Digital Certificate to get the sender\u0026rsquo;s public key.\nIn the ssl/tls protocol, the most common situation is the http. The process is as follows:\nThe server developer register a domain and apply for a certificate from the CA. The server prepare an asymmetric key pair. Provide the information about the domain, and the public key public_key_server to the CA. The CA will verify the information, and then sign the public key with its private key to generate the Digital Certificate. server_certificate = (domain, validity, issuer, public_key_server, ..., signature = (encrypt(hash(domain, validity, issuer, public_key_server, ...), private_key_ca))) The server can deploy the server_certificate and the private key, then provides the service. The client normal already has the CA's public key, so when the client request the server, the client will receive the server_certificate. Then use the CA\u0026rsquo;s public key to verify the certificate and get the public_key_server. server ===== server_certificate =======\u0026gt; client: hash(domain, validity, issuer, public_key_server, ...) == decrypt(signature, public_key_ca) ? Then the client will generate a symmetric key, and use the public_key_server to encrypt the symmetric key. Then communicate with the server using the symmetric key. Moreover, the actual process is more complex, eg, the https uses the Two-way certification. In https step3, the client will send the client_certificate which contains the public_key_client to the server. The server will verify the certificate and get the public_key_client via the root certificate.\nThen send the supported cipher suite list to the server, the server will choose one cipher suite and send the suite via public_key_client to the client. Then generate the symmetric key and communicate with the server using the symmetric key.1\nMaybe you have attention the root certificate in the above process. What\u0026rsquo;s this?\nIn deed, there is a certificate chain2, as you can find in the browser:\nCertificate Hierarchy The end-entity certificate is the certificate of the server *.jianshu.com, and the intermediate certificate is the certificate of the CA. The root certificate is the certificate of the CA\u0026rsquo;s CA.\nCertificate Chain So where is the root certificate? Normally, the root certificate is installed in the browser or device OS(trusted root certificates), eg Lists of available trusted root certificates in macOS. Also you can download to install it.\nDigital signature algorithm # RSA signature algorithm is the most common one. Such as MD5withRSA means use the MD5 hash algorithm to generate the digest, and then use the RSA algorithm to sign the digest.\nDSA (Digital Signature Algorithm): It is a digital signature algorithm based on the discrete logarithm problem. Only can be used for signing, not for encryption.\nECDSA (Elliptic Curve Digital Signature Algorithm): It is a digital signature algorithm based on the elliptic curve. Secure and fast.\nhttps://www.jianshu.com/p/2b2d1f511959\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://support.dnsimple.com/articles/what-is-ssl-certificate-chain/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":29,"href":"/posts/how-to-publish-your-code-as-a-pip-module/","title":"How to Publish Your Code as a Pip Module","section":"Blog","content":"Last week, I have made a python cli tool. To make it more convenient to use, I want to publish it as a pip module, so I have made some research and mistakes, and finally succeeded.\nPrerequisites # Register a pypi account in the official website Apply a token in the pypi management page Make sure your module name will be unique, you can check it in the pypi search page Check your code # Make sure your code is in a \u0026ldquo;package\u0026rdquo; folder (Must have __init__.py file) Create a README.md Create a LICENSE Create the pyproject.toml file in the root folder of the package, you can use the following content as a template: [build-system] requires = [\u0026#34;hatchling\u0026#34;] build-backend = \u0026#34;hatchling.build\u0026#34; [project] name = \u0026#34;biliupload\u0026#34; # make sure your module name is unique version = \u0026#34;0.0.1\u0026#34; authors = [ { name=\u0026#34;timerring\u0026#34;}, ] description = \u0026#34;Login and upload videos to bilibili\u0026#34; readme = \u0026#34;README.md\u0026#34; license = { file=\u0026#34;LICENSE\u0026#34; } requires-python = \u0026#34;\u0026gt;=3.10\u0026#34; classifiers = [ \u0026#34;Programming Language :: Python :: 3\u0026#34;, \u0026#34;License :: OSI Approved :: MIT License\u0026#34;, \u0026#34;Operating System :: OS Independent\u0026#34;, ] dependencies = [ \u0026#34;PyYAML==6.0.2\u0026#34;, \u0026#34;qrcode==8.0\u0026#34;, \u0026#34;Requests==2.32.3\u0026#34;, \u0026#34;requests_html==0.10.0\u0026#34;, ] [project.urls] \u0026#34;Homepage\u0026#34; = \u0026#34;https://github.com/timerring/biliupload\u0026#34; The below is the basic template, if you are making a cli tool, you also need to add the project.scripts section.\nFor more details, you can refer to the official documentation\nBuild the package # Latest version of PyPA’s build is recommended python3 -m pip install --upgrade build Build the package(run this command from the same directory where pyproject.toml is located) python3 -m build You will see the following output in dist/ folder:\n.whl file: the built distribution .tar.gz file: the source distribution Upload distribution archive for TestPyPI # The TestPyPI is a isolated website of PyPI, you can upload your package to it first to check if it works.\nRegister on TestPyPI Apply a token in the TestPyPI management page install the twine package python3 -m pip install --upgrade twine upload the distribution packages via python3 -m twine upload --repository testpypi dist/* and you can check your package. Test your package in a new environment python3 -m pip install --index-url https://test.pypi.org/simple/ biliupload That maybe have some problems, you can refer to the Conflict problem section to fix it.\nUpload distribution archive formally # Make sure your production version for build is ready Upload the distribution archive via python3 -m twine upload dist/* Then you can pip it formally pip install biliupload Your can also refer to the official documentation if you want more details.\nAdditionally, you may also want to make it into a executable file, you can just use the pyinstaller package command pyinstaller -F biliupload/cli.py to do it.\nNote: pyinstaller is not a cross-platform compiler, you need to compile it on the platform you want to run it on via docker or some other tools. And if you want to Using Data Files from a Module, don\u0026rsquo;t forget to include it via pyinstaller --add-data \u0026quot;bilitool/authenticate/config.json:bilitool/authenticate\u0026quot; -F bilitool/cli.py.\nConflict problem # You may encounter the following error:\nThe conflict is caused by: biliupload 0.1.1 depends on pyyaml==6.0.2 biliupload 0.1.0 depends on pyyaml==6.0.2 To fix this you could try to: 1. loosen the range of package versions you\u0026#39;ve specified 2. remove package versions to allow pip to attempt to solve the dependency conflict That is because you are using the pyyaml package which is not exist in the testpypi, so you can refer to this stackoverflow to fix it via pip install -i https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple biliupload==0.1.0, which the --extra-index-url pointer to the pypi.org.\n"},{"id":30,"href":"/posts/some-good-things-to-share-about-the-packages/","title":"Some Good Things to Share About the Packages","section":"Blog","content":"As a developer, I have been using many packages in my projects. Sometimes I may even forget the existence of the packages. When I changed my devices or some one else asked me, it\u0026rsquo;s hard for me to remember everything to recommend. Thus I will try to list the things which I think are very helpful and useful to share with you. Maybe we can even make an \u0026ldquo;Annual Oscar Awards ceremony\u0026rdquo; for the things.\nNote: This post will be updated regularly.\nPython # Env # pipdeptree # Most of the time, when we use the pip list or pip freeze command, we can only get all packages that we have installed. We don\u0026rsquo;t know the dependencies of the packages, and that\u0026rsquo;s where the advantage of pipdeptree to show.\n# usage pipdeptree pip-autoremove # Sometimes we want to remove the specific package, but usually it has some dependencies packages, on top of know the tree structure and remove them one by one, you can try to use the pip-autoremove to remove them all.\n# usage pip-autoremove \u0026lt;package_name\u0026gt; pipreqs # The pipreqs package can help us to generate the most simplified requirements.txt file for the project.\n# usage pipreqs "},{"id":31,"href":"/posts/introduction-to-the-http-and-https-protocol/","title":"Introduction to the HTTP and HTTPS Protocol","section":"Blog","content":"HTTP protocol is the foundation of the Internet, and HTTPS is the secure version of HTTP. HTTP is an application layer protocol based on TCP/IP protocol. It does not involve packet (packet) transmission, mainly specifying the communication format between the client and the server, and the default port is 80.\nHTTP/0.9 # In the 1991, HTTP 0.9 was released, there is only one method: GET. When the tcp connection is established, the client sends a GET request to the server, and the server only returns the HTML resource. Then closes the connection.\nHTTP/1.0 # In the 1996, HTTP 1.0 was released, besides the data parts, every request and response should have a HTTP header. It added the following features:\nMultiple methods: GET, POST, HEAD Status codes: 100, 200, 300, 400, 500 Headers: Content-Type(text/plain, text/html, image/jpeg, video/mp4, etc. You can also define your own), Content-Encoding(Accept-Encoding, you can specify you can accept the encoding format, such as gzip, deflate, etc.), Date, Server, Last-Modified, ETag, Expires, Cache-Control. But you can only request once in a TCP connection. It will be closed after the request is completed. If you want to request multiple resources, you need to open multiple TCP connections.\nHTTP/1.1 # persistent connection # In the 1997, HTTP 1.1 was released. It introduced the persistent connection, which means that the TCP connection will not be closed after the request is completed. You can request multiple resources in the same connection. You can establish 6 connections at most to the same domain. When the server and client are idle for a period of time, the connection can be closed actively (The client send Connection: close).\nBut you can just send one request in the TCP connection at the same time(which is serial).\npipelining # It added the pipelining mechanism, client can send multiple requests without waiting for the response of the previous request, the requests are clarified by Content-Length: the length of request body. But the server still needs to respond in the order of the request. But the server needs to wait for all the operations to be completed before sending the response. Hence introduces the chunked transfer encoding Transfer-Encoding: chunked, which means the server will send the response in chunks. Just as the streaming mode.\npipelining from https://shurutech.com/the-evolution-of-http/ Host header and methods # Besides, it added the Host: example.com header, which means the server can distinguish different domains, along with other methods, such as PUT、PATCH、HEAD、OPTIONS、DELETE.\nCons # The cons is that the response will cause the Head-of-line blocking problem.\nHTTP/2 # frame # In the 2015, HTTP 2 was released. It is a binary protocol, which means the data is transferred in binary format, which is so-called \u0026ldquo;frame\u0026rdquo;. It uses the SPDY protocol, which is a protocol for improving the performance of the web, it can implement the multiplexing mechanism, that means the client can send multiple requests in the same TCP connection at the same time. And because the packet is divided into frames, the server can send the response in any order.\nstream # A request or response is called as stream. And every frame has a stream identifier, which can be used to identify the stream. The id of client streams are odd, and the id of server streams are even. And the client or server can dismiss the stream by sending RST_STREAM frame. The client can specify the priority of the stream.\nheader compression # Besides, in order to improve the performance of the web, it added the HPACK compression algorithm, which can compress the HTTP header. And establishs a header table, which can be used to index the header between the client and the server. So they can only send the index in the following requests.\nserver push # Meanwhile, the server can implement the server push mechanism, which means the server can push the resources to the client before the client requests.\nPush mechanism eg. You can see the Network tab in the browser, some headers of requests are provisional headers are shown, which means the server is pushing the resources to the client. Normally, the Size column is from disk cache or from memory cache, which means the resources are from the browser cache. You can also check the Protocol column, it is h2, which means the protocol is HTTP/2.\nSSE # Server-Sent Events (SSE) is a technology where a browser receives automatic updates from a server via HTTP connection(which is normally requests by the browser). It is a server push mechanism, which means the server can push the resources to the client without the client requests.\nCons: The SSE is the one-way serial communication, which means the server can only push the resources to the client Web APP.\nCons # H2 Server push cons: The server push of h2 can only push the resources that are in the cache, which means you cannot push the message directly to the client Web APP. The Web does not have the api to query the push events. So if you want to implement the message push to the client, you can combine the h2 and SSE. TCP Head-of-Line (HOL) Blocking: HTTP/2.0 experiences delays known as TCP head-of-line blocking, which occurs within the TCP layer. If a single packet in the TCP stream gets lost, all streams using that connection have to wait for it to be retransmitted. TCP HOL Blocking from https://simrankhanna.substack.com/p/unclogging-the-pipeline-how-http Packet Sequence Requirement: Each TCP packet must be received in a specific sequence due to assigned sequence numbers. If any packet is lost, subsequent packets are stalled. TCP Buffer Holding: Lost packets cause subsequent packets to wait in the TCP buffer until the missing packet is retransmitted and received. Impact on HTTP Layer: The HTTP layer, built on top of TCP, does not handle these TCP retransmissions. It only notices a delay when attempting to retrieve data from the socket. Inability to Process Received Data: Even if received packets contain a complete HTTP request or response, they cannot be processed until the lost packet is retrieved. HTTP/3 # UDP # In the 2021, HTTP 3 was released. It is a protocol based on UDP, which means the data is transferred in UDP packets(when specific stream lost packet, other streams are not affected, which will not cause the head-of-line blocking problem).\nQUIC # Because the UDP is the unreliable transmission. So it uses the QUIC（Quick UDP Internet Connections） protocol to ensure the reliability. The QUIC is also a protocol that needs three handshakes to establish a connection, the main purpose is to determine the connection ID. And it can implement the multiplexing mechanism. And because the packet is divided into frames, the server can send the response in any order.\nHandshake process from https://www.catchpoint.com/http3-vs-http2 Pros # The HTTP based on TCP, it use the source IP, source port, destination IP, destination port to determine a TCP connection. So if the IP or port is changed, the connection will be closed. And needs to establish a new connection through the TCP three-way handshakes and the TLS handshake. But the QUIC protocol uses the connection ID to determine the connection. So it can implement the multiplexing mechanism. So even the IP or port is changed, the message has the same connection ID, it can be identified as the same connection. HTTPS # Now the most browers recommend HTTPS instead of HTTP.\nHttps is not a new protocol, it is the secure version of HTTP. It is based on the HTTP protocol, and adds a layer of security. The http protocol will cause eavesdropping, tampering, and pretending issues. The SSL/TLS protocol is used to solve these problems: encrypted, verification, and certificates.\nThe process of https is as follows:\nthe client requests the public key from the server, and uses the public key to encrypt the data. the server uses the private key to decrypt the data after receiving the request. how to ensure the public key is not tampered?\nUse the CA to sign the public key. If the CA is reliable, then the public key is reliable.\nhow to optimize the performance?\nUse the symmetric encryption key(session key) to encrypt the data. And the public key used to encrypt the symmetric encryption key.\nThe TLS handshake process is as follows (the handshake process is plaintext).\nClientHello # the protocol version, eg TLS 1.0 a random number(Client random) generated by the client, which will be used to generate the \u0026ldquo;session key\u0026rdquo; the supported encryption methods, such as RSA public key encryption the supported compression methods ServerHello # Confirm the encrypted communication protocol version used, eg TLS 1.0 a random number(Server random) generated by the server, which will be used to generate the \u0026ldquo;session key\u0026rdquo; confirm the encryption method, such as RSA public key encryption send the server certificate, and the server\u0026rsquo;s public key in the certificate If the server needs to confirm the client\u0026rsquo;s identity, it will include an additional request to require the client to provide a \u0026ldquo;client certificate\u0026rdquo;. For example, financial institutions often only allow authenticated customers to connect to their network, and will provide a USB key to formal customers, which contains a client certificate.\nClient Response # verify the server certificate. If the certificate is signed by a trusted CA, or the domain in the certificate is the same as the domain in the request or the certificate is expired, the client will warn the user. If the certificate is signed by a trusted CA, the client will use the public key in the certificate to encrypt the data.\na random number(which is so-called \u0026ldquo;pre-master secret\u0026rdquo;). The random number is encrypted by the server\u0026rsquo;s public key, which is used to prevent eavesdropping. (The three random numbers are used to generate the \u0026ldquo;session key\u0026rdquo;, which maximally guarantees randomness to prevent the key from being eavesdropping). encoding change notification, which means the subsequent information will be sent using the encryption method and key agreed by both parties. client handshake end notification, which means the client\u0026rsquo;s handshake phase has ended. This item is also the hash value of all the previous sent content, which is used to verify the server. Server Response # Decrypt the \u0026ldquo;pre-master secret\u0026rdquo; using the server\u0026rsquo;s private key. Use the three random numbers to generate the \u0026ldquo;session key\u0026rdquo;.\nencoding change notification. server handshake end notification, which means the server\u0026rsquo;s handshake phase has ended. This item is also the hash value of all the previous sent content, which is used to verify the client. Now the following communication uses the symmetric encryption key(session key) to encrypt the HTTP data.\nThe asymmetric encryption is only used in the handshake process.\nThe whole process is as follows:\nTLS handshake process from cloudflare And due to the asymmetric encryption keys are only used once, so if someone(eg. bank) wants to use the cdn service, but he doesn\u0026rsquo;t want to submit the private key to the cdn provider, then he can keep it in his server, and use it to encrypt and decrypt, and other process will be done by the cdn provider. As you can see, the bank\u0026rsquo;s server only needs to process the step 4.\nKeyless process from Cloudflare Eavesdropping # The TLS handshake process is plaintext, so it is easy to be eavesdropped. The eavesdropper can get the encryption method and the two random numbers. So whether the communication is really secure depends on the \u0026ldquo;pre-master secret\u0026rdquo; is not cracked.\nIn theory, as long as the server\u0026rsquo;s public key is long enough, such as 2048 bits, it is impossible to crack the \u0026ldquo;pre-master secret\u0026rdquo;. But if you pursue the ultra security, you can use the Diffie-Hellman algorithm, which will only need to exchange the DH parameters. And they can calculate the \u0026ldquo;pre-master secret\u0026rdquo; together.\nResume the session # If the session is disconnected, then needs to re-handshake. There are two ways to resume the session:\nSession ID: When the client reconnects, it will send the session ID to the server, and the server will use the session ID to find the \u0026ldquo;session key\u0026rdquo; in the memory. But it still has problem, the session ID is only stored in one server, if the client sends it to other server, the session won\u0026rsquo;t be resumed. Session resume from Cloudflare Session Ticket: When the client reconnects, it will send the session ticket(which is sent by the server at the end of the TLS handshake) to the server, and the server will use the ticket key to decrypt the session ticket and get the \u0026ldquo;session key\u0026rdquo;. Session resume from Cloudflare SSL and TLS # In brief, TLS is the successor of SSL. The SSL(Secure Sockets Layer) protocol is deprecated, and nearly does not be used. And the TLS(Transport Layer Security) protocol is the standard. Due to the historical reasons, the TLS certifcate sometimes is called as SSL certificate, but it is not the same.\nReferences # https://www.catchpoint.com/http3-vs-http2 https://shurutech.com/the-evolution-of-http/ https://simrankhanna.substack.com/p/unclogging-the-pipeline-how-http https://www.ruanyifeng.com/blog/2014/02/ssl_tls.html https://blog.cloudflare.com/tls-session-resumption-full-speed-and-secure/ "},{"id":32,"href":"/posts/mail-service-and-protocol/","title":"Mail Service and Protocol","section":"Blog","content":"Recently, I have been working on DNS of my domain name. And then I need to set up the mail service of my domain name. When I tried many times, I always failed to receive the mail on my phone. Suddenly, I remembered that I didn\u0026rsquo;t set up IMAP service. So let\u0026rsquo;s review the mail service and protocol.\nThe most common mail service is SMTP, POP3, and IMAP.\nSMTP # SMTP（Simple Mail Transfer Protocol）is used to transfer mail between mail servers. It is a protocol based on TCP/IP, and its main task is to define how mail is sent from the sender\u0026rsquo;s mail server to the receiver\u0026rsquo;s mail server. SMTP uses a client/server model, with the sender\u0026rsquo;s mail server acting as the client connecting to the receiver\u0026rsquo;s mail server.\nEmail is submitted by a mail client (mail user agent, MUA) to a mail server (mail submission agent, MSA) using SMTP on TCP port 465. The MSA delivers the mail to its mail transfer agent (MTA). And this process can be done by multiple machines. The boundary MTA uses DNS to look up the MX (mail exchanger) record for the recipient\u0026rsquo;s domain (the part of the email address on the right of @). The MX record contains the name of the target MTA. Based on the target host and other factors, the sending MTA selects a recipient server and connects to it to complete the mail exchange. Once the final hop accepts the incoming message, it hands it to a mail delivery agent (MDA) for local delivery. Once delivered to the local mail server, the mail is stored for batch retrieval by authenticated mail clients (MUAs), using Internet Message Access Protocol (IMAP), a protocol that both facilitates access to mail and manages stored mail, or the Post Office Protocol (POP) which typically uses the traditional mbox mail file format MIME（Multipurpose Internet Mail Extensions）is a standard that extends the format of email messages to support text in character sets other than ASCII, and to allow audio, video, images, and application data to be included in email.\nThe default port is 25, but it is not secure, so it is recommended to use port with SSL/TLS encryption, such as 587(TLS) or 465(SSL). Details depend on the ISP.\nPOP3 # POP3（Post Office Protocol version 3）is a protocol used to download mail from the mail server to the local device. After downloading, the mail on the server is usually deleted or marked as read. It is suitable for users who check mail occasionally through a single device. The operation of POP3 is one-way, which means that the client\u0026rsquo;s operations are not reflected on the server.\nThe default port is 110, but it is not secure, so it is recommended to use port with SSL/TLS encryption, such as 995(SSL).\nIMAP # IMAP（Internet Message Access Protocol）is a protocol used to receive mail. It is similar to POP3, which is a mail retrieval protocol. Its main function is to allow mail clients (such as iPhone, Foxmail) to retrieve mail information and download mail from the mail server. IMAP is designed to provide a two-way communication between the webmail and the mail client, so that the client\u0026rsquo;s operations can be reflected on the server.\nThe default port is 143, but it is not secure, so it is recommended to use port with SSL/TLS encryption, such as 993(SSL).\n"},{"id":33,"href":"/posts/understanding-clash-through-configuration/","title":"Understanding Clash Through Configuration","section":"Blog","content":"We can learn the proxy process of clash through the configuration file.\nClash # Clash is a popular tool which can use different protocols to access the resources.\nTo use the proxy, you may have received the subscription link. So what\u0026rsquo;s inside? We can dig it out.\nAbout configuration # When you fill the subscription link into the Clash, you will get a configuration file.\nI select some example items from the configuration file. And we\u0026rsquo;re able to conduct a detailed analysis.\nA sample configuration file mixed-port: 7890 allow-lan: true bind-address: \u0026#39;*\u0026#39; mode: rule log-level: info external-controller: \u0026#39;127.0.0.1:9090\u0026#39; dns: enable: true ipv6: false default-nameserver: [223.5.5.5, 119.29.29.29] enhanced-mode: fake-ip fake-ip-range: 198.18.0.1/16 use-hosts: true nameserver: [\u0026#39;https://doh.pub/dns-query\u0026#39;, \u0026#39;https://dns.alidns.com/dns-query\u0026#39;] fallback: [\u0026#39;https://doh.dns.sb/dns-query\u0026#39;, \u0026#39;https://dns.cloudflare.com/dns-query\u0026#39;, \u0026#39;https://dns.twnic.tw/dns-query\u0026#39;, \u0026#39;tls://8.8.4.4:853\u0026#39;] fallback-filter: { geoip: true, ipcidr: [240.0.0.0/4, 0.0.0.0/32] } proxies: - { name: HK02, type: ss, server: domain-or-ip, port: port-num, cipher: chacha20-ietf-poly1305, password: a-example-password, udp: true } - { name: \u0026#39;HK03 x2\u0026#39;, type: ss, server: domain-or-ip, port: port-num, cipher: chacha20-ietf-poly1305, password: a-example-password, udp: true } proxy-groups: - { name: custom-policy-group, type: select, proxies: [HK02, \u0026#39;HK03 x2\u0026#39;] } - { name: Netflix, type: select, proxies: [HK02, \u0026#39;HK03 x2\u0026#39;] } - { name: auto, type: url-test, proxies: [HK02, \u0026#39;HK03 x2\u0026#39;], url: \u0026#39;http://www.gstatic.com/generate_204\u0026#39;, interval: 86400 } - { name: fallback, type: fallback, proxies: [HK02, \u0026#39;HK03 x2\u0026#39;, \u0026#39;Antarctic\u0026#39;], url: \u0026#39;http://www.gstatic.com/generate_204\u0026#39;, interval: 7200 } rules: - \u0026#39;DOMAIN-SUFFIX,services.googleapis.cn,custom-policy-group\u0026#39; - \u0026#39;DOMAIN-KEYWORD,netflixdnstest,Netflix\u0026#39; - \u0026#39;DOMAIN,netflix.com.edgesuite.net,Netflix\u0026#39; - \u0026#39;DOMAIN-SUFFIX,fast.com,Netflix\u0026#39; - \u0026#39;IP-CIDR,8.41.4.0/24,Netflix,no-resolve\u0026#39; - \u0026#39;IP-CIDR,23.246.0.0/18,Netflix,no-resolve\u0026#39; - \u0026#39;DOMAIN-KEYWORD,guanggao,REJECT\u0026#39; - \u0026#39;DOMAIN-SUFFIX,icloud.com,DIRECT\u0026#39; - \u0026#39;DOMAIN-SUFFIX,zhihu.com,DIRECT\u0026#39; - \u0026#39;DOMAIN-KEYWORD,gmail,custom-policy-group\u0026#39; - \u0026#39;IP-CIDR,91.108.4.0/22,custom-policy-group,no-resolve\u0026#39; - \u0026#39;IP-CIDR6,2001:67c:4e8::/48,custom-policy-group,no-resolve\u0026#39; - \u0026#39;DOMAIN-SUFFIX,cn,DIRECT\u0026#39; - \u0026#39;DOMAIN-KEYWORD,-cn,DIRECT\u0026#39; - \u0026#39;GEOIP,CN,DIRECT\u0026#39; - \u0026#39;MATCH,custom-policy-group\u0026#39; Basic # In the path, we can divide the locations where the traffic passes into three layers:\nFrom your traffic source to the proxy software, such as your browser to the Clash you use. From the proxy software to the remote server, such as from the Clash you use to a server in Hong Kong. From the remote server to the target server, such as from a server in Hong Kong to the Google server. But no matter what software you use, these software will listen on the specific port, which will take over the traffic to the port. Generally, the port will use socks5 or http proxy. Http only can proxy the http request. But socks5 can proxy the tcp or udp request.\nmixed-port: 7890 allow-lan: true bind-address: \u0026#39;*\u0026#39; mode: rule log-level: info external-controller: \u0026#39;127.0.0.1:9090\u0026#39; The mixed-port is the port that HTTP(S) and SOCKS4(A)/SOCKS5 proxy services share to listen on. The allow-lan is whether to allow connections from other LAN IP addresses. The bind-address is the address that the proxy software listens on(Only valid when allow-lan is true). The mode is the mode of the proxy software(rule, global, direct). The log-level is the log level of the proxy software(info / warning / error / debug / silent). The external-controller is the address that the RESTful Web API listens on.\nDNS # dns: enable: true # Whether to enable DNS. ipv6: false # Whether to enable IPv6. default-nameserver: [223.5.5.5, 119.29.29.29] # The default DNS server. enhanced-mode: fake-ip # Use fake IP to resolve the DNS request. https://www.rfc-editor.org/rfc/rfc3089 fake-ip-range: 198.18.0.1/16 # The CIDR of fake IP. use-hosts: true # Whether to use the hosts file to resolve. nameserver: [\u0026#39;https://doh.pub/dns-query\u0026#39;, \u0026#39;https://dns.alidns.com/dns-query\u0026#39;] # The DoH server. fallback: [\u0026#39;https://doh.dns.sb/dns-query\u0026#39;, \u0026#39;https://dns.cloudflare.com/dns-query\u0026#39;, \u0026#39;https://dns.twnic.tw/dns-query\u0026#39;, \u0026#39;tls://8.8.4.4:853\u0026#39;] # The fallback DNS server. fallback-filter: { geoip: true, ipcidr: [240.0.0.0/4, 0.0.0.0/32] } # The fallback filter. Redir-Host # The redir-host mode and fake-ip mode are the two modes of TAP/TUN. And most softwares will not use the system proxy, so the proxy system virtualize a network interface which takes over all the traffic(that\u0026rsquo;s what our phones do). So the TAP/TUN mode is working on the network layer, so it can not get the domain name, it can only get the IP address. So it will use other methods to get the domain name, the mothod is intercepting the DNS request at the 53 port and maintain a mapping table. When the terminal sends a DNS request, it will first check the mapping table, if the domain name is in the mapping table, then it will directly return the IP address. If the domain name is not in the mapping table, then it will send the DNS request to the DNS server, and then update the mapping table.\nYou can refer to my Real Computer Network to understand the OSI model.\nBesides, the TAP/TUN mode will not be able to encapsulate network layer data packets, so it isn\u0026rsquo;t the VPN(only can encapsulate the network layer data packets can implement the remote networking and can be called as VPN, the feature of VPN is obvious, that means it can be detected easily). Some commands in the network layer(eg. ICMP protocol) like ping will not work. The ttl is the latency between the terminal and the virtual network interface.(If it uses the fake-ip mode, then the ip of target domain such as google.com is also a fake ip).\nBut if multiple domains are deployed on the same IP, or are polluted to the same IP, then the redir-host will not work(except for Sniff). In this case, the clash will use the fallback DNS(nomorally use the overseas encrypted DNS server to avoid pollution) to resolve the DNS request. But the IP returned is just used as a key in the mapping table, does it really matter? That\u0026rsquo;s why the fake-ip mode appears.\nThe default nameserver means the DNS server that the local clash will use to resolve the DNS request, usually used in local resolution(direct mode). If the domain needs proxy, then use the fallback DNS to resolve the DNS request. Fallback means if the default DNS server returns a foreign IP(Always redirect to the unknown foreign IP), then use the fallback DNS servers return to make sure the foreign IP is not polluted.\nThe Classless Inter-Domain Routing (CIDR) is a method of allocating IP addresses. And the default CIDR of fake IP is 198.18.0.1/16, which is a reserved IP address. When the DNS request is sent to Clash DNS, the Clash kernel will allocate an idle fake-ip address from the pool through the management of the internal domain name and its fake-ip address mapping.\neg. You can find that the 198.18.1.79 is a fake IP, which is allocated by the Clash kernel.\nbase ❯ curl -v http://google.com \u0026lt;---- cURL asks your system DNS (Clash) about the IP address of google.com * Host google.com:80 was resolved. * IPv6: (none) * IPv4: 198.18.1.79 ----\u0026gt; Clash allocates 198.18.1.79 as google.com * Trying 198.18.1.79:80... \u0026lt;---- cURL connects to 198.18.1.79 tcp/80 ----\u0026gt; Clash will accept the connection immediately * Connected to google.com (198.18.1.79) port 80 ----\u0026gt; Clash looks up in its memory and found 198.18.1.79 being google.com ----\u0026gt; Clash looks up in the rules and sends the packet via the matching outbound \u0026gt; GET / HTTP/1.1 \u0026gt; Host: google.com \u0026gt; User-Agent: curl/8.7.1 \u0026gt; Accept: */* \u0026gt; * Request completely sent off \u0026lt; HTTP/1.1 301 Moved Permanently ...... Fake IP # Why fake IP?\nSome regions not only block the specific IP, but also pollute the DNS. In other words, the domain name is resolved to an incorrect IP. Due to the feature of TCP/IP, when the application initiates a TCP connection, it first sends a DNS question (sends an IP Packet), obtains the IP address of the server to be connected, and then directly connects to this IP address.\nSo we need to proxy the DNS request. The process is that the request needs to be examined locally first, (if needs proxy) then request the remote server DNS, then return the resolution result. Then the clash core maps the IP address to the domain, and then match the rules. In this process, you can find that the DNS request is not necessary. Because the IP returned is just used as a key with the value(domain), but if the fake ip occasionally match specific rules, and was redirected wrongly, which will cause the failure of visiting. Considering these issues, the official has stopped the redir-host mode directly. So, the Fake-IP technology appears.\nBut in fake IP, the clash core will directly return a fake IP to the client. And the client will use the fake IP to request the resource. Then in the clash core will map the fake IP to the domain name. And then use the domain name to match the rules. This will omit the DNS request. Which not only saves time, but also avoids the DNS leakage.\nBut it still has some problems, because any system has a DNS cache mechanism. If one day, due to some reason, you need to turn off the proxy software, then the client will not get any response when requesting this fake IP. You need to manually refresh the system DNS cache to solve it.\nproxies # Then let\u0026rsquo;s talk about the proxies. About the ratio of the proxy, I have written a lot in the Real Computer Network. You can review it.\nproxies: - { name: HK02, type: ss, server: domain-or-ip, port: port-num, cipher: chacha20-ietf-poly1305, password: a-example-password, udp: true } - { name: \u0026#39;HK03 x2\u0026#39;, type: ss, server: domain-or-ip, port: port-num, cipher: chacha20-ietf-poly1305, password: a-example-password, udp: true } The type is ss, which means the proxy is using the Shadowsocks protocol.\nThe udp is to enable the UDP proxy. Or you will not be able to proxy the UDP even in the tun mode.\nThe cipher is the encryption method of the proxy server. The chacha20-ietf-poly1305 is the encryption method of password. And the encryption methods which are supported by the Shadowsocks protocol are:\naes-128-gcm aes-192-gcm aes-256-gcm aes-128-cfb aes-192-cfb aes-256-cfb aes-128-ctr aes-192-ctr aes-256-ctr rc4-md5 chacha20-ietf xchacha20 chacha20-ietf-poly1305 xchacha20-ietf-poly1305 proxy-groups # proxy-groups: - { name: custom-policy-group, type: select, proxies: [HK02, \u0026#39;HK03 x2\u0026#39;] } - { name: Netflix, type: select, proxies: [HK02, \u0026#39;HK03 x2\u0026#39;] } - { name: auto, type: url-test, proxies: [HK02, \u0026#39;HK03 x2\u0026#39;], url: \u0026#39;http://www.gstatic.com/generate_204\u0026#39;, interval: 86400 } - { name: fallback, type: fallback, proxies: [HK02, \u0026#39;HK03 x2\u0026#39;, \u0026#39;Antarctic\u0026#39;], url: \u0026#39;http://www.gstatic.com/generate_204\u0026#39;, interval: 7200 } The proxy-groups can be understood as filters one after another. So when you send a request, at which filter it will be intercepted depends on the match between the request and the rules. You should choose a meaningful name for the proxy-group. The type mainly includes four types:\nselect: Select the proxy manually in the proxy-group. url-test: Test the response time of the proxy-group. And use the proxy with the shortest response time. The url is used to ensure the connection to network to test the response time. 204 is the response code of the no content page. For more information, you can refer to the rfc7231. fallback: Use the first proxy of the proxy-group, if breakdown, use the second proxy of the proxy-group\u0026hellip; load-balance: Use the proxy with the least number of connections. relay: The traffic will be relayed through the proxies in order in the proxy-group. (Not support UDP). rules # The basic format of the rules is:\nTYPE,ARGUMENT,POLICY(,no-resolve) rules: - \u0026#39;DOMAIN-SUFFIX,services.googleapis.cn,custom-policy-group\u0026#39; - \u0026#39;DOMAIN-KEYWORD,netflixdnstest,Netflix\u0026#39; - \u0026#39;DOMAIN,netflix.com.edgesuite.net,Netflix\u0026#39; - \u0026#39;DOMAIN-SUFFIX,fast.com,Netflix\u0026#39; - \u0026#39;IP-CIDR,8.41.4.0/24,Netflix,no-resolve\u0026#39; - \u0026#39;IP-CIDR,23.246.0.0/18,Netflix,no-resolve\u0026#39; - \u0026#39;DOMAIN-KEYWORD,guanggao,REJECT\u0026#39; - \u0026#39;DOMAIN-SUFFIX,icloud.com,DIRECT\u0026#39; - \u0026#39;DOMAIN-SUFFIX,zhihu.com,DIRECT\u0026#39; - \u0026#39;DOMAIN-KEYWORD,gmail,custom-policy-group\u0026#39; - \u0026#39;IP-CIDR,91.108.4.0/22,custom-policy-group,no-resolve\u0026#39; - \u0026#39;IP-CIDR6,2001:67c:4e8::/48,custom-policy-group,no-resolve\u0026#39; - \u0026#39;DOMAIN-SUFFIX,cn,DIRECT\u0026#39; - \u0026#39;DOMAIN-KEYWORD,-cn,DIRECT\u0026#39; - \u0026#39;GEOIP,CN,DIRECT\u0026#39; - \u0026#39;MATCH,custom-policy-group\u0026#39; DOMAIN: route the request match the DOMAIN to the POLICY. DOMAIN-SUFFIX: route the request match the domain suffix to the POLICY. DOMAIN-KEYWORD: route the request match the domain keyword to the POLICY. IP-CIDR: route the request match the IPv4 address to the POLICY. (if you want to skip DNS resolution, add the no-resolve option, then the domain will not be resolved, so it cannot match the rules, and directly skip this rule) IP-CIDR6: route the request match the IPv6 address to the POLICY. (if you want to skip DNS resolution, add the no-resolve option) GEOIP: route the request match the country to the POLICY. Use the geolite2 database. Clash will resolve the domain name to the IP address, then find the country code of the IP address. If you want to skip DNS resolution, add the no-resolve option. eg. 'GEOIP,CN,DIRECT' redirect the request targeting CN ip to the DIRECT policy. MATCH: route the remaining request to the POLICY. For more info, you canrefer to the Clash Rules.\nReference # https://www.pupboss.com/post/2024/clash-tun-fake-ip-best-practice/ https://blog.skk.moe/post/what-happend-to-dns-in-proxy/ https://www.youtube.com/watch?v=qItL005LUik "},{"id":34,"href":"/posts/thinking-about-advertisement-from-an-open-source-perspective/","title":"Thinking About Advertisement from an Open Source Perspective","section":"Blog","content":"我在本科期间听到过很多关于广告论调，仍记得某位教授对于短视频所代表的互联网深恶痛绝，在课堂上批判广告的价值，抨击广告行业的浮躁。我身边的同学也大抵持相同的观点，认为互联网广告就是智商税，广告的目的是为了卖课，卖课的目的是为了收割。可事实真的如此吗？\n广告真的重要吗？ # 我们无法否认广告所呈现出来的种种弊端，但是广告仍发挥着至关重要的作用。作为网民日常生活的一部分，一天可能十分之一到五分之一的时间都在短视频或者各种流媒体之中，作为一个占比如此多的时间，其覆盖受众已经远远超过了大部分行业。\n上周在腾讯云线下听了章鱼老师的分享，印象深刻的一句话是“工科是一种视角”。\n这让我想到生活中总能听到一种论调“短视频也叫工作？”，持这种观点的人，可能总是以消费者的视角来看待短视频，但是在一个良好的生态中，消费者只是其次，生产者才是关键。有优质的生产者提供内容，才会有优质的消费者。\n生活中你没有意识到的广告有哪些？ # 我们作为广告的受众，可能平时看广告也是同一种视角：\n在公众的日常生活里，能接触到的广告无非是线上和线下两种。\n线下的广告通常是静态的、平面的。如果没有特定的场景，在大家都习惯性忘记的今天，路人扫一眼甚至有可能完全不会发现到广告，能留下印象都是无从说起。于是很多商家通过各种方式增加公众在广告上的停留，最出名的莫过于华与华设计的蜜雪冰城的广告歌曲，将品牌的概念通过听觉印入每一个消费者甚至是路人的脑中。还有商家会选择在目光不得不长时间停留的地方植入广告，例如电梯的屏幕，厕所的墙，工作人员的衣服，甚至每一个从写字楼里走出来的员工带着的工牌本质都是广告的一部分。\n线上广告更是五花八门，它甚至在你没有解锁手机，进入网络世界之前就已经产生了。各类应用会用推送广告证明它的存在。开屏同样会碰到各类防不胜防的广告。而内容同样是广告，目前广告行业最活跃的莫过于各类汽车广告，一些汽车行业的创始人争相拍短视频广告，为什么要这么做？后面的章节我会分析。由互联网广告还延伸出来了公域和私域的概念，公域是分发，多账号多平台多时间段的内容目的就是增大宣传广度，因为非常关键的点在于，没有用户量就没有优质用户。如果想获得垂直的优质用户，首先需要做的就是增大宣传广度。所谓的私域则是一种“提纯”的手段，通过拉人进群的方式，在每次广告时 @ 出每一个人，亦或是通过朋友圈的每天曝光，强制增加目标用户在广告上的停留时间，这就是增加宣传深度。\n我身边很多人都以互联网行业工作者，科技工作者自居，认为广告就是消费行业的专属，殊不知，其所看的“三大顶会”，AI 科技组织和公司早已将广告植入在各种类型的宣传中盈利了。\n广告真的有价值吗？ # 国内市场的用户没有付费订阅的习惯，因此一个互联网项目如果没有广告，那么不是在消失就是在消失的路上了。如果一个项目没有广告仍然能够维持，那它就一定不是真正的互联网项目，没有广度的用户，充其量只能算借助互联网宣传的项目。\n毫无疑问，广告就代表互联网的商业价值。信息的分发广度决定了信息主体的广告价值。胖东来短时间内以极低的成本推送到了 10 亿网民的面前，其广告的价值远大于企业自身盈利能力的价值。\n之前我关于网络的博客中参考的一篇公众号文章，作者是 IPIP 的创始人高春辉，根据报道，他早期做手机之家时，让他真正下定决心 all in 网站就是因为接到了广告业务。字节跳动早期也是通过广告业务探索来支撑，类似的例子还有很多。大多数时候人们关注的是技术本身，是产品本身，但是却忽略了广告本身，或者没有意识到广告的存在。\n我自己的广告实践 # 我最近做了一个开源项目 bilive 项目地址 https://github.com/timerring/bilive 文档地址 https://bilive.timerring.com 自动监听并录制B站直播和弹幕、通过 whisper 推理语音识别字幕并渲染弹幕和字幕，自动切片并通过智谱 GLM-4V-PLUS 大模型生成吸引眼球的标题，自动投稿视频和切片到平台，兼容无GPU版本，兼容超低配置服务器与主机。\n如何让项目通过广告推广，我做了一个小型的尝试，在某开发者交流论坛中，帖子显示的顺序是按照最后一位回复的时间先后排列，用户会在帖子中提出问题或者感谢，我每个小时回复一次用户，以确保帖子始终能在平台用户的视线之内。很快一周之内项目收获了数百 stars。\n因此，我对于开发者类型的论坛定位就是一个早期的微博，同早期的微博一样，最开始微博的发展处于一个混沌的时期，平台在移动互联网时期飞速扩张，但是相应的规则并不完善，因此博主在拓展粉丝的同时，还可以通过自己的受众和知名度对广告商进行议价，微博也很难有精力针对平台内部博主限制，因为创作者同样也是稀缺资源，人人网，饭否，网易等平台都在扩张，给与创作者最优质的收入，获得最好的广告效果是博主、平台和广告商三赢的过程。但是等到蓝海扩展结束，进入一个红海存量或者还在被新兴事物挤压缩小的阶段，平台就不得不考虑规范化以及维持生存的问题了。\n很多时候我们都只会因为局限站在一种角度思考问题，例如平台的粉丝都会认为博主应该专注创作“好活”，不应该接商单。但是由于国内没有付费订阅制的习惯，加上 toC 的版权很难得到有效的保障，因此注定了媒体传播的行业就是要为广告服务。因此创作者思考的绝对不是接或者不接广告的问题，而是如何优化广告的问题。\n广告并不是详细叙述，而是植入“感觉”，创造期待 # 慢慢运行 bilive 的账号也拥有了一定的粉丝，也有小型商家让我推送广告，从我接广告和商家的交流经验来说，商家在乎的是你本身的规模以及宣传效果。因此很多时候我认为广告并不是要详细介绍产品本身，而在与植入“感觉”。\n一个好的广告并不是要把每一个优点，每一项指标都贴在海报上，让用户驻足细看，用户很少有时间和耐心在广告上，而是要把一种“感觉”植入用户的内心。让用户看到这个商品就能想到这种“感觉”，从而创造期待，然后在逐渐的了解中，做到超出预期（当然要想做到超出预期，广告就一定要真实，如果广告内容是虚假的，结果一定适得其反）。\n特别认同一句话，你眼中的你不是你，别人眼中的你不是你，而你眼中的别人才是你。\n这也同时解释了我对于一个现象的疑问，有时在网上总会看到两个人为了比“谁支持的博主更有钱”争得不可开交，相互发评论几十条，起初我还不能理解，浪费这时间做点别的不好吗。后来思考后我才明白，他们表面上争论的是这个问题，本质上还是争论的自我认同的问题，为了证明“我的眼光更高”以及“我认同这种感觉”。\n因此一个好的广告应该植入用户内心，让其认同他眼中的别人都是这种“感觉”，从而间接认为别人眼中的他也是这种“感觉”。\n广告就是先熟悉再“重复” # 通常人对于陌生的新鲜事物会本能抗拒，因此广告的第一个作用就是打破陌生，让用户熟悉。同样两种商品，如果用户对其中一种熟悉，那么就会自然地选择熟悉的那一种。很少有人会为了一个完全陌生的事物在短时间内直接抉择。熟悉之后广告的重点就是重复，因为只有在重复中才能留下印象。如果用户无法接受三字经类型的重复，就换一种重复的方式，做到内容新鲜，内核重复。汽车创始人争相拍短视频，就是熟悉和重复的过程。因为“品牌“并非通过某个爆款形成，而是以一定的节奏在你信息流里持续曝光。\n复盘我所做的内容 # 回到我的项目bilive 我的宣传语就只有一条：\n7 x 24 小时无人监守录制、渲染弹幕、识别字幕、自动切片、自动上传、兼容超低配机器，启动项目，人人都是录播员。\n这个项目的宣传省略了所有的细节描述，直接描述流程，让用户熟悉项目的整个运作过程，植入全自动化的省事感觉，“兼容超低配机器”植入要求门槛低的感觉，最后“启动项目，人人都是录播员”，让用户产生期待，从而点击项目主页再进行详细查看并尝试使用，我相信项目开发的功能以及使用文档会远超用户的期待，从而产生更多的有效使用及建议反馈。此外，正如我最开始在开发者论坛中做的一样，持续地重复曝光才是宣传的关键。\n偶然间发现，如今在前端框架中炙手可热的 Vue 也曾经历过开源项目推广的广告过程，看完尤雨溪的博客 First Week of Launching Vue.js 不禁会心一笑。\n再回到一开始，互联网的底层逻辑就是广告。一个不懂广告的人，一个抗拒营销的企业，恐怕还不理解商业社会的基本规则，等不到被 AGI 取代的那一天，就早已被淘汰了。\n"},{"id":35,"href":"/posts/a-brief-introduction-to-dns/","title":"A Brief Introduction to DNS","section":"Blog","content":"In my mind, DNS is the key of the internet. I always believe if you control the DNS, you control the Internet world. So let us get started to know the DNS.\nHosts # Long long ago, if we want to access a computer, we need to know its IP address. But it\u0026rsquo;s hard to remember, and if the computer\u0026rsquo;s IP is changed, we need to notify the others.\nSo we need to make a list to map the computer\u0026rsquo;s name to its IP address and save it in every computer as well as updating from a specific computer which to maintain the list. And the list is called hosts.(eg. timerring: 88.88.88.88) And this is the origin of ARPANET.\nBut with the development of Internet, the number of IP is increasing, the hosts file is too large, and the names will be conflicts. Then comes the DNS.\nDNS # Paul Mockapetris proposed the Domain Name System in 1983, which is a distributed database that maps domain names to IP addresses.\nSo every time we want to access a website, we just need to query the website domain name to the DNS server and get the corresponding IP address.\nWe don\u0026rsquo;t need the local hosts records anymore.(The hosts file of the computers are empty.)\nDHCP # The ip of DNS server may be dynamic(for residential broadband). Every time you go online, it will be allocated by the gateway, which is so-called DHCP mechanism. (Dynamic Host Configuration Protocol)\nAnd it may be assigned the fixed address. You can check the DNS server ip of your Linux in /etc/resolv.conf.\nDNS protocol # First we need a specific rule to ensure the domain name is unique. So just like th e address in real world, from the street to city, the domain also obey the rule home.google.com.\nThe level of domain # So how can the DNS server know the ip of every domain? The answer is hierarchical query. Look back to the info of query math.stackexchange.com.. There is a pot . in the end, which means the root domain. It is usually omitted. So the level is as follows:\nroot domain: . top-level domain(TLD): such as .com. second-level domain(SLD): This level domain can be registered by user normally. host: user can assign the name of host, such as www Domain Name Resource Record # Every domain has a corresponding record in the DNS, and its format is:\nDomain_name Time_to_live Class Type Value Domain_name: The domain name. Time_to_live: The time to live of the record. Class: Most of the time is IN(Internet). Type: The type of the record. Value: The value of the record. The type of the record # The type of the record is as follows:\nA: address record, return the IPv4 of domain AAAA: address record, return the IPv6 of domain NS: Name server record. Every level of domain has its own NS record. This record point out the server of this level domain. These server know the every record of sublevel of domain.(Authoritative Name Server) MX: Mail eXchange record: return the server address of receiving email. CNAME: Canonical Name record: return another domain, which means the domain is a springboard for another domain. PTR: Pointer Record: PTR is used to check if the ip actually possesses the domain which it claims to. DNS server # Second, we have the unique domain name, but we query the same DNS server is not realistic. Now we need to split the server according to the top level domain(TLD) to form the DNS zone.\neg. Now every zone has a master server and many slave servers to backup and expedite the query. These servers are called Authoritative Name Servers.\nAnd it save the two types of records: This zone\u0026rsquo;s domain name resource records. This zone\u0026rsquo;s parent DNS and sub-DNS server records(mainly NS records). Now you can find that the A and B zone don\u0026rsquo;t have the parent DNS server, so how to ensure they know each other? The answer is the root DNS server ..\nYou can find the root DNS server here: root DNS server.\nDomain Name Resolution # Every time you connect to the network, you will get default DNS server(operator provide) or you can use the public DNS, and you query the domain name to the DNS server(so-called local DNS server).\nIf the domain name is actually in the same zone, then the local DNS server will return the record directly. Else the local DNS server will query the root DNS server(The root domain server is fixed, which is built in the DNS server.) to get the parent DNS server of the domain. And then query the DNS server recursively(eg. www.google.com: .-\u0026gt; com -\u0026gt; google.com -\u0026gt; www.google.com -\u0026gt; xx.xx.xx.xx). Caching Mechanism # Most of us usually visit the just 20% websites or we may visit the different pages of the same website, so our browser and OS can cache the records of these websites to improve the query efficiency.\nSo the Time_to_live segment is used to determine the cache time of the record. For those stable domain eg. google.com, the Time_to_live is set to a large value. And for those who often changes the domain resolution results, the Time_to_live is set to a small value.\nMeanwhile, the DNS server will also cache the records of the domain just as our computer do.\nDNS leaking # Check your DNS is leaking or not. https://ipleak.net/ It uses the random sub domain to record the last DNS servers to the authorative DNS.\nBesides, check this https://browserleaks.com/webrtc Avoid the WebRTC, it will leak your public ip because it will send the stun packet via UDP, and the UDP somehow cannot be proxied by the http proxy. But even you use the socks5 proxy, it will still leak ip because the browser like chrome will not give the UDP packet to the proxy. (If you use the tun mode or router proxy, you still need to check the node has the UDP proxy in the configuration.)\nThe main reason of DNS leaking is that the DNS request is plaintext, so every server in the request process will know what you are requesting. If the website you visit does not need to DNS request, then it can directly connect or proxy. But if the website does not hit the rules, it will send the DNS request locally. And the local request will only used to match the rules(redir-host), so the mode is abandoned by the official.\nMethods to avoid DNS leaking:\nDoH: DNS over HTTPS mainly HTTP/2 443 (RFC 8484) DoT: DNS over TLS mainly UDP 853 Make all requests encrypted and send to remote node, and let the remote node process all the requests. (Fake IP mode: have the TCP connection and domain name, the proxy client can easily package it using SOSCKS5 or some other protocol and send to the remote node.) For more details, you can refer to my blog Understanding Clash Through Configuration.\nExample process # To begin with, you should know the OSI model. I have drawn a good image to explain it. You can also read my previous blog Real Computer Network. OSI model So just imagine the process of go surfing the internet. eg. you query google.com on your browser, and then you get the page, what happened?\nIn the common case, when you purchase the broadband, the operator will provide you a Fiber - optic Modem, and you will buy a router. The router connects to the internet via PPPoE and get the public WAN ip (in fact still a intranet ip) and the DNS servers ip (common two DNS servers). Your router is as the gateway of your local network, so it will has own local network ip, and it will allocate the ip and DNS server ip to your devices through DHCP(commonly the DNS ip and the gateway ip are all the router\u0026rsquo;s local ip).\nYour browser first checks the browser\u0026rsquo;s cache to see if it has the ip of google.com, and then checks the OS\u0026rsquo;s cache(include the host file if there is the mapping relationship). If there isn\u0026rsquo;t, it will send a DNS request.(eg. tell me the ip of google.com) In the transport layer, the source port is eg.222 and the default destination DNS port is 53. In the network layer, the source ip is your computer\u0026rsquo;s ip 192.168.1.10 and the destination ip is the DNS server\u0026rsquo;s ip 8.8.8.8. But the DNS ip you cannot find locally, so you need to send the DNS request to the gateway you connect. And because the communication via MAC address in the same network, so it will be processed in data link layer. In data link layer, the source MAC address AA-AA-AA-AA is your computer\u0026rsquo;s MAC address and the destination MAC address CC-CC-CC-CC (get through ARP protocol) is the gateway\u0026rsquo;s MAC address. Then it will be sent through NIC and in the cable. The switch will receive the packet and forward it to the gateway(eg.router). In the data link layer of router, router finds the MAC is itself and resolve it pass to the network layer. But it cannot find the DNS 8.8.8.8 in its routing table, so it will send the packet to the default router(in the public network). Before sends to the public network, the router will use NAT to change the source private ip to the public ip(the WAN ip of router). In the public network, the routers will find and change the MAC addressed to forward the packet to the next router. Then the DNS server will receive the packet, and resolve the packet, in the transport layer, it find the destination port is 53, so it knows it is a DNS request. And it resolve the ip of google.com and return the ip. And the return process is similar to the request process. Finally, after those, your computer will receive the ip of google.com and request the ip of google.com to obtain the page, and the process is similar as above. But sometimes, the process won\u0026rsquo;t be so smooth, due to the DNS server is overseas, so the traffic needs to go through the public exit port(except for using the IPLC intranet of ISP). Every packet will be checked, thus causing the DNS pollution（tampers a not exist ip）, TCP reset(sends the RST packet in advance toreject the connection request), block ip or active detection.\ndig # The dig command is a powerful tool used for querying Domain Name System (DNS) servers.\neg. dig baidu.com which is equivalent to dig a baidu.com (record type is A).\n# You will see the query parameters and statistics. ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; baidu.com ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 17961 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0 # This is the query content `A` record is the abbreviation for address. ;; QUESTION SECTION: ;baidu.com.\tIN\tA # This is the answer from server. `6` means that TTL which is the abbreviation of `Time to live`. ;; ANSWER SECTION: baidu.com.\t6\tIN\tA\t198.18.28.63 # This is the conclusion of the query. # You can find the DNS of your machine. The #53 means the port, which is the default port of DNS. ;; Query time: 1 msec ;; SERVER: 198.19.0.3#53(198.19.0.3) ;; WHEN: Thu Dec 26 22:50:14 CST 2024 ;; MSG SIZE rcvd: 43 dig +short baidu.com: Show in brief only get the IP dig @4.2.2.2 baidu.com: Query through the public DNS server via @DNSserver, such as 8.8.8.8 of google and 4.2.2.2 of Level3. dig ns com: You can query the each level of domain separately. dig mx github.com: Query the mail server of the domain. dig -x ip: Query the PTR record of the ip. dig +trace baidu.com: Show the trace of the query. First list all the root servers, then query these ip for the toplevel and sublevel server. But in fact, most of time the server first reply will be cached, and you will just see the cached result (except you are the first user to query a very niche domain, at that time you can actually see the whole process from the root server to the TLD to the SLD to the host). Normally the cache results just like the following: ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; +trace baidu.com ;; global options: +cmd .\t221\tIN\tNS\tm.root-servers.net. .\t221\tIN\tNS\td.root-servers.net. .\t221\tIN\tNS\th.root-servers.net. .\t221\tIN\tNS\tl.root-servers.net. .\t221\tIN\tNS\tf.root-servers.net. .\t221\tIN\tNS\tg.root-servers.net. .\t221\tIN\tNS\ta.root-servers.net. .\t221\tIN\tNS\tj.root-servers.net. .\t221\tIN\tNS\tb.root-servers.net. .\t221\tIN\tNS\tk.root-servers.net. .\t221\tIN\tNS\ti.root-servers.net. .\t221\tIN\tNS\tc.root-servers.net. .\t221\tIN\tNS\te.root-servers.net. ;; Received 239 bytes from 198.19.0.3#53(198.19.0.3) in 9 ms baidu.com.\t6\tIN\tA\t198.18.28.63 ;; Received 43 bytes from 198.18.29.158#53(c.root-servers.net) in 0 ms Above are thirteen root domain server all over the world. From A.ROOT-SERVERS.NET to M.ROOT-SERVERS.NET.\ndig cname facebook.github.io: The cname is mainly for the internal jump of domain, which provide the server configuration with more convenience. This is transparent to users. ... ;; ANSWER SECTION: facebook.github.io. 3370 IN CNAME github.map.fastly.net. github.map.fastly.net. 600 IN A 103.245.222.133 We can see that the CNAME of facebook.github.io points to the github.map.fastly.net, and return the ip of github.map.fastly.net. So when we tend to change the ip, we can directly change the configuration of github.map.fastly.net, there is no need to change the facebook.github.io anymore.\nYou cannot set other records anymore after you set CNAME. This will avoid the conflicts with other records because CNAME means a substitute.(eg. test.com and try.com have their own MX records, if the rules are not the same, then there will be some conflicts)\nwhois # Find the register info of domain\nwhois github.com Pay attention to the similar command whoami which is used to check the user name of system.\nReference:\nhttps://www.rfc-editor.org/rfc/rfc883.html https://viewdns.info/ https://www.ruanyifeng.com/blog/2016/06/dns.html https://www.petekeen.net/dns-the-good-parts/ https://selfboot.cn/2015/11/05/dns_theory/ https://www.youtube.com/watch?v=GQg5JIQIjgk https://blog.skk.moe/post/what-happend-to-dns-in-proxy/ "},{"id":36,"href":"/posts/real-computer-network/","title":"Real Computer Network","section":"Blog","content":"For the computer network, I believe that most people have learned it in the course of university or college. No matter whether you are familiar with it or not, this article will give you a different perspective of the network. It can really help you a lot.\nBasic concepts # Providers # IDC: Internet Data Center, which is the data center of the internet. It is the place where the servers are located.\nISP: Internet Service Provider, which is the internet service provider. It is the company that provides the internet service.\nMain Public Cloud Service Providers:\nAWS: Amazon Web Services, which is the cloud service of Amazon. Azure: Microsoft Cloud Service, which is the cloud service of Microsoft. Alibaba Cloud: Alibaba Cloud, which is the cloud service of Alibaba. GCP: Google Cloud Platform, which is the cloud service of Google. PCCW(Recommended): Pacific Century CyberWorks, which is the internet service provider of Hong Kong, China.(make sure it is Two-way instead of one-way) Besides, if your neighbors(the shared users of the server) are all \u0026ldquo;one-click script\u0026rdquo; masters, using serverspeeder, violent modification BBR, KCPTun multiple times more packet sending, etc. But you don\u0026rsquo;t do any optimization, then even you have a whole submarine cable, you will still be slower than them.\nWhen you buy a VPS, the bandwidth is provided by the operators which the VPS supplier purchase from the operators.\nNetwork Circuits # Public Network # You can check your network AS info here: https://ipinfo.io/\nInternational export: The international export is the last port of the internet in China when the traffic is going to the overseas. Mainly located in Beijing, Shanghai and Guangzhou. 163: AS4134. The 163 network is the backbone of the internet in China. And mainly used by China Telecom. The backbone node is started with 202.97. There is basically no performance bottleneck when accessing each other within the 163 network in China. Congestion only occurs at the international export..\n169: AS4837. The 169 network is the backbone of the internet in China. And mainly used by China Unicom. AS 9929: Also known as the former China Netcom Internet (CNCNET). The AS number of China Unicom\u0026rsquo;s A network backbone is 9929. It was built by the China Netcom, and after the merger of China Netcom and China Unicom, so the A network was available. Now the users are less and mainly aims for the government. AS9808: The AS9808 is the backbone of the internet in China. And mainly used by China Mobile. The PCCW line is currently among the lines between China and Hong Kong, China or those from China to other regions in Southeast Asia where transshipment in Hong Kong, China is required. It is the line with the best quality in China except for the pure CN2 GIA line. Educational Network / China Science and Technology Network / Private Broadband. CN2(CNCN): China telecom next carrier network, which is the backbone of the internet in China(Public network), which is more advanced than the old 163 network. The backbone node is started with 59.43. CN2GT: CN2 Global Transit, cheaper but congested, in CN2 GT product in the city → province → international export section is 163 network, the international export → overseas access point section is CN2 network, the return is the same. CN2GIA: CN2 Global Internet Access, expensive but less congested.(eg. Bandwagonhost has the most stable and cheapest CN2GIA line, but it is the key examined object of domestic.) GIA the whole network is CN2 network. Dedicated Network # IPLC: International Private Leased Circuit, which is the international dedicated line.\nBut for many service suppliers, it is just the intranet of Alibaba(深港means Alicloud shenzhen \u0026ndash; Alicloud HK), which usually purchase multiple end to end IPLC lines to connect the every data center. Then it has two pros:\nThe bandwidth in IPLC is dedicated, which is more stable than public network.(The decicated line is normally described the physical line, the most traffic in Submarine optical cable is the public network traffic and small part is the IPLC traffic.) The traffic when transiting, it will not go through the specific examination. IEPL: International Ethernet Private Line, which is the enhanced version of IPLC. It is a dedicated management bandwidth service from end to end.\nAnycast: it is a network technology that allows a single IP address to be broadcast to multiple locations. That means the IP will be the same, but the location is different.\nThis IP address is in the physical link between domestic and abroad. When users access this IP address, they will be connected to the nearest server. Just like the CDN. The cons:\nThe bandwidth is bind to the single IP address, so if it was DDOS, it is hard to deal with it. Expensive. Service Concepts # The three major operators are paid inter-network settlement, which means the traffic you consume is across the operators, they will pay the cost internally.\nQoS: Quality of Service, which is the service quality of the network. It can dynamically adjust the priority of the traffic, such as video calls(bandwidth first), online games(latency priority), etc.\nSLA: Service Level Agreement, which is the agreement between the supplier and the user. It is the service level of the supplier.\nBGP: Border Gateway Protocol, which can dynamically select the best route.(when you buy vps, you can easily find it on the supplier website) Which means the same IP in multiple operator\u0026rsquo;s network is directly connected.\nRedirect: Redirect data from one server to another. Alibaba Cloud public network transit is more common in small-scale suppliers redirect.\nPort: The port is the communication endpoint of the network. It is the port of the server.\nIP # IP database # IP Database stores IP address and physical address mapping.\nThe most used domestic IP database is IPIP. The overseas is MaxMind. Cellular Base Station # When using the cellular network, the IP address is the private IP address, and then it will be converted to the public IP address through the NAT. This situation leads to everyone using the same IP segment to access websites or apps. Generally, we refer to these relatively fixed IP segments as base station IP addresses.\nSo it is hard to locate the IP address to the specific city, which means the same IP address can be in different cities. And sometimes a series of IP is assigned to the 2~3 cities. Due to the NAT，every IP address corresponds to many users, so suspending the IP address will affect many users. Roaming: When you are roaming, the IP address will be changed. It\u0026rsquo;s mainly has three operators: mobile: roaming to the local network. If your card is in Beijing, and you go to Shanghai, then you will see the IP of Shanghai. Unicom and Telecom: roaming back to the local network. If your card is in Beijing, and you go to Shanghai, you will still see the IP of Beijing. So that accounts for the reason why you are using your card abroad, you are still blocked to visit websites. Cause you are roaming back to china and then try to access the websites. Check IP # https://ip.skk.moe/\nIP question # Why the ip is not correct?\nYou are using the phone to test? (It is the base station\u0026rsquo;s fault) What is your current operator? (If it is the secondary operator, it is the fault of the third-party export) What browser are you using? (It is the cloud acceleration\u0026rsquo;s fault) What is the third-party export?\nEvery operator will not only have their customers, but also have to obtain the network export, but the backbone networks are built by primary operators. So if the secondary operator wants to access the internet, it will have to pay the access fee to the primary operator. The primary operator avoids the competition, so it will bring up an expensive fee. But some people in the primary operator will sell them in a lower price, which is usually used by the secondary operator informally.\nIP attribution # The IP attribution identified based on your access purpose: Residential broadband, Business broadband, IDC, etc.\nNative IP # Native IP: The operator\u0026rsquo;s IP address. Broadcasting country is generally the same as registration country. Normally, it can used to unlock the hulu, netflix, etc. Because the Native IP is generally not used to the cloud computing service or have a good reputation.\nBroadcast IP: The IP address of the VPS is not the same as the location of the VPS. That means this IP is from another country / region.\nStreaming media unlock: Many streaming media platforms will restrict the specific IP access due to the copyright issues. Generally, the network operators(eg.HKT) have their own IP, which is used in commercial or Residential broadband. And the IP won\u0026rsquo;t be blocked because they are all the objective customers. Besides, the Residential broadband is hard to blocked because it is dynamic.\nProtocols # Shadowsocks: the fastest rtt.\nShadowsocksR.\nV2Ray(Vmess is the self protocol, V2Ray is the collection of protocols): more handshakes, which means slower rtt. Vmess + TLS encryption + websocket, which is more stable.\nTrojan: fix some cons of V2Ray. The core is only websocket + TLS. (TLS is the most secure which banks are using). It imitates the normal http request and works on the 443 port. If it receives the illegal request, it will provide service, or it will transit the traffic to nginx, and the nginx deal with the request. Its behavior is similar to nginx, so it is hard to detect. Meanwhile, in order to avoid the malicious detection, it will redirect the 80 port traffic to the 443, and only open the 80 and 443 port, which is similar to the normal web server.\nobfs (simple-obfs): it is a tool that can encrypt the traffic. It can be used to avoid the detection. obfs has two modes, one is HTTP, the other is TLS.\nDNS # DNS is the domain name system, which is the system that can translate the domain name to the IP address.\nNo matter how we use PPPoE to dial up or connect the optical cat through DHCP, the Internet service provider (ISP) will send you two DNS. For convenience, I call these two DNS as ISP DNS.\nIn the DNS resolution process, the user initiates a request to the recursive DNS, and the recursive DNS requests the resolution result from the authoritative DNS. In other words, the recursive DNS plays a forwarding role. The ISP\u0026rsquo;s DNS is a recursive DNS; at the same time, some individuals or Internet service providers also set up their own recursive DNS for everyone to use, which is called public DNS.\nSome famous public DNS:\nCloudflare DNS: 1.1.1.1, 1.0.0.1 Google Public DNS: 8.8.8.8, 8.8.4.4 Alidns: 223.5.5.5, 223.6.6.6 Tencent DNSPod: 119.29.29.29, 119.28.28.28 For most people, the ISP DNS provided by the operator should be the most accurate and suitable, with a short response time and the most accurate CDN resolution result.\nBut the operator often does DNS pollution, it will lead the user to their cache server or some advertising mirror website or tamper the TTL, which will lead to the DNS load less.\nBut the public DNS does not accelerate the resolution speed. Public DNS is a local DNS service provided by some enterprises, which usually provides one or more Anycast IP addresses, but actually has multiple cluster services.\nWhen users go online, the client will request the resolution from the A address in the cluster, this A address is called DNS entry; the public DNS will use the B address in the DNS cluster to compare with the IP library when judging the user\u0026rsquo;s source, this B address is called DNS exit.\nSo in the process of going online, users will get the resolution from the DNS entry, and the NS server will allocate the intelligent resolution to the DNS exit. If the DNS entry does not have the requested resolution cache, it will request the upper DNS to query, and finally request to the NS server, till then the user get the resolution result.\nHence, when the DNS entry and the DNS exit are inconsistent with the user\u0026rsquo;s actual network, it may lead to the DNS resolution result not being the optimal.\nCDN # CDN is the content delivery network, which can provide the content to the user faster and more reliable via the nearest server.\nBesides, the DNS exit is very important for CDN. The public DNS is essentially forwarding your query request to the upstream DNS; without EDNS, the authoritative DNS of the CDN will determine your operator and your location based on the request IP used by the public DNS (that is, the DNS exit), and then return the nearest node IP. In brief, CDN will return the nearest IP to the DNS exit.\nFor more information, you can refer my article A brief introduction to DNS\nProxy # Every traffic transit in public network across the countries will be examined.\nThe proxy means that the traffic is go through the proxy server.\nProxy Types # The basic proxy methods are mainly three types:\nSoftware Proxy: VPN: virtual private network which can ensure the end-to-end communication security. But nowadays, it might remains some risks. Encrypted traffic: mainly exists in the Shadowsocks, SSR, V2Ray and Trojan protocols, which usually encrypt the traffic locally and send to the transit server to forward and to the remote server to decrypt it. And the protocols should be implemented in the application layer, which means you have to used some applications to use it, such as Shadowsocks, V2Ray, Clash, Surge, etc. There still exists some issues, the application can only address the traffic which it can takes over. It(Without TUN mode) can not deal with the traffic which is on the OS layer, such as the UWP, some banks software, etc. The tun mode which can virtualize the network interface, and forced take over all the traffic. Hardware Proxy: can also called as router proxy, which is the end of whole internet. But the issue is that some router are not designed for this purpose, so the computing ability cannot meet the requirements of encryption and decryption. Gateway Proxy: it is usually implemented by the software installed on the PC, and via the CPU, it can easily complete the tasks. And you can change the nodes in the software. Ratio # The most suppliers will take all the traffic (upstream and downstream) into account.\nThe ratio between the traffic you consume and the traffic the supplier statistics.\neg. You use the 0.5x ratio node, and you consume 100G, then the supplier statistics is 50G.\nWhy there are different ratio nodes?\nThe ratio is related to the line quality and user experience. Now the supplier have two types of lines: Direct line: The line is directly connected to the overseas proxy server via public network. But the transmission effect is poor because the interconnection bandwidth between the overseas operator and the domestic operator is limited, and the domestic operator usually limits the speed of the data flow from the mainland. Besides, if you are the customer of unicom and the overseas server optimize the telecom, then you will get poorer performance than the telecom customer. Transit line: Add the transit server, which means the traffic will be transit through the server, hence the traffic is cost in every transit server, so the ratio is high. But it can provide more stable and faster service. BGP transit: The domestic transit server will provide the BGP, so you will have a better experience. Normally, it will use some tunnel protocols to balance the traffic and improve the stability. IPLC transit: Transit through the IPLC eg. SZ-HK, and then from HK to overseas proxy server. Latency # Note: The RTT is not the speed. The RTT is just the building time of the connection. The speed will depend on the shortcomings of the whole network.\nPING # ping aims to send a ICMP request to the target host and wait for the response.\nThe program will estimate the loss rate of data packets and the round-trip delay time based on the time and the number of successful responses.\nicmp ping: the classic ping. It test the latency between the machine and the transit server. tcp ping / ​http ping: It test the latency of the data packages of corresponding protocol. Normally, it will be slower than the icmp ping. Node selection mechanism # Load balancing url-test: It normally send the request to specific url, and select the best node. fall­back SSID Strategy # Traffic diversion # The traffic diversion is the strategy that can distribute the traffic to the different server, which can speed up the access speed.\nPAC1 # Proxy auto con­fig which is a method of web proxy. It can automatically select the suitable proxy server and only affect the browser.\nIt mainly depends on the specific rules which maintained by community, such as gfwlist.\nRouter traffic diversion # For traffic diversion in the routing table, you can refer to SSR-win. There are PAC diversion rules and routing diversion. For routing diversion, you need to add a SOCKS5 proxy in the browser by yourself or use Proxifier to set up a system proxy. Similarly, the settings of V2RayN on Windows also involve routing diversion. As for SS/SSR on Android, it is based on ACL (Access Control List) diversion, and there are ACLs made by third parties. On iOS, there is rule-based diversion. Since common protocols themselves don\u0026rsquo;t stipulate how to conduct diversion, it is all achieved by the software itself.\nStrategy diversion # proxy, direct, reject\nSocks5 # Socks5 is the protocol of the Session Layer, which is lower than the HTTP protocol. Socks5 mainly focus on the data package transmission, and it is not concerned with the specific protocol and usage.\nSocket is more like a \u0026ldquo;どこでもドア\u0026rdquo; in Doraemon, it needs the source IP and port and destination IP and port. The developer only needs to send the data to the Socket and get data from the Socket. The details of transport are transparent to the developer.\nVPN vs Socks5 # The OSI model is shown as above.\nThe Application Layer you search a query on the browser, and the browser sends it to the Presentation Layer via http.\nThe Presentation Layer translate the query to the machine language and sends it to the Session Layer.\nThe Session Layer maintains the session between the client and the server.\nThe Transport Layer defines the protocol and the port.\nThe Network Layer adds the IP address.\nThe Data Link Layer adds the MAC address.\nThe Physical Layer sends the data in bits.\nVPN is the same as virtual network interface, it will take over all the traffic. You can refer to the image below.\nSo we can find that the Socks5(without TUN mode) cannot proxy the traffic of online games, and the network commands.\nThe most VPN is used the TCP protocol, it needs to establish a connection, so it can be interupted by the reset packet. Besides, the feature of the VPN is very clear, so it is easy to be detected.\nNowadays, A new VPN called wireguard used the UDP protocol, so it cannot be interupted by normal methods. But it will be limited the speed in QoS.\nReference:\nhttps://www.rfc-editor.org/ https://www.youtube.com/watch?v=wAxOjL_gDzk https://www.duyaoss.com/archives/1086/ https://mp.weixin.qq.com/s/2teDwwIhyZ6BYIEQ_HL1vQ https://zhuanlan.zhihu.com/p/64467370 https://doubibackup.com/6r9z6_wi-2.html https://www.duyaoss.com/archives/2741/ https://blog.revincx.icu/posts/proxy-summary/index.html https://ephen.me/2017/PublicDns_1/ https://www.youtube.com/watch?v=Ty0n7AgcX0w "},{"id":37,"href":"/posts/python-generator-iterator-and-decorator/","title":"Python Generator Iterator and Decorator","section":"Blog","content":"This article will review the basic knowledge of Python, including generator, iterator, and decorator.\nCH9 Generator, Iterator, and Decorator # Generator # ls = [ i**2 for i in range(1, 1000001)] for i in ls: pass Disadvantage: Occupying a lot of memory\nUse lazy calculation Do not need to store a large amount of data at once Calculate on the fly, only calculate the value needed each time Actually always executing the next() operation until there is no value left Generator Expression # Do not need to store all data\nsum((i for i in range(101))) # Sum, inside a generator # 5050 Generator Function yield # Generate Fibonacci sequence def fib(max): n, a, b = 0, 1, 1 while n \u0026lt; max: print(a) a, b = b, a + b n = n + 1 Construct generator function, execute each time when next() is called, return when encountering yield statement, continue execution from the yield statement last returned.\nDifference:\nA normal function executes all code in the function body at once when called, returning a result (if there is a return value) and then ends. The function containing yield (i.e., the generator function) executes until yield, returns a value, and then continues execution multiple times based on needs, possibly returning new values each time, until the function is executed to completion (e.g., when reaching the end of the function or encountering a return statement). It seems like the function has memory effect. def fib(max): n, a, b = 0, 1, 1 while n \u0026lt; max: yield a a, b = b, a + b n = n + 1 fib(10) # \u0026lt;generator object fib at 0x000001BE11B19048\u0026gt; for i in fib(10): print(i) # 1 # 1 # 2 # 3 # 5 # 8 # 13 # ... Iterator # Iterable # Objects that can be directly used in for loops are collectively referred to as Iterable:\nList, tuple, string, dictionary, set, file\nUse isinstance() to determine if an object is an Iterable object\nfrom collections import Iterable isinstance([1, 2, 3], Iterable) # True Generator\nGenerators can not only be used in for loops but also be called by the next() function\nsquares = (i**2 for i in range(5)) isinstance(squares, Iterable) # True print(next(squares)) # Until there is no data to take, throw StopIteration print(next(squares)) # StopIteration: Objects that can be called by the next() function and return the next value until there is no data to take are called Iterators: Iterator\nIterator # Use isinstance() to determine if an object is an Iterator object\nGenerators are Iterators from collections import Iterator squares = (i**2 for i in range(5)) isinstance(squares, Iterator) # True List, tuple, string, dictionary, set are not Iterators isinstance([1, 2, 3], Iterator) # False Can create an Iterator by iter(Iterable) isinstance(iter([1, 2, 3]), Iterator) # True for item in Iterable is equivalent to: First get the Iterator of the Iterable by iter() function, then call next() method on the obtained Iterator to get the next value and assign it to item, and the loop ends when encountering the StopIteration exception.\nzip enumerate and other functions in itertools are Iterators x = [1, 2] y = [\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;] zip(x, y) # \u0026lt;zip at 0x1be11b13c48\u0026gt; for i in zip(x, y): print(i) isinstance(zip(x, y), Iterator) # True numbers = [1, 2, 3, 4, 5] enumerate(numbers) # \u0026lt;enumerate at 0x1be11b39990\u0026gt; for i in enumerate(numbers): print(i) isinstance(enumerate(numbers), Iterator) # True File is an Iterator with open(\u0026#34;测试文件.txt\u0026#34;, \u0026#34;r\u0026#34;, encoding = \u0026#34;utf-8\u0026#34;) as f: print(isinstance(f, Iterator)) # True Iterator is consumable squares = (i**2 for i in range(3)) for square in squares: print(square) # 0 # 1 # 4 for square in squares: print(square) # Cannot iterate anymore, because it has been exhausted range() is not an Iterator Can be called range() as a lazy sequence, it is a sequence, but does not contain any content in memory, but answers questions through calculation.\nnumbers = range(10) isinstance(numbers, Iterator) # False print(len(numbers)) # Has length 10 print(numbers[0]) # Can be indexed 0 print(9 in numbers) # Can exist calculation True next(numbers) # Cannot be called by next() TypeError: \u0026#39;range\u0026#39; object is not an iterator # Will not be exhausted Decorator # Template # Talk is cheap, just show the code\ndef decorator(func): # The wrapper is the function to be decorated(must keep the same parameters with the original function) def wrapper(*args, **kwargs): # Do something before the function is called res = func(*args, **kwargs) # Call the original function # Do something after the function is called return res return wrapper # Return the wrapper(if the original function returns value, the wrapper must return the value) Demand # (1) Need to add some features to the already developed program (2) Cannot modify the source code of the function in the program (3) Cannot change the calling method of the function in the program\nFunction Object # You can assign a function to a variable and call the variable to realize the function of the original function. Functions can be passed as parameters def square(x): return x**2 print(type(square)) # square is an instance of the function class # \u0026lt;class \u0026#39;function\u0026#39;\u0026gt; pow_2 = square # It can be understood that this function is given an alias pow 2 print(pow_2(5)) # 25 print(square(5)) # 25 High-order Function # One of the following is sufficient:\nReceives a function as a parameter Or returns a function def square(x): return x**2 def pow_2(fun): return fun f = pow_2(square) f(8) # 64 print(f == square) # True Nested Function # Define a function inside a function\ndef outer(): print(\u0026#34;outer is running\u0026#34;) def inner(): print(\u0026#34;inner is running\u0026#34;) inner() outer() Closure # def outer(): x = 1 z = 10 def inner(): y = x + 100 return y, z return inner f = outer() # In fact, f contains the inner function itself + the environment of the outer function print(f) # \u0026lt;function outer.\u0026lt;locals\u0026gt;.inner at 0x000001BE11B1D730\u0026gt; print(f.__closure__) # __closure__ property contains information from the outer function for i in f.__closure__: print(i.cell_contents) # (\u0026lt;cell at 0x000001BE0FDE06D8: int object at 0x00007FF910D59340\u0026gt;, \u0026lt;cell at 0x000001BE0FDE0A98: int object at 0x00007FF910D59460\u0026gt;) # 1 # 10 res = f() print(res) # (101, 10) Closure: Function that extends the scope\nIf a function is defined in the scope of another function and references a variable in the outer function, then this function is called a closure\nA closure is an entity composed of a function and its related reference environment (i.e., closure = function + reference environment)\nOnce a variable with the same name is redefined inside the inner function, it becomes a local variable\ndef outer_function(x): outer_variable = x def inner_function(y): return outer_variable + y return inner_function closure = outer_function(10) # x is 10 closure = inner_function result = closure(5) # y is 5 print(result) # 15 def outer(): x = 1 def inner(): x = x+100 return x return inner f = outer() f() # \u0026lt;ipython-input-87-d2da1048af8b\u0026gt; in inner() # 3 # 4 def inner(): # ----\u0026gt; 5 x = x+100 # 6 return x # UnboundLocalError: local variable \u0026#39;x\u0026#39; referenced before assignment nonlocal allows the inner function to modify the closure variable, indicating that it is not an internal variable, and uses the variable of the outer function.\ndef outer(): x = 1 def inner(): nonlocal x x = x+100 return x return inner f = outer() f() # 1 # 101 LEGB rules # In order to understand actually how the scope resolution happening in Python, we should analyze the LEGB rule because, it is the sequence of names(variables, functions, objects and so on) that Python resolves in a program.\nLEGB stanas for： （L） Local scope （E） Enclosed scope （G） Global scope （B） Built-in scope\nSo the sequence of the scope resolution is: Local scope -\u0026gt; Enclosed scope -\u0026gt; Global scope -\u0026gt; Built-in scope. In a closure environment, if firstly search the local scope, if not, it will search the enclosed scope, you can refer to this example:\n#foo.py filename = \u0026#34;foo.py\u0026#34; print(filename) def call_func(f): filename = \u0026#34;foo_call.py\u0026#34; print(filename) return f() #func.py import foo filename = \u0026#34;func_global.py\u0026#34; print(filename) def wrapper(): filename = \u0026#34;func_enclosed.py\u0026#34; print(filename) def show_filename(): filename = \u0026#34;func_inner.py\u0026#34; print(filename) return f\u0026#34;filename: {filename}\u0026#34; print(foo.call_func(show_filename)) if __name__ == \u0026#34;__main__\u0026#34;: wrapper() Guess what the output will be? Can you explain the print sequence?\nthe main call the foo.call_func first, so the foo will be loaded first, the foo.py will be printed when import foo. then the func.py will be loaded, the func_global.py will be printed. Now the wrapper is called, the func_enclosed.py will be printed. And the call_func is a closure, it contains the f() and its environment. Now execute the closure, so the foo_call.py will be printed. Now execute the return, actually the return is the show_filename function, so the func_inner.py will be printed. And the filename return is the func_inner.py. (Local scope) So the output is:\nfoo.py func_global.py func_enclosed.py foo_call.py func_inner.py filename: func_inner.py Now we delete the func_inner.py, can you guess the filename?\nfilename: func_enclosed.py As we can see, the closure contains the show_filename function and its environment, there is no filename inside the function, so it search the env filename is func_enclosed.py. (Enclosed scope)\nIf we delete the func_enclosed.py, the filename will be func_global.py. (Global scope)\nNow if we delete the func_global.py, can you guess the output? Should the filename be foo_call.py?\nAbsolutely not, because the return is the show_filename function, and there is no filename inside both function and its environment, so the function cannot be executed originally, let alone called any other way. It will just appear the NameError: name 'filename' is not defined.\nHope this example can help you better understand the concepts.\nA Simple Decorator # Implemented with nested functions\nimport time def timer(func): def inner(): print(\u0026#34;inner run\u0026#34;) start = time.time() func() end = time.time() print(\u0026#34;{} function running time: {:.2f} seconds\u0026#34;.format(func.__name__, (end-start))) return inner def f1(): print(\u0026#34;f1 run\u0026#34;) time.sleep(1) f1 = timer(f1) # Contains inner() and the environment of timer, such as the passed parameter func f1() # inner run # f1 run # f1 function running time: 1.00 seconds Syntax Sugar # import time def timer(func): def inner(): print(\u0026#34;inner run\u0026#34;) start = time.time() func() end = time.time() print(\u0026#34;{} function running time: {:.2f} seconds\u0026#34;.format(func.__name__, (end-start))) return inner @timer # Equivalent to implementing f1 = timer(f1) def f1(): print(\u0026#34;f1 run\u0026#34;) time.sleep(1) Decorate Function with Parameters # import time def timer(func): def inner(*args, **kwargs): print(\u0026#34;inner run\u0026#34;) start = time.time() func(*args, **kwargs) end = time.time() print(\u0026#34;{} function running time: {:.2f} seconds\u0026#34;.format(func.__name__, (end-start))) return inner @timer # Equivalent to implementing f1 = timer(f1) def f1(n): print(\u0026#34;f1 run\u0026#34;) time.sleep(n) f1(2) Function decorated with return value\nimport time def timer(func): def inner(*args, **kwargs): print(\u0026#34;inner run\u0026#34;) start = time.time() res = func(*args, **kwargs) end = time.time() print(\u0026#34;{} function running time: {:.2f} seconds\u0026#34;.format(func.__name__, (end-start))) return res return inner @timer # Equivalent to implementing f1 = timer(f1) def f1(n): print(\u0026#34;f1 run\u0026#34;) time.sleep(n) return \u0026#34;wake up\u0026#34; res = f1(2) print(res) # inner run # f1 run # f1 function running time: 2.00 seconds # wake up Decorator with Parameters # Decorators themselves need to pass some additional parameters\nRequirement: Sometimes you need to count the absolute time, sometimes you need to count the absolute time twice def timer(method): def outer(func): def inner(*args, **kwargs): print(\u0026#34;inner run\u0026#34;) if method == \u0026#34;origin\u0026#34;: print(\u0026#34;origin_inner run\u0026#34;) start = time.time() res = func(*args, **kwargs) end = time.time() print(\u0026#34;{} function running time: {:.2f} seconds\u0026#34;.format(func.__name__, (end-start))) elif method == \u0026#34;double\u0026#34;: print(\u0026#34;double_inner run\u0026#34;) start = time.time() res = func(*args, **kwargs) end = time.time() print(\u0026#34;{} function running time: {:.2f} seconds\u0026#34;.format(func.__name__, 2*(end-start))) return res return inner return outer @timer(method=\u0026#34;origin\u0026#34;) # Equivalent to timer = timer(method = \u0026#34;origin\u0026#34;) f1 = timer(f1) def f1(): print(\u0026#34;f1 run\u0026#34;) time.sleep(1) @timer(method=\u0026#34;double\u0026#34;) def f2(): print(\u0026#34;f2 run\u0026#34;) time.sleep(1) f1() print() f2() # inner run # origin_inner run # f1 run # f1 function running time: 1.00 seconds # inner run # double_inner run # f2 run # f2 function running time: 2.00 seconds Understanding closures is key!\n9、When does the decorator execute\nExecute immediately when decorated, no need to wait for the call func_names=[] def find_function(func): print(\u0026#34;run\u0026#34;) func_names.append(func) return func @find_function def f1(): print(\u0026#34;f1 run\u0026#34;) @find_function def f2(): print(\u0026#34;f2 run\u0026#34;) # run # run for func in func_names: print(func.__name__) func() print() # f1 # f1 run # f2 # f2 run Return to the Source # The properties of the original function are hidden import time def timer(func): def inner(): print(\u0026#34;inner run\u0026#34;) start = time.time() func() end = time.time() print(\u0026#34;{} function running time: {:.2f} seconds\u0026#34;.format(func.__name__, (end-start))) return inner @timer # Equivalent to implementing f1 = timer(f1) def f1(): time.sleep(1) print(\u0026#34;f1 run\u0026#34;) print(f1.__name__) # inner Return to the source import time from functools import wraps def timer(func): @wraps(func) def inner(): print(\u0026#34;inner run\u0026#34;) start = time.time() func() end = time.time() print(\u0026#34;{} function running time: {:.2f} seconds\u0026#34;.format(func.__name__, (end-start))) return inner @timer # Equivalent to implementing f1 = timer(f1) def f1(): time.sleep(1) print(\u0026#34;f1 run\u0026#34;) print(f1.__name__) f1() # f1 # inner run # f1 run # f1 function running time: 1.00 seconds "},{"id":38,"href":"/posts/python-underlying-mechanism/","title":"Python Underlying Mechanism","section":"Blog","content":"This article will review the underlying mechanism of Python, including data types and derivation operations.\nCH8 Underlying Mechanism # Data Type Implementation # Strange List # list_1 = [1, [22, 33, 44], (5, 6, 7), {\u0026#34;name\u0026#34;: \u0026#34;Sarah\u0026#34;}] Shallow copy # list_3 = list_1 # Error! list_2 = list_1.copy() # Or list_1[:] \\ list(list_1) can also implement shallow copy Operations on the shallow copy of the two lists list_2[1].append(55) print(\u0026#34;list_1: \u0026#34;, list_1) # list_1: [1, [22, 33, 44, 55], (5, 6, 7), {\u0026#39;name\u0026#39;: \u0026#39;Sarah\u0026#39;}] print(\u0026#34;list_2: \u0026#34;, list_2) # list_2: [1, [22, 33, 44, 55], (5, 6, 7), {\u0026#39;name\u0026#39;: \u0026#39;Sarah\u0026#39;}] List Implementation # Concept of reference array\nThe elements inside the list can be scattered in memory\nThe list actually stores the addresses of these elements, and the addresses are stored in a continuous manner.\nShallow copy copies the addresses of the elements.\nlist_1 = [1, [22, 33, 44], (5, 6, 7), {\u0026#34;name\u0026#34;: \u0026#34;Sarah\u0026#34;}] list_2 = list(list_1) # Shallow copy The same functionality as list_1.copy() （1）New elements\nlist_1.append(100) list_2.append(\u0026#34;n\u0026#34;) print(\u0026#34;list_1: \u0026#34;, list_1) # list_1: [1, [22, 33, 44], (5, 6, 7), {\u0026#39;name\u0026#39;: \u0026#39;Sarah\u0026#39;}, 100] print(\u0026#34;list_2: \u0026#34;, list_2) # list_2: [1, [22, 33, 44], (5, 6, 7), {\u0026#39;name\u0026#39;: \u0026#39;Sarah\u0026#39;}, \u0026#39;n\u0026#39;] （2）Modify elements, pay attention to the difference between modifying mutable or immutable types.\nLists and dictionaries, which are mutable types, content changes, but the address does not change. While tuples, numbers, and strings, which are immutable types, content changes, and the address changes. list_1[0] = 10 list_2[0] = 20 print(\u0026#34;list_1: \u0026#34;, list_1) # list_1: [10, [22, 33, 44], (5, 6, 7), {\u0026#39;name\u0026#39;: \u0026#39;Sarah\u0026#39;}, 100] print(\u0026#34;list_2: \u0026#34;, list_2) # list_2: [20, [22, 33, 44], (5, 6, 7), {\u0026#39;name\u0026#39;: \u0026#39;Sarah\u0026#39;}, \u0026#39;n\u0026#39;] （3）Operations on list elements\nlist_1[1].remove(44) list_2[1] += [55, 66] print(\u0026#34;list_1: \u0026#34;, list_1) # list_1: [10, [22, 33, 55, 66], (5, 6, 7), {\u0026#39;name\u0026#39;: \u0026#39;Sarah\u0026#39;}, 100] print(\u0026#34;list_2: \u0026#34;, list_2) # list_2: [20, [22, 33, 55, 66], (5, 6, 7), {\u0026#39;name\u0026#39;: \u0026#39;Sarah\u0026#39;}, \u0026#39;n\u0026#39;] Because the operation is on the list, and the original list maps the address, after modifying the element, the address is mapped, so the modification of list1 and 2 is the same\n（4）Operations on tuple elements\nlist_2[2] += (8,9) print(\u0026#34;list_1: \u0026#34;, list_1) # list_1: [10, [22, 33, 55, 66], (5, 6, 7), {\u0026#39;name\u0026#39;: \u0026#39;Sarah\u0026#39;}, 100] print(\u0026#34;list_2: \u0026#34;, list_2) # list_2: [20, [22, 33, 55, 66], (5, 6, 7, 8, 9), {\u0026#39;name\u0026#39;: \u0026#39;Sarah\u0026#39;}, \u0026#39;n\u0026#39;] Tuples are immutable! They are equivalent to adding a tuple (5, 6, 7, 8, 9), and list2 points to this tuple.\n（5）Operations on dictionary elements\nlist_1[-2][\u0026#34;age\u0026#34;] = 18 print(\u0026#34;list_1: \u0026#34;, list_1) # list_1: [10, [22, 33, 55, 66], (5, 6, 7), {\u0026#39;name\u0026#39;: \u0026#39;Sarah\u0026#39;, \u0026#39;age\u0026#39;: 18}, 100] print(\u0026#34;list_2: \u0026#34;, list_2) # list_2: [20, [22, 33, 55, 66], (5, 6, 7, 8, 9), {\u0026#39;name\u0026#39;: \u0026#39;Sarah\u0026#39;, \u0026#39;age\u0026#39;: 18}, \u0026#39;n\u0026#39;] Deep Copy # After shallow copy\nOperations on immutable elements (numbers, strings, tuples) are effective Operations on mutable elements (lists, sets) cause some confusion Introducing deep copy\nDeep copy copies all related elements at all levels, completely separating them, clearly distinguishing between them, avoiding the above issues import copy list_1 = [1, [22, 33, 44], (5, 6, 7), {\u0026#34;name\u0026#34;: \u0026#34;Sarah\u0026#34;}] list_2 = copy.deepcopy(list_1) list_1[-1][\u0026#34;age\u0026#34;] = 18 list_2[1].append(55) print(\u0026#34;list_1: \u0026#34;, list_1) # list_1: [1, [22, 33, 44], (5, 6, 7), {\u0026#39;name\u0026#39;: \u0026#39;Sarah\u0026#39;, \u0026#39;age\u0026#39;: 18}] print(\u0026#34;list_2: \u0026#34;, list_2) # list_2: [1, [22, 33, 44, 55], (5, 6, 7), {\u0026#39;name\u0026#39;: \u0026#39;Sarah\u0026#39;}] Dictionary Implementation # Implement value storage and access through sparse arrays\nDictionary Creation Process\nCreate a sparse array (N \u0026raquo; n) d = {} Step 1: Calculate the hash value of the key through hash() print(hash(\u0026#34;python\u0026#34;)) # -4771046564460599764 print(hash((1,2))) # 3713081631934410656 # Adding a key-value pair, first calculate the hash value of the key hash(\u0026#34;age\u0026#34;) d[\u0026#34;age\u0026#34;] = 18 print(hash(\u0026#34;age\u0026#34;)) Step 2: Determine its position in the sparse array based on the calculated hash value, and if there is a hash value collision, there is a corresponding method to resolve the conflict. Step 3: Store the value at that position Summary\n（1）Dictionary data type, through space for time, implements fast data lookup, which also means that the space utilization efficiency of the dictionary is low.\n（2）Because the order of the hash value corresponding position may be different from the order of the key in the dictionary, the dictionary appears to be unordered.\nCompact String # Implement string storage through compact arrays, data is stored in memory in a continuous manner, more efficient, and saves space.\nAs a sequence type, why is the list implemented using reference arrays, while the string is implemented using compact arrays? The list can change, so it is not convenient to reserve space.\nMutable and Immutable Types # Immutable types: numbers, strings, tuples The content remains unchanged throughout its lifecycle, in other words, if it is changed, it is no longer itself (the id changes). The += operation on immutable objects actually creates a new object Tuples are not always immutable，if a tuple contains a mutable type, then the tuple can still change.\nt = (1,[2]) t[1].append(3) # (1, [2, 3]) Mutable types: lists, dictionaries, sets id remains unchanged, but the content can change The += operation on mutable objects actually modifies the original object in place A Few Examples of List Operations # Deleting a Specific Element in a List # Method 1 Existence Operation Deletion Disadvantage: Each existence operation requires traversing the list from the beginning, searching, and being inefficient\nalist = [\u0026#34;d\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;d\u0026#34; ,\u0026#34;d\u0026#34;, \u0026#34;4\u0026#34;] s = \u0026#34;d\u0026#34; while True: if s in alist: alist.remove(s) else: break print(alist) # [\u0026#39;2\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;4\u0026#39;] Method 2 Delete all elements at once First, alist is being deleted, but the index s is in order, so there may be a phenomenon where some elements are skipped, but the deletion is still performed in the order of scanning from the list head.\nalist = [\u0026#34;d\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;d\u0026#34; ,\u0026#34;d\u0026#34;, \u0026#34;4\u0026#34;] for s in alist: if s == \u0026#34;d\u0026#34;: alist.remove(s) # remove（s） Remove the first occurrence of the element in the list print(alist) # [\u0026#39;2\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;4\u0026#39;] Solution: Use negative indexing Negative indexing scans in reverse order, ensuring that each traversal is the list head, and the deletion is also the list head.\nalist = [\u0026#34;d\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;d\u0026#34; ,\u0026#34;d\u0026#34;, \u0026#34;4\u0026#34;] for i in range(-len(alist), 0): if alist[i] == \u0026#34;d\u0026#34;: alist.remove(alist[i]) # remove（s） Remove the first occurrence of the element in the list print(alist) # [\u0026#39;2\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;4\u0026#39;] Creating a Multi-Dimensional List # ls = [[0]*10]*5 ls[0][0] = 1 # [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0], # [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], # [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], # [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], # [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]] Because the four lists below are copies of the first list, so if the first list changes, the following ones will also change.\nMore Concise Syntax # Parsing Syntax # ls = [[0]*10 for i in range(5)] ls[0][0] = 1 # [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0], # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] List Comprehension # [expression **for value in iterable** if condition]\n# Equivalent to the following code result = [] for value in iterale: if condition: result.append(expression) squares = [i**2 for i in range(1,21) if i%2 == 1] print(squares) Support multiple variables\nx = [1, 2, 3] y = [1, 2, 3] results = [ i*j for i,j in zip(x, y)] results # [1, 4, 9] Support nested loops\ncolors = [\u0026#34;black\u0026#34;, \u0026#34;white\u0026#34;] sizes = [\u0026#34;S\u0026#34;, \u0026#34;M\u0026#34;, \u0026#34;L\u0026#34;] tshirts = [\u0026#34;{} {}\u0026#34;.format(color, size) for color in colors for size in sizes] tshirts # [\u0026#39;black S\u0026#39;, \u0026#39;black M\u0026#39;, \u0026#39;black L\u0026#39;, \u0026#39;white S\u0026#39;, \u0026#39;white M\u0026#39;, \u0026#39;white L\u0026#39;] Dictionary Comprehension # squares = {i: i**2 for i in range(3)} for k, v in squares.items(): print(k, \u0026#34;: \u0026#34;, v) # 0 : 0 # 1 : 1 # 2 : 4 Set Comprehension # squares = {i**2 for i in range(10)} squares # {0, 1, 4, 9, 16, 25, 36, 49, 64, 81} Generator Expression # squares = (i**2 for i in range(10)) squares # \u0026lt;generator object \u0026lt;genexpr\u0026gt; at 0x000001DB37A58390\u0026gt; Conditional Expression # expr1 if condition else expr2\nn = -10 if n \u0026gt;= 0: x = n else: x = -n # which is equivalent to x = n if n\u0026gt;= 0 else -n # 10 "},{"id":39,"href":"/posts/python-files-exceptions-and-modules/","title":"Python Files Exceptions and Modules","section":"Blog","content":"This article will review the basic knowledge of Python, including files, exceptions, and modules.\nCH7 Files, Exceptions, and Modules # File Read and Write # File Open # The general format for opening a file, recommended to use with, because if not using with, then you need to consider the close operation # The advantage of using with block: automatically close the file after execution with open(\u0026#34;file path\u0026#34;, \u0026#34;open mode\u0026#34;, encoding = \u0026#34;character encoding of the file\u0026#34;) as f: \u0026#34;file read and write operation\u0026#34; with open(\u0026#34;E:\\ipython\\test.txt\u0026#34;, \u0026#34;r\u0026#34;, encoding = \u0026#34;gbk\u0026#34;) as f: text = f.read() print(text) Open Mode # r Read mode，if the file does not exist, an error will be reported\nw Overwrite write mode，if the file does not exist, it will be created; if the file exists, it will completely overwrite the original file\nx Create write mode，if the file does not exist, it will be created; if the file exists, an error will be reported\na Append write mode，if the file does not exist, it will be created; if the file exists, it will be added to the original file\nb Binary file mode，cannot be used alone, it needs to be used together with rb, wb, ab, this mode does not need to specify encoding\nt Text file mode，default value, it needs to be used together with rt, wt, at, generally omitted, abbreviated as r, w, a\n+，with r, w, x, a, it adds read and write functions to the original functions\nThe default open mode is read mode\nCharacter Encoding # Universal code utf-8:includes all characters needed by all countries Chinese encoding gbk:used to solve the problem of Chinese encoding, in windows, if omitted, it defaults to gbk (the encoding of the region)。 For clarity, except for processing binary files, it is recommended not to omit encoding File Read # Read the entire content——f.read() with open(\u0026#34;never_gonna_give_you_up_utf.txt\u0026#34;, \u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: text = f.read() print(text) # with open(\u0026#34;never_gonna_give_you_up_utf.txt\u0026#34;, \u0026#34;r\u0026#34;) as f: # UnicodeDecodeError: \u0026#39;gbk\u0026#39; codec can\u0026#39;t decode byte 0x80 in position 50: illegal multibyte sequence Read line by line——f.readline() with open(\u0026#34;never_gonna_give_you_up_utf.txt\u0026#34;, \u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: while True: text = f.readline() # Read each line if not text: # print(text is \u0026#34;\u0026#34;) break else: # print(text == \u0026#34;\\n\u0026#34;) # The newline character is not considered empty print(text, end=\u0026#34;\u0026#34;) # Keep the original newline, so that the print() newline does not take effect, because print itself also has a newline effect Read all lines, forming a list with each line as an element f.readlines() with open(\u0026#34;never_gonna_give_you_up_utf.txt\u0026#34;, \u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: text = f.readlines() # Note that each line has a newline character at the end print(text) # [\u0026#39;never gonna give you up\\n\u0026#39;, \u0026#39;never gonna let you down\\n\u0026#39;, \u0026#39;never gonna run around and desert you\\n\u0026#39;, \u0026#39;never gonna make you cry\\n\u0026#39;, \u0026#39;never gonna say goodbye\\n\u0026#39;, \u0026#39;never gonna tell a lie and hurt you\\n\u0026#39;] for text in f.readlines(): print(text) # If you don\u0026#39;t want to change lines, use print(text, end=\u0026#34;\u0026#34;) Text file read summary When the file is large, read() and readlines() occupy too much memory, not recommended. readline is not convenient to use\nwith open(\u0026#34;never_gonna_give_you_up_gbk.txt\u0026#34;, \u0026#34;r\u0026#34;, encoding=\u0026#34;gbk\u0026#34;) as f: for text in f: # f itself is an iterable object, and each iteration reads one line of content print(text) Binary file Image: binary file\nwith open(\u0026#34;test.jpg\u0026#34;, \u0026#34;rb\u0026#34;) as f: print(len(f.readlines())) # 69 File Write # Write a string or byte stream (binary) to a file——f.write() with open(\u0026#34;恋曲1980.txt\u0026#34;, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: f.write(\u0026#34;你曾经对我说\\n\u0026#34;) # If the file does not exist, it will immediately create one f.write(\u0026#34;你永远爱着我\\n\u0026#34;) # If you need to change lines, add a newline character \\n at the end, and the file will be written on the second line Write a list of strings to a file——f.writelines() ls = [\u0026#34;春天刮着风\\n\u0026#34;, \u0026#34;秋天下着雨\\n\u0026#34;, \u0026#34;春风秋雨多少海誓山盟随风远去\\n\u0026#34;] with open(\u0026#34;恋曲1980.txt\u0026#34;, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: f.writelines(ls) Read and Write # r+ If the file name does not exist, an error will be reported The pointer is at the beginning, and the pointer must be moved to the end to start writing, otherwise it will overwrite the previous content with open(\u0026#34;浪淘沙_北戴河.txt\u0026#34;, \u0026#34;r+\u0026#34;, encoding=\u0026#34;gbk\u0026#34;) as f: # for line in f: # print(line) # After reading all, the pointer reaches the end f.seek(0,2) # Or you can move the pointer to the end f.seek(offset in bytes, position (0: start; 1: current position; 2: end)) text = [\u0026#34;萧瑟秋风今又是，\\n\u0026#34;, \u0026#34;换了人间。\\n\u0026#34;] f.writelines(text) w+ If the file does not exist, it will be created, and if the file exists, it will be immediately cleared. with open(\u0026#34;浪淘沙_北戴河.txt\u0026#34;, \u0026#34;w+\u0026#34;, encoding=\u0026#34;gbk\u0026#34;) as f: text = [\u0026#34;萧瑟秋风今又是，\\n\u0026#34;, \u0026#34;换了人间。\\n\u0026#34;] # Clear the original content f.writelines(text) # Write new content, pointer at the end f.seek(0,0) # Move the pointer to the start print(f.read()) # Read content ​\na+: If the file does not exist, it will be created, and if the file exists, the pointer is at the end, and new content will be added, without clearing the original content. Data Storage and Reading # Common data formats can be loaded and stored in different languages\ncsv format A character sequence separated by commas that can be opened by excel\nRead with open(\u0026#34;score.csv\u0026#34;, \u0026#34;r\u0026#34;, encoding=\u0026#34;gbk\u0026#34;) as f: ls = [] for line in f: # Read line by line ls.append(line.strip(\u0026#34;\\n\u0026#34;).split(\u0026#34;,\u0026#34;)) # Remove the newline character and split by \u0026#34;,\u0026#34; for res in ls: print(res) Write ls = [[\u0026#39;id\u0026#39;, \u0026#39;math\u0026#39;, \u0026#39;chinese\u0026#39;], [\u0026#39;1\u0026#39;, \u0026#39;100\u0026#39;, \u0026#39;98\u0026#39;], [\u0026#39;2\u0026#39;, \u0026#39;96\u0026#39;, \u0026#39;99\u0026#39;], [\u0026#39;3\u0026#39;, \u0026#39;97\u0026#39;, \u0026#39;95\u0026#39;]] with open(\u0026#34;score.csv\u0026#34;, \u0026#34;w\u0026#34;, encoding=\u0026#34;gbk\u0026#34;) as f: # encoding=\u0026#34;utf-8\u0026#34; for row in ls: # Write line by line f.write(\u0026#34;,\u0026#34;.join(row)+\u0026#34;\\n\u0026#34;) # Combine with commas, add a newline character at the end The above operations can also be completed using the csv module\n2、json format\nOften used to store dictionaries\nWrite - dump() import json scores = {\u0026#34;Petter\u0026#34;:{\u0026#34;math\u0026#34;:96 , \u0026#34;physics\u0026#34;: 98}, \u0026#34;Paul\u0026#34;:{\u0026#34;math\u0026#34;:92 , \u0026#34;physics\u0026#34;: 99}, \u0026#34;Mary\u0026#34;:{\u0026#34;math\u0026#34;:98 , \u0026#34;physics\u0026#34;: 97}} with open(\u0026#34;score.json\u0026#34;, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: # Write the entire object # indent pretty print, ensure_ascii=False display Chinese json.dump(scores, f, indent=4, ensure_ascii=False) Read - load() with open(\u0026#34;score.json\u0026#34;, \u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: scores = json.load(f) # Load the entire object for k,v in scores.items(): print(k,v) # Petter {\u0026#39;math\u0026#39;: 96, \u0026#39;physics\u0026#39;: 98} # Paul {\u0026#39;math\u0026#39;: 92, \u0026#39;physics\u0026#39;: 99} # Mary {\u0026#39;math\u0026#39;: 98, \u0026#39;physics\u0026#39;: 97} Exception Handling # When an exception occurs, if no pre-set processing method is provided, the program will interrupt\nCommon Exception Generation # Division by zero - ZeroDivisionError 1/0 # ZeroDivisionError: division by zero File not found - FileNotFoundError with open(\u0026#34;nobody.csv\u0026#34;) as f: pass # FileNotFoundError: [Errno 2] No such file or directory: \u0026#39;nobody.csv\u0026#39; Value error - ValueError Passing a value that is not expected by the caller, even though the value is of the correct type\ns = \u0026#34;1.3\u0026#34; n = int(s) # ValueError: invalid literal for int() with base 10: \u0026#39;1.3\u0026#39; Index error - IndexError Index out of sequence boundary\nls = [1, 2, 3] ls[5] # IndexError: list index out of range Type error - TypeError Passing an object type that does not match the requirement\n1 + \u0026#34;3\u0026#34; # TypeError: unsupported operand type(s) for +: \u0026#39;int\u0026#39; and \u0026#39;str\u0026#39; Other common exception types NameError - Using an undefined variable\nKeyError - Attempting to access a key that does not exist in a dictionary\nprint(a) # NameError: name \u0026#39;a\u0026#39; is not defined d = {} d[\u0026#34;1\u0026#34;] # KeyError: \u0026#39;1\u0026#39; Exception Handling # Improve the stability and reliability of the program\ntry except If the try block code runs smoothly, the except block is not triggered\nIf an error occurs in the try block, the except block is triggered, and the code in the except block is executed\nSingle branch\nx = 10 y = 0 try: z = x/y except ZeroDivisionError: # Generally, it is predicted what error will occur print(\u0026#34;0 cannot be divided!\u0026#34;) # 0 cannot be divided! Multiple branches ls = [] d = {\u0026#34;name\u0026#34;: \u0026#34;timerring\u0026#34;} try: y = m # ls[3] # d[\u0026#34;age\u0026#34;] except NameError: print(\u0026#34;Variable name does not exist\u0026#34;) except IndexError: print(\u0026#34;Index out of bounds\u0026#34;) except KeyError: print(\u0026#34;Key does not exist\u0026#34;) # Variable name does not exist Universal exception Exception (the ancestor of all errors) ls = [] d = {\u0026#34;name\u0026#34;: \u0026#34;timerring\u0026#34;} try: # y = m ls[3] # d[\u0026#34;age\u0026#34;] except Exception: print(\u0026#34;Error\u0026#34;) # Error Capture the value of the exception as ls = [] d = {\u0026#34;name\u0026#34;: \u0026#34;timerring\u0026#34;} # y = x try: y = m # ls[3] # d[\u0026#34;age\u0026#34;] except Exception as e: # Although it cannot obtain the specific type of error, it can obtain the reason for the error print(e) # name \u0026#39;m\u0026#39; is not defined try except else If the try module is executed, the else module is also executed, and the else module can be seen as an additional reward for the successful try. try: with open(\u0026#34;files.txt\u0026#34;) as f: text = f.read() except FileNotFoundError: print(\u0026#34;File not found\u0026#34;) else: for s in [\u0026#34;\\n\u0026#34;, \u0026#34;，\u0026#34;, \u0026#34;。\u0026#34;, \u0026#34;？\u0026#34;]: # Remove newline and punctuation characters text = text.replace(s, \u0026#34;\u0026#34;) print(\u0026#34;files consists of {} characters.\u0026#34;.format(len(text))) # files consists of 65 characters. try except finally Whether the try module is executed, finally is executed ls = [] d = {\u0026#34;name\u0026#34;: \u0026#34;timerring\u0026#34;} # y = x try: y = m # ls[3] # d[\u0026#34;age\u0026#34;] except Exception as e: # Although it cannot obtain the specific type of error, it can obtain the value of the error print(e) finally: print(\u0026#34;Whether an exception is triggered, it will be executed\u0026#34;) # name \u0026#39;m\u0026#39; is not defined # Whether an exception is triggered, it will be executed Module Introduction # Already encapsulated, no need to \u0026ldquo;invent the wheel\u0026rdquo; yourself, declare import, and use it directly.\nBroad module classification # Python built-in # Time library time Random library random Container data type collection Iterator function itertools\nThird-party library # Data analysis numpy, pandas Data visualization matplotlib Machine learning scikit-learn Deep learning Tensorflow\nCustom file # Separate py file Package contains multiple py files, needs to add an __init__.py file. Module import # Import the entire module——import module name Call method: module name.function name or class name import time start = time.time() # Call time module\u0026#39;s time() time.sleep(3) # Call time module\u0026#39;s sleep() end = time.time() print(\u0026#34;{:.2f}s\u0026#34;.format(end-start)) Import classes or functions from a module——from module import class name or function name Call method: function name or class name from itertools import product # This case can directly use the function name for use, without adding the previous module name ls = list(product(\u0026#34;AB\u0026#34;, \u0026#34;123\u0026#34;)) # Import multiple from function import fun1, fun2 fun1.f1() fun2.f2() Import all classes and functions from a module——from module import * (not recommended) Call method: function name or class name from random import * # Wildcard import all files, so you can use without adding the module name prefix print(randint(1,100)) print(random()) Module search path # Module search order:\nPriority memory loaded module Built-in module When Python starts, the interpreter will load some modules into sys.modules sys.modules variable contains a dictionary of modules loaded into the interpreter (fully and successfully imported), with the module name as the key and its location as the value import sys print(len(sys.modules)) # 738 print(\u0026#34;math\u0026#34; in sys.modules) # True print(\u0026#34;numpy\u0026#34; in sys.modules) # False for k,v in list(sys.modules.items())[:20]: print(k, \u0026#34;:\u0026#34;, v) # sys : \u0026lt;module \u0026#39;sys\u0026#39; (built-in)\u0026gt; # builtins : \u0026lt;module \u0026#39;builtins\u0026#39; (built-in)\u0026gt; # _frozen_importlib : \u0026lt;module \u0026#39;importlib._bootstrap\u0026#39; (frozen)\u0026gt; # _imp : \u0026lt;module \u0026#39;_imp\u0026#39; (built-in)\u0026gt; # _thread : \u0026lt;module \u0026#39;_thread\u0026#39; (built-in)\u0026gt; # _warnings : \u0026lt;module \u0026#39;_warnings\u0026#39; (built-in)\u0026gt; # _weakref : \u0026lt;module \u0026#39;_weakref\u0026#39; (built-in)\u0026gt; # zipimport : \u0026lt;module \u0026#39;zipimport\u0026#39; (built-in)\u0026gt; # _frozen_importlib_external : \u0026lt;module \u0026#39;importlib._bootstrap_external\u0026#39; (frozen)\u0026gt; # _io : \u0026lt;module \u0026#39;io\u0026#39; (built-in)\u0026gt; # marshal : \u0026lt;module \u0026#39;marshal\u0026#39; (built-in)\u0026gt; # nt : \u0026lt;module \u0026#39;nt\u0026#39; (built-in)\u0026gt; # winreg : \u0026lt;module \u0026#39;winreg\u0026#39; (built-in)\u0026gt; # encodings : \u0026lt;module \u0026#39;encodings\u0026#39; from \u0026#39;C:\\\\Users\\\\ibm\\\\Anaconda3\\\\lib\\\\encodings\\\\__init__.py\u0026#39;\u0026gt; # codecs : \u0026lt;module \u0026#39;codecs\u0026#39; from \u0026#39;C:\\\\Users\\\\ibm\\\\Anaconda3\\\\lib\\\\codecs.py\u0026#39;\u0026gt; # _codecs : \u0026lt;module \u0026#39;_codecs\u0026#39; (built-in)\u0026gt; # encodings.aliases : \u0026lt;module \u0026#39;encodings.aliases\u0026#39; from \u0026#39;C:\\\\Users\\\\ibm\\\\Anaconda3\\\\lib\\\\encodings\\\\aliases.py\u0026#39;\u0026gt; # encodings.utf_8 : \u0026lt;module \u0026#39;encodings.utf_8\u0026#39; from \u0026#39;C:\\\\Users\\\\ibm\\\\Anaconda3\\\\lib\\\\encodings\\\\utf_8.py\u0026#39;\u0026gt; # _signal : \u0026lt;module \u0026#39;_signal\u0026#39; (built-in)\u0026gt; # __main__ : \u0026lt;module \u0026#39;__main__\u0026#39;\u0026gt; modules in sys.path import sys sys.path # [\u0026#39;E:\\\\ipython\u0026#39;, # \u0026#39;C:\\\\Users\\\\ibm\\\\Anaconda3\\\\python37.zip\u0026#39;, # \u0026#39;C:\\\\Users\\\\ibm\\\\Anaconda3\\\\DLLs\u0026#39;, # \u0026#39;C:\\\\Users\\\\ibm\\\\Anaconda3\\\\lib\u0026#39;, # \u0026#39;C:\\\\Users\\\\ibm\\\\Anaconda3\u0026#39;, # \u0026#39;\u0026#39;, # \u0026#39;C:\\\\Users\\\\ibm\\\\AppData\\\\Roaming\\\\Python\\\\Python37\\\\site-packages\u0026#39;, # \u0026#39;C:\\\\Users\\\\ibm\\\\Anaconda3\\\\lib\\\\site-packages\u0026#39;, # \u0026#39;C:\\\\Users\\\\ibm\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32\u0026#39;, # \u0026#39;C:\\\\Users\\\\ibm\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib\u0026#39;, # \u0026#39;C:\\\\Users\\\\ibm\\\\Anaconda3\\\\lib\\\\site-packages\\\\Pythonwin\u0026#39;, # \u0026#39;C:\\\\Users\\\\ibm\\\\Anaconda3\\\\lib\\\\site-packages\\\\IPython\\\\extensions\u0026#39;, # \u0026#39;C:\\\\Users\\\\ibm\\\\.ipython\u0026#39;] The first path in sys.path is the folder where the current execution file is located If you need to import a module that is not in this folder, you need to add the module path to sys.path import sys sys.path.append(\u0026#34;C:\\\\Users\\\\ibm\\\\Desktop\u0026#34;) # Note that it is double slashes, if it is win import fun3 fun3.f3() # Import fun3 successfully "},{"id":40,"href":"/posts/python-object-oriented-programming/","title":"Python Object Oriented Programming","section":"Blog","content":"This article will review the object-oriented programming of Python. Mainly about the class, inheritance, and polymorphism.\nCH6 Class # Introduction # Everything is an object Everything has its own attributes Everything has its own methods Class is the carrier of objects\nEach cat is an object, we can abstract the common features of a class of objects, and create a generic class.\n# Create a class class Cat(): \u0026#34;\u0026#34;\u0026#34;Simulate a cat\u0026#34;\u0026#34;\u0026#34; def __init__(self, name): \u0026#34;\u0026#34;\u0026#34;Initialize the attributes\u0026#34;\u0026#34;\u0026#34; self.name = name def jump(self): \u0026#34;\u0026#34;\u0026#34;Simulate a cat jumping\u0026#34;\u0026#34;\u0026#34; print(self.name + \u0026#34; is jumping\u0026#34;) my_cat = Cat(\u0026#34;Loser\u0026#34;) your_cat = Cat(\u0026#34;Lucky\u0026#34;) print(my_cat.name) # Loser print(your_cat.name) # Lucky # Call method my_cat.jump() # Loser is jumping your_cat.jump() # Lucky is jumping Define a class # Three elements: class name, attributes, methods\nClass naming # Camel case - the first letter of each word is capitalized Leave two blank lines before the \u0026ldquo;class\u0026rdquo; and leave two blank lines after the class Class attributes # class Car(): \u0026#34;\u0026#34;\u0026#34;Simulate a car\u0026#34;\u0026#34;\u0026#34; # def __init__(self, the parameters to be passed) def __init__(self, brand, model, year): \u0026#34;\u0026#34;\u0026#34;Initialize the attributes of the car\u0026#34;\u0026#34;\u0026#34; self.brand = brand self.model = model self.year = year self.mileage = 0 Class methods # A function defined inside a class\nclass Car(): \u0026#34;\u0026#34;\u0026#34;Simulate a car\u0026#34;\u0026#34;\u0026#34; def __init__(self, brand, model, year): \u0026#34;\u0026#34;\u0026#34;Initialize the attributes of the car\u0026#34;\u0026#34;\u0026#34; self.brand = brand self.model = model self.year = year self.mileage = 0 def get_main_information(self): # you cannot omit self \u0026#34;\u0026#34;\u0026#34;Get the main information of the car\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Brand: {} Model: {} Year: {}\u0026#34;.format(self.brand, self.model, self.year)) def get_mileage(self): \u0026#34;\u0026#34;\u0026#34;Get the mileage of the car\u0026#34;\u0026#34;\u0026#34; return \u0026#34;Mileage: {} km\u0026#34;.format(self.mileage) Create an instance # Assign the instance to an object, and pass the corresponding parameters during instantiation.\nmy_new_car = Car(\u0026#34;Audi\u0026#34;, \u0026#34;A6\u0026#34;, 2018) Access attributes # Instance name.attribute name\nprint(my_new_car.brand) # Audi print(my_new_car.model) # A6 print(my_new_car.year) # 2018 Call methods # Instance name.method name(necessary parameters)\nmy_new_car = Car(\u0026#34;Audi\u0026#34;, \u0026#34;A6\u0026#34;, 2018) my_new_car.get_main_information() # Brand: Audi Model: A6 Year: 2018 Modify attributes # Direct modification # First access, then modify\nmy_old_car.mileage = 12000 print(my_old_car.mileage) # 12000 Modify attributes through methods # class Car(): \u0026#34;\u0026#34;\u0026#34;Simulate a car\u0026#34;\u0026#34;\u0026#34; def __init__(self, brand, model, year): \u0026#34;\u0026#34;\u0026#34;Initialize the attributes of the car\u0026#34;\u0026#34;\u0026#34; self.brand = brand self.model = model self.year = year self.mileage = 0 def get_main_information(self): # self cannot be omitted \u0026#34;\u0026#34;\u0026#34;Get the main information of the car\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Brand: {} Model: {} Year: {}\u0026#34;.format(self.brand, self.model, self.year)) def set_mileage(self, distance): \u0026#34;\u0026#34;\u0026#34;Set the mileage of the car\u0026#34;\u0026#34;\u0026#34; self.mileage = distance my_old_car.set_mileage(8000) You can create an infinite number of instances\nmy_new_car = Car(\u0026#34;Audi\u0026#34;, \u0026#34;A6\u0026#34;, 2018) my_cars = [my_new_car, my_old_car] Inheritance of classes # Inheritance is the process of low-level abstraction inheriting high-level abstraction\nSimple inheritance # Parent class\nclass Car(): \u0026#34;\u0026#34;\u0026#34;Simulate a car\u0026#34;\u0026#34;\u0026#34; def __init__(self, brand, model, year): \u0026#34;\u0026#34;\u0026#34;Initialize the attributes of the car\u0026#34;\u0026#34;\u0026#34; self.brand = brand self.model = model self.year = year self.mileage = 0 def get_main_information(self): # self cannot be omitted \u0026#34;\u0026#34;\u0026#34;Get the main information of the car\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Brand: {} Model: {} Year: {}\u0026#34;.format(self.brand, self.model, self.year)) def get_mileage(self): \u0026#34;\u0026#34;\u0026#34;Get the mileage of the car\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Mileage: {} km\u0026#34;.format(self.mileage)) def set_mileage(self, distance): \u0026#34;\u0026#34;\u0026#34;Set the mileage of the car\u0026#34;\u0026#34;\u0026#34; if distance \u0026gt;= 0: self.mileage = distance else: print(\u0026#34;Mileage cannot be negative!\u0026#34;) def increment_mileage(self, distance): \u0026#34;\u0026#34;\u0026#34;Accumulate the mileage\u0026#34;\u0026#34;\u0026#34; if distance \u0026gt;= 0: self.mileage += distance else: print(\u0026#34;The added mileage cannot be negative!\u0026#34;) Subclass\nclass Subclass name (Parent class name):\nCreate an electric car class class ElectricCar(Car): \u0026#34;\u0026#34;\u0026#34;Simulate an electric car\u0026#34;\u0026#34;\u0026#34; def __init__(self, brand, model, year): \u0026#34;\u0026#34;\u0026#34;Initialize the attributes of the electric car\u0026#34;\u0026#34;\u0026#34; super().__init__(brand, model, year) # Declare the inheritance of the parent class, super is the superclass (parent class) Automatically inherit all methods from the parent class my_electric_car = ElectricCar(\u0026#34;NextWeek\u0026#34;, \u0026#34;FF91\u0026#34;, 2046) my_electric_car.get_main_information() # Brand: NextWeek Model: FF91 Year: 2046 Add attributes and methods to the subclass # class ElectricCar(Car): \u0026#34;\u0026#34;\u0026#34;Simulate an electric car\u0026#34;\u0026#34;\u0026#34; def __init__(self, brand, model, year, bettery_size):# New parameters: bettery_size \u0026#34;\u0026#34;\u0026#34;Initialize the attributes of the electric car\u0026#34;\u0026#34;\u0026#34; super().__init__(brand, model, year) # Declare the inheritance of the parent class self.bettery_size = bettery_size # Battery capacity self.electric_quantity = bettery_size # Battery remaining capacity self.electric2distance_ratio = 5 # Electric quantity distance conversion coefficient 5 km/kW.h self.remainder_range = self.electric_quantity*self.electric2distance_ratio # Remaining mileage def get_electric_quantit(self): \u0026#34;\u0026#34;\u0026#34;Get the current battery capacity\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Current battery remaining capacity: {} kW.h\u0026#34;.format(self.electric_quantity)) def set_electric_quantity(self, electric_quantity): \u0026#34;\u0026#34;\u0026#34;Set the battery remaining capacity, recalculate the mileage that can be supported\u0026#34;\u0026#34;\u0026#34; if electric_quantity \u0026gt;= 0 and electric_quantity \u0026lt;= self.bettery_size: self.electric_quantity = electric_quantity self.remainder_range = self.electric_quantity*self.electric2distance_ratio else: print(\u0026#34;The battery remaining capacity is not set in a reasonable range!\u0026#34;) def get_remainder_range(self): \u0026#34;\u0026#34;\u0026#34;Get the remaining mileage\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;The current battery remaining capacity can continue driving {} km\u0026#34;.format(self.remainder_range)) my_electric_car = ElectricCar(\u0026#34;NextWeek\u0026#34;, \u0026#34;FF91\u0026#34;, 2046, 70) my_electric_car.get_electric_quantit() # Current battery remaining capacity: 70 kW.h my_electric_car.get_remainder_range() # The current battery remaining capacity can continue driving 350 km my_electric_car.set_electric_quantity(50) # Reset the battery remaining capacity my_electric_car.get_electric_quantit() # Current battery remaining capacity: 50 kW.h my_electric_car.get_remainder_range() # The current battery remaining capacity can continue driving 250 km Overwrite the parent class method - Polymorphism # First look for the method in the subclass, if not found, look for the method in the parent class. Therefore, the subclass can overwrite the parent class method.\nclass ElectricCar(Car): \u0026#34;\u0026#34;\u0026#34;Simulate an electric car\u0026#34;\u0026#34;\u0026#34; def __init__(self, brand, model, year, bettery_size): \u0026#34;\u0026#34;\u0026#34;Initialize the attributes of the electric car\u0026#34;\u0026#34;\u0026#34; super().__init__(brand, model, year) # Declare the inheritance of the parent class self.bettery_size = bettery_size # Battery capacity self.electric_quantity = bettery_size # Battery remaining capacity self.electric2distance_ratio = 5 # Electric quantity distance conversion coefficient 5 km/kW.h self.remainder_range = self.electric_quantity*self.electric2distance_ratio # Remaining mileage def get_main_information(self): # Overwrite the parent class method \u0026#34;\u0026#34;\u0026#34;Get the main information of the car\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Brand: {} Model: {} Year: {} Remaining mileage: {} km\u0026#34; .format(self.brand, self.model, self.year, self.bettery_size*self.electric2distance_ratio)) my_electric_car = ElectricCar(\u0026#34;NextWeek\u0026#34;, \u0026#34;FF91\u0026#34;, 2046, 70) my_electric_car.get_main_information() # Brand: NextWeek Model: FF91 Year: 2046 Remaining mileage: 350 km Using instances in classes # Abstract the battery as an object.\nclass Bettery(): \u0026#34;\u0026#34;\u0026#34;Simulate the battery of an electric car\u0026#34;\u0026#34;\u0026#34; def __init__(self, bettery_size = 70): self.bettery_size = bettery_size # Battery capacity self.electric_quantity = bettery_size # Battery remaining capacity self.electric2distance_ratio = 5 # Electric quantity distance conversion coefficient 5 km/kW.h self.remainder_range = self.electric_quantity*self.electric2distance_ratio # Remaining mileage def get_electric_quantit(self): \u0026#34;\u0026#34;\u0026#34;Get the current battery remaining capacity\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Current battery remaining capacity: {} kW.h\u0026#34;.format(self.electric_quantity)) def set_electric_quantity(self, electric_quantity): \u0026#34;\u0026#34;\u0026#34;Set the battery remaining capacity, recalculate the mileage that can be supported\u0026#34;\u0026#34;\u0026#34; if electric_quantity \u0026gt;= 0 and electric_quantity \u0026lt;= self.bettery_size: self.electric_quantity = electric_quantity self.remainder_range = self.electric_quantity*self.electric2distance_ratio else: print(\u0026#34;The battery remaining capacity is not set in a reasonable range!\u0026#34;) def get_remainder_range(self): \u0026#34;\u0026#34;\u0026#34;Get the remaining mileage\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;The current battery remaining capacity can continue driving {} km\u0026#34;.format(self.remainder_range)) class ElectricCar(Car): \u0026#34;\u0026#34;\u0026#34;Simulate an electric car\u0026#34;\u0026#34;\u0026#34; def __init__(self, brand, model, year, bettery_size): \u0026#34;\u0026#34;\u0026#34;Initialize the attributes of the electric car\u0026#34;\u0026#34;\u0026#34; super().__init__(brand, model, year) # Declare the inheritance of the parent class self.bettery = Bettery(bettery_size) # Battery def get_main_information(self): # Overwrite the parent class method \u0026#34;\u0026#34;\u0026#34;Get the main information of the car\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Brand: {} Model: {} Year: {} Remaining mileage: {} km\u0026#34; .format(self.brand, self.model, self.year, self.bettery.bettery_size*self.bettery.electric2distance_ratio)) my_electric_car = ElectricCar(\u0026#34;NextWeek\u0026#34;, \u0026#34;FF91\u0026#34;, 2046, 70) my_electric_car.get_main_information() # Brand: NextWeek Model: FF91 Year: 2046 Remaining mileage: 350 km my_electric_car.bettery.get_electric_quantit() # Current battery remaining capacity: 70 kW.h my_electric_car.bettery.set_electric_quantity(50) # Reset the battery remaining capacity my_electric_car.bettery.get_electric_quantit() # Current battery remaining capacity: 50 kW.h my_electric_car.bettery.get_remainder_range() # The current battery remaining capacity can continue driving 250 km "},{"id":41,"href":"/posts/python-cheatsheet/","title":"Python Cheatsheet","section":"Blog","content":" Basic Data Types # bin(16) oct(16) hex(16)\nint(a, 2) int(b, 8) int(c, 16)\nround(a, 1)\npow(x,n)\nForward indexing - starts incrementing from 0, and a space is also a position. Backward indexing - starts decrementing from -1, and a space is also a position. variable[start🔚step]\nwhen the step is -1, which means the previous position is greater than the latter position by -1.\nString # .split(\u0026quot; \u0026quot;) \u0026quot;,\u0026quot;.join(str) .strip(the character to be removed) .replace(\u0026quot;to be replaced\u0026quot;，\u0026quot;replace\u0026quot;) .count(\u0026quot;str\u0026quot;) .upper() .lower() .title() any() all() type() isinstance(var, type) str(num) int() float() eval() List # variable[start🔚step] use + to concatenate lists use * to duplicate lists\n.append(element) .extend(list) .insert(position, element) .remove(element) default is the first element meeting the condition .pop(position) default is the last .clear() empty the list .index(element) return its index .count(element) .reverse() in-place reverse, no return value .copy() shallow copy == [:] .sort() in-place sort, no return value. list.sort(reverse = True) sorted(列表) temporary sort, original list remains unchanged. sorted(列表, reverse = True) Tuple # def f1(x): return x2, x3 # packing\na, b = f1(3) unpacking zip() packing\nDictionary # unordered key must be immutable dict[key] variable[new key] = new value del variable[key] .keys() return the list of keys .values() return the list of values .items() return the list of key-value pairs .get(key, default) get the value of the key, if the key does not exist, return the default value .update(dict) update the dictionary, if the key does not exist, add it .pop(key) delete the value of the key .clear() clear the dictionary .copy() shallow copy Set # unordered, which means the pop is invalid element must be immutable .add(element) .remove(element) .clear() .copy() S \u0026amp; T intersection S | T union S - T difference S ^ T XOR "},{"id":42,"href":"/posts/python-function-parameter/","title":"Python Function Parameter","section":"Blog","content":"This blog will review the function parameter of Python. Mainly about the parameter passing, Keyword and Position Parameter, Anonymous Function, and unit test with assert.\nCH5 Function # Function Parameter # Parameter Passing # Formal and Actual Parameters # Formal parameter: parameter in function definition, actually variable name Actual parameter: parameter in function call, actually variable value Position Parameter # Assign value to formal parameter according to position order Actual parameter and formal parameter must be one-to-one, one cannot be more, one cannot be less def function(x, y, z): print(x, y, z) function(1, 2, 3) # x = 1; y = 2; z = 3 Keyword Parameter # Break position limit, directly call name to pass value (formal parameter = actual parameter) def function(x, y, z): print(x, y, z) function(y=1, z=2, x=3) # x = 1; y = 2; z = 3 Position parameter can be mixed with keyword parameter, position parameter must be placed before keyword parameter function(1, z=2, y=3) Default Parameter # Assign value to formal parameter in definition stage – the common value of the parameter Default parameter must be placed after non-default parameter When calling function, the parameter can be omitted def register(name, age, sex=\u0026#34;male\u0026#34;): print(name, age, sex) register(\u0026#34;timerring\u0026#34;, 18) Default parameter should be set to immutable type (number, string, tuple) def function(ls=[]): print(id(ls)) ls.append(1) print(id(ls)) print(ls) function() # 1759752744328 # 1759752744328 # [1] function() # 1759752744328 # 1759752744328 # [1, 1] As can be seen from the above, the address of the list has not changed. Each operation is performed on the original address list, and the content has changed, so it seems to have a memory function. Because the default parameter is set to a mutable type (list).\ndef function(ls=\u0026#34;Python\u0026#34;): print(id(ls)) ls += \u0026#34;3.7\u0026#34; print(id(ls)) print(ls) function() # 1759701700656 # 1759754352240 # Python3.7 function() # 1759701700656 # 1759754353328 # Python3.7 Will not produce \u0026ldquo;memory function\u0026rdquo;, each increment to a new address\nVariable Length Parameter *args # Don\u0026rsquo;t know how many parameters will be passed *args This parameter must be placed at the end of the parameter list def foo(x, y, z, *args): print(x, y ,z) print(args) foo(1, 2, 3, 4, 5, 6) # Extra parameters, packaged and passed to args # 1 2 3 # (4, 5, 6) Unpacking actual parameters def foo(x, y, z, *args): print(x, y ,z) print(args) foo(1, 2, 3, [4, 5, 6]) # List is packaged as a tuple and assigned to args # 1 2 3 # ([4, 5, 6],) foo(1, 2, 3, *[4, 5, 6]) # * unpacks these lists, strings, tuples, or sets # 1 2 3 # (4, 5, 6) Variable Length Parameter **kwargs # def foo(x, y, z, **kwargs): print(x, y ,z) print(kwargs) foo(1, 2, 3, a=4, b=5, c=6) # Extra parameters, packaged and passed to kwargs in the form of a dictionary # 1 2 3 # {\u0026#39;a\u0026#39;: 4, \u0026#39;b\u0026#39;: 5, \u0026#39;c\u0026#39;: 6} Unpacking actual parameters def foo(x, y, z, **kwargs): print(x, y ,z) print(kwargs) foo(1, 2, 3, **{\u0026#34;a\u0026#34;: 4, \u0026#34;b\u0026#34;: 5, \u0026#34;c\u0026#34;:6}) # 1 2 3 # {\u0026#39;a\u0026#39;: 4, \u0026#39;b\u0026#39;: 5, \u0026#39;c\u0026#39;: 6} Variable Length Parameter Combination # def foo(*args, **kwargs): print(args) print(kwargs) foo(1, 2, 3, a=4, b=5, c=6) # (1, 2, 3) # {\u0026#39;a\u0026#39;: 4, \u0026#39;b\u0026#39;: 5, \u0026#39;c\u0026#39;: 6} Function Body and Variable Scope # Function body is a piece of code that will only be executed when the function is called, and the code structure is no different from other code Local variable – only defined and used within the function body Global variable – all variables defined outside are global variables, and global variables can be used directly within the function body Define global variable in function body through global def multipy(x, y): global z z = x*y return z print(multipy(2, 9)) # 18 print(z) # 18 Return Value # Single Return Value # def foo(x): return x**2 Multiple Return Values – in the form of a tuple # def foo(x): return 1, x, x**2, x**3 # Comma separated, packaged and returned print(foo(3)) # (1, 3, 9, 27) a, b , c, d = foo(3) # Unpacking assignment print(a) # 1 print(b) # 3 print(c) # 9 print(d) # 27 No return statement default is None # Suggestions # Function and parameter naming:A combination of lowercase letters and underscores. Should include a brief description of the function\u0026rsquo;s functionality, followed by the function definition def foo(): # This function is used to... pass Leave two lines before and after the function definition def f1(): pass # Leave two lines def f2(): pass Default parameter assignment does not require spaces on both sides Unit Test with assert – Assertion # assert expression Trigger exception when expression result is false assert game_over(21, 8) == False --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) \u0026lt;ipython-input-42-88b651626036\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 4 assert game_over(21, 8) == False AssertionError: Anonymous Function # The most suitable use of anonymous functions is with key = lambda Ensemble: Function body, especially when paired with sort() sorted()\nSort sort() sorted() ls = [(93, 88), (79, 100), (86, 71), (85, 85), (76, 94)] ls.sort(key = lambda x: x[1])# Sort by the second data of each tuple ls # [(86, 71), (85, 85), (93, 88), (76, 94), (79, 100)] ls = [(93, 88), (79, 100), (86, 71), (85, 85), (76, 94)] temp = sorted(ls, key = lambda x: x[0]+x[1], reverse=True)# Get descending sort temp # [(93, 88), (79, 100), (85, 85), (76, 94), (86, 71)] max() min() ls = [(93, 88), (79, 100), (86, 71), (85, 85), (76, 94)] n = max(ls, key = lambda x: x[1]) n # (79, 100) "},{"id":43,"href":"/posts/python-control-structure/","title":"Python Control Structure","section":"Blog","content":"This blog will review the control structure of Python.\nCH4 Program Control Structure # Condition Test # Logical Operation # Priority of compound logical operations\nNon \u0026gt; And \u0026gt; Or print((a \u0026gt; b) and (b \u0026gt; c)) # And print((a \u0026gt; b) or (b \u0026gt; c)) # Or print(not(a \u0026gt; b)) # Not Existence Operation # Element in list/string\nBranch Structure if Statement # Simple Version # if age \u0026gt; 7: print(\u0026#34;if\u0026#34;) else: print(\u0026#34;else\u0026#34;) Multiple Branch # if age \u0026lt; 7: print(\u0026#34;7\u0026#34;) elif age \u0026lt; 13: print(\u0026#34;13\u0026#34;) elif age \u0026lt; 60: print(\u0026#34;60\u0026#34;) else: # Sometimes for clarity, it can also be written as elif age \u0026gt;= 60: print(\u0026#34;60\u0026#34;) No matter how many branches, only one branch is executed\nIteration Loop for Loop # Execution Process # Extract each element from the iterable object and perform the corresponding operation List[ ], tuple( ), set{ }, string\u0026quot; \u0026quot;\ngraduates = (\u0026#34;apple\u0026#34;, \u0026#34;google\u0026#34;, \u0026#34;timerring\u0026#34;) for graduate in graduates: print(\u0026#34;Congratulations, \u0026#34;+graduate) Dictionary\nstudents = {201901: \u0026#39;apple\u0026#39;, 201902: \u0026#39;google\u0026#39;, 201903: \u0026#39;timerring\u0026#39;} for k, v in students.items(): print(k, v) for student in students.keys(): # for student in students is equivalent to above range()\nres = [] for i in range(1, 10, 2): res.append(i ** 2) print(res) break and continue # break End whole cycle continue End this cycle for and else # If for loop is executed completely without being interrupted by break, then run else block\nproduct_scores = [89, 90, 99, 70, 67, 78, 85, 92, 77, 82] i = 0 for score in product_scores: if score \u0026lt; 75: i+=1 if i == 2: print(\u0026#34;Product sampling fails\u0026#34;) break else: print(\u0026#34;Product sampling qualified\u0026#34;) while # while and else # If while loop is executed completely without being interrupted by break, then run else block\ncount = 0 while count \u0026lt;= 2 : count += 1 print(\u0026#34;Loop\u0026#34;,count) else: print(\u0026#34;over\u0026#34;) # Loop 1 # Loop 2 # Loop 3 # over "},{"id":44,"href":"/posts/python-composite-data-type/","title":"Python Composite Data Type","section":"Blog","content":"This article will review the composite data type in Python, such as list, tuple, dictionary, and set.\nCH3 Composite Data Type # List # List Definition # Sequence type: Internal elements have positional relationships and can be accessed by position number List is a sequence type that can use multiple types of elements, supports element addition, deletion, query, and modification operations ls = [\u0026#34;Python\u0026#34;, 1989, True, {\u0026#34;version\u0026#34;: 3.7}] Another way to generate: list(iterable), iterable includes: string, tuple, set, range, etc. list(\u0026#34;Welcome to subscribe this column\u0026#34;) # [\u0026#39;W\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39; \u0026#39;, \u0026#39;t\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39; \u0026#39;, \u0026#39;s\u0026#39;, \u0026#39;u\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;s\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;r\u0026#39;, \u0026#39;i\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39; \u0026#39;, \u0026#39;t\u0026#39;, \u0026#39;h\u0026#39;, \u0026#39;i\u0026#39;, \u0026#39;s\u0026#39;, \u0026#39; \u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;u\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;n\u0026#39;] list((\u0026#34;I\u0026#34;, \u0026#34;am\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;student\u0026#34;)) # [\u0026#39;I\u0026#39;, \u0026#39;am\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;student\u0026#39;] list({\u0026#34;Jim\u0026#34;, \u0026#34;Green\u0026#34;}) # [\u0026#39;Green\u0026#39;, \u0026#39;Jim\u0026#39;] range(start number, end number, number interval)\nIf the start number is omitted, it defaults to 0 Must include the end number, closed on the left and open on the right The number interval is omitted, it defaults to 1 for i in range(1, 11, 2): print(i) # 1 # 3 # 5 # 7 # 9 list(range(1, 11, 2)) # [1, 3, 5, 7, 9] List Properties # List length —— len(list) List index —— Same as string cars = [\u0026#34;BYD\u0026#34;, \u0026#34;BMW\u0026#34;, \u0026#34;AUDI\u0026#34;, \u0026#34;TOYOTA\u0026#34;] print(cars[0]) # BYD List Slicing # variable name[start position:end position:slice interval]\ncars = [\u0026#34;BYD\u0026#34;, \u0026#34;BMW\u0026#34;, \u0026#34;AUDI\u0026#34;, \u0026#34;TOYOTA\u0026#34;] print(cars[:3]) # The first three elements, start position omitted, defaults to 0; slice interval omitted, defaults to 1 # [\u0026#39;BYD\u0026#39;, \u0026#39;BMW\u0026#39;, \u0026#39;AUDI\u0026#39;] print(cars[1:4:2]) # The second to fourth elements, the difference between the front and back index is 2 # [\u0026#39;BMW\u0026#39;, \u0026#39;TOYOTA\u0026#39;] print(cars[:]) # Get the entire list, end position omitted, defaults to the last # [\u0026#39;BYD\u0026#39;, \u0026#39;BMW\u0026#39;, \u0026#39;AUDI\u0026#39;, \u0026#39;TOYOTA\u0026#39;] print(cars[-4:-2]) # Get the first two elements # [\u0026#39;BYD\u0026#39;, \u0026#39;BMW\u0026#39;] Reverse slicing cars = [\u0026#34;BYD\u0026#34;, \u0026#34;BMW\u0026#34;, \u0026#34;AUDI\u0026#34;, \u0026#34;TOYOTA\u0026#34;] print(cars[:-4:-1]) # Start position omitted, defaults to -1 # [\u0026#39;TOYOTA\u0026#39;, \u0026#39;AUDI\u0026#39;, \u0026#39;BMW\u0026#39;] print(cars[::-1]) # Get the reverse list # [\u0026#39;TOYOTA\u0026#39;, \u0026#39;AUDI\u0026#39;, \u0026#39;BMW\u0026#39;, \u0026#39;BYD\u0026#39;] List Operators # Use + to concatenate lists Use * to multiply lists [0]*10 # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] List Methods # Add Element # Add element to the end —— list.append(element) Insert element at any position —— list.insert(position number, element) Append another list to the end —— list.extend(another list) append will add the entire list as a single element to the end of the list.\nlanguages.append([\u0026#34;Ruby\u0026#34;, \u0026#34;PHP\u0026#34;]) # [\u0026#39;Python\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;C++\u0026#39;, \u0026#39;R\u0026#39;, \u0026#39;Java\u0026#39;, [\u0026#39;Ruby\u0026#39;, \u0026#39;PHP\u0026#39;]] extend will add each element in the second list to the first list.\nlanguages = [\u0026#39;Python\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;C++\u0026#39;, \u0026#39;R\u0026#39;, \u0026#39;Java\u0026#39;] languages.extend([\u0026#34;Ruby\u0026#34;, \u0026#34;PHP\u0026#34;]) # [\u0026#39;Python\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;C++\u0026#39;, \u0026#39;R\u0026#39;, \u0026#39;Java\u0026#39;, \u0026#39;Ruby\u0026#39;, \u0026#39;PHP\u0026#39;] Delete Element # Delete element at list i position —— list.pop(i) languages = [\u0026#39;Python\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;C++\u0026#39;, \u0026#39;R\u0026#39;, \u0026#39;Java\u0026#39;] languages.pop(1) # [\u0026#39;Python\u0026#39;, \u0026#39;C++\u0026#39;, \u0026#39;R\u0026#39;, \u0026#39;Java\u0026#39;] Do not write position information, default to delete the last element —— list.pop() Delete the first occurrence of the element to be deleted in the list —— list.remove(element) Find Element # The position of the first occurrence of the element to be searched in the list —— list.index(element) Modify Element # Modify element by \u0026ldquo;index first, then assign\u0026rdquo; —— list[position]=new value List Copy # languages_2 = languages Incorrect way: This only creates an alias for the list\nCorrect way —— Shallow copy\nMethod 1: list.copy()\nMethod 2: list[:] is equivalent to slicing the entire list\nList Sorting # Use list.sort() to sort the list in place, no return value, default to increasing. Decreasing order list.sort(reverse = True) Use sorted(list) to temporarily sort the list, the original list remains unchanged, and the sorted list is returned. The same decreasing order sorted(list, reverse = True) List Reversal # Use list.reverse() to reverse the list in place, no return value List Traversal # Use for loop Tuple # Tuple Expression # Treat the tuple as an \u0026ldquo;immutable list\u0026rdquo; Does not support element addition, element deletion, element modification operations, and other operations are completely consistent with list operations Common Uses of Tuple # Packing and Unpacking\nExample 1: The return value is packaged as a tuple def f1(x): return x**2, x**3 # Packing return print(f1(3)) # (9, 27) print(type(f1(3))) # \u0026lt;class \u0026#39;tuple\u0026#39;\u0026gt; a, b = f1(3) # Unpacking assignment print(a) # 9 print(b) # 27 Example 2: Use zip function to pack numbers = [201901, 201902, 201903] name = [\u0026#34;Apple\u0026#34;, \u0026#34;Google\u0026#34;, \u0026#34;Tesla\u0026#34;] list(zip(numbers, name)) # [(201901, \u0026#39;Apple\u0026#39;), (201902, \u0026#39;Google\u0026#39;), (201903, \u0026#39;Tesla\u0026#39;)] # Unpack each tuple immediately for number,name in zip(numbers,name): print(number, name) # 201901 Apple # 201902 Google # 201903 Tesla Dictionary # Dictionary Expression # Regular dictionary is unordered and can only be accessed by key Dictionary Key Requirements\nDictionary keys cannot be repeated. If they are repeated, the previous keys are overwritten.\nDictionary keys must be immutable types，if the key is mutable, the corresponding stored value cannot be found.\nImmutable types: numbers, strings, tuples. Mutable types: lists, dictionaries, sets. Once determined, they can be freely added, deleted, and modified. Therefore, these three types cannot be used as dictionary keys. Dictionary Properties # Dictionary length len() —— Number of key-value pairs Dictionary index, get the corresponding value through dictionary[key] Dictionary Operations # Add key-value pair\nvariable name[new key] = new value Delete key-value pair\ndel variable name[key to be deleted] variable name.pop(key to be deleted) variable name.popitem() Randomly delete a key-value pair and return it as a tuple Modify value\nModify the corresponding value through \u0026ldquo;index first, then assign\u0026rdquo; d.get( )\nd.get(key, default) Get the value corresponding to the key from the dictionary d, if there is no such key, return default\nExample: Count the frequency of characters in \u0026ldquo;牛奶奶找刘奶奶买牛奶\u0026rdquo;\ns = \u0026#34;牛奶奶找刘奶奶买牛奶\u0026#34; d = {} print(d) for i in s: d[i] = d.get(i, 0)+1 # If the character appears for the first time, return default 0, then add 1 to count. If there is already a key i, return the value corresponding to the key i. print(d) # {} # {\u0026#39;牛\u0026#39;: 1} # {\u0026#39;牛\u0026#39;: 1, \u0026#39;奶\u0026#39;: 1} # {\u0026#39;牛\u0026#39;: 1, \u0026#39;奶\u0026#39;: 2} # {\u0026#39;牛\u0026#39;: 1, \u0026#39;奶\u0026#39;: 2, \u0026#39;找\u0026#39;: 1} # {\u0026#39;牛\u0026#39;: 1, \u0026#39;奶\u0026#39;: 2, \u0026#39;找\u0026#39;: 1, \u0026#39;刘\u0026#39;: 1} # {\u0026#39;牛\u0026#39;: 1, \u0026#39;奶\u0026#39;: 3, \u0026#39;找\u0026#39;: 1, \u0026#39;刘\u0026#39;: 1} # {\u0026#39;牛\u0026#39;: 1, \u0026#39;奶\u0026#39;: 4, \u0026#39;找\u0026#39;: 1, \u0026#39;刘\u0026#39;: 1} # {\u0026#39;牛\u0026#39;: 1, \u0026#39;奶\u0026#39;: 4, \u0026#39;找\u0026#39;: 1, \u0026#39;刘\u0026#39;: 1, \u0026#39;买\u0026#39;: 1} # {\u0026#39;牛\u0026#39;: 2, \u0026#39;奶\u0026#39;: 4, \u0026#39;找\u0026#39;: 1, \u0026#39;刘\u0026#39;: 1, \u0026#39;买\u0026#39;: 1} # {\u0026#39;牛\u0026#39;: 2, \u0026#39;奶\u0026#39;: 5, \u0026#39;找\u0026#39;: 1, \u0026#39;刘\u0026#39;: 1, \u0026#39;买\u0026#39;: 1} d.keys( ) d.values( )\nGet all keys and values separately.\nstudents = {201901: \u0026#39;Apple\u0026#39;, 201902: \u0026#39;Google\u0026#39;, 201903: \u0026#39;Tesla\u0026#39;} print(list(students.keys())) print(list(students.values())) # [201901, 201902, 201903] # [\u0026#39;Apple\u0026#39;, \u0026#39;Google\u0026#39;, \u0026#39;Tesla\u0026#39;] d.items( )\nprint(list(students.items())) # [(201901, \u0026#39;Apple\u0026#39;), (201902, \u0026#39;Google\u0026#39;), (201903, \u0026#39;Tesla\u0026#39;)] for k, v in students.items(): # Unpack print(k, v) # 201901 Apple # 201902 Google # 201903 Tesla Set # Set Expression # A collection of unordered elements that are mutually exclusive, which can be used for deduplication Elements must be immutable types: numbers, strings, or tuples, which can be considered as the keys of dictionaries Can be considered as a dictionary without values or values as None Set Operations # S \u0026amp; T Returns a new set, including elements that are in both sets S and T S | T Returns a new set, including all elements in sets S and T S ^ T Returns a new set, including non-common elements in sets S and T S - T Returns a new set, including elements in set S but not in set T Set Methods # Add element S.add(x) Remove element S.remove(x) Set length len(S) Set traversal —— Use for loop for star in stars: print(star) "},{"id":45,"href":"/posts/python-basic-data-type/","title":"Python Basic Data Type","section":"Blog","content":"This article will explore data types and common methods for them in Python. The content of this review is as follows: number type, string type, boolean type and type conversion.\nCH2 Basic Data Type # 1.Number Type # Basic Type # Integer Type # Default is decimal Binary: 0b, Octal: 0o, Hexadecimal: 0x a = bin(16) # Binary b = oct(16) # Octal c = hex(16) # Hexadecimal print(a, b, c) # 0b10000 0o20 0x10 # Attention: str type Convert other base to decimal d = int(a, 2) # Binary e = int(b, 8) # Octal f = int(c, 16) # Hexadecimal print(d, e, f) # 16 16 16 Float Type # Floating point number (0.1+0.2) == 0.3 # 0.30000000000000004 # False Computer uses binary to represent floating point number\nReason: Some decimal numbers cannot be represented by binary\nBinary Decimal 0.00011001100110011001 0.09999942779541016\n0.0011001100110011 0.1999969482421875\n0.01001100110011001 0.29999542236328125\n0.01100110011001101 0.40000152587890625\n0.1 === $1*2^{-1}$ === 0.5\nUsually not affect calculation precision Can use rounding to solve: round(parameter, certain number of decimal places) a = 3*0.1 print(a) # 0.30000000000000004 b = round(a, 1) print(b) # 0.3 b == 0.3 # True Complex Type # # Capital J or lowercase j 3+4j 2+5J # When the imaginary part coefficient is 1, it needs to be explicitly written 2+1j Operations # Addition, subtraction, multiplication, division Negation - Exponentiation ** Integer quotient //: x/y floor division Modulo operation %: x/y calculate remainder Integer and floating point number operations result in floating point numbers\nThe result of division is a floating point number 8/4 = 2.0\nOperations Functions # Calculate absolute value abs() abs(3+4j) # Calculate the modulus of the complex number a+bj (a^2+b^2)=0.5 # 5.0 Power pow(x,n) is equivalent to x**n Power modulo pow(x,n,m) is equivalent to x**n % m Rounding round(x,n) n is the number of decimal places, default is no n, rounding to integer Integer quotient and modulo operation divmod(x,y) is equivalent to returning a tuple (x//y,x % y) Sequence maximum/minimum value max( ) min( ) a = [3, 2, 3, 6, 9, 4, 5] print(\u0026#34;max:\u0026#34;, max(a)) print(\u0026#34;min:\u0026#34;, min(a)) # max: 9 # min: 2 Sum sum(x) Note: sum needs to fill in a sequence data sum((1, 2, 3, 4, 5)) # 15 Use scientific calculation library math\\scipy\\numpy import math # Import library print(math.exp(1)) # Exponential operation e^x print(math.log2(2)) # Logarithmic operation print(math.sqrt(4)) # Square root operation Equivalent to 4^0.5 import numpy as np a = [1, 2, 3, 4, 5] print(np.mean(a)) # Calculate mean print(np.median(a)) # Calculate median print(np.std(a)) # Calculate standard deviation 2.String Type # String Expression # Use \u0026quot;\u0026quot; or '' to enclose any character, refer to the situation where the string contains double quotes or single quotes. If you only want to use one, you can use the escape character \\ to achieve it. # print(\u0026#34;\u0026#34;Python\u0026#34; is good\u0026#34;) # False print(\u0026#34;\\\u0026#34;Python\\\u0026#34; is good\u0026#34;) # \\ character # \u0026#34;Python\u0026#34; is good The escape character can be used to continue inputting on a new line s = \u0026#34;py\\ thon\u0026#34; print(s) # python String Properties # String Index (Single Character) # Variable name[position number]\nPositive index – starts from 0 and increases, spaces are also a position Negative index – starts from -1 and decreases Position number cannot exceed the length of the string s = \u0026#34;My name is Peppa Pig\u0026#34; print(s[0]) # M print(s[2]) # print(s[-1]) # g print(s[-3]) # P String Slicing (Multiple Characters) # Variable name[start position:end position:slice interval]\nThe slice interval defaults to 1, which can be omitted Range: front closed and back open s = \u0026#34;Python\u0026#34; print(s[0:3:1]) == print(s[0:3]) # Pyt print(s[0:3:2]) # Pt The starting position is 0, which can be omitted The end position is omitted, which means it can be taken to the last character s = \u0026#34;Python\u0026#34; print(s[0:6]) == print(s[:6]) == print(s[:]) # Python Reverse Slicing\nThe starting position is -1, which can be omitted The end position is omitted, which means it can be taken to the first character The key point is -1, which means the previous position is -1 larger than the next position s = \u0026#34;123456789\u0026#34; print(s[-1:-10:-1]) # 987654321 print(s[:-10:-1]) # 987654321 print(s[::-1]) # 987654321 String Operators # String Concatenation # String1 + String2 String Multiplication # String * n c = a+b print(c*3) print(3*c) Member Operation # Subset in full set: Any continuous slice is a subset of the original string folk_singers = \u0026#34;Peter, Paul and Mary\u0026#34; \u0026#34;Peter\u0026#34; in folk_singers # True Traverse string characters: for character in string for s in \u0026#34;Python\u0026#34;: print(s) # P # y # t # h # o # n String Processing Functions # String Length # Number of characters len(string) Character Encoding # Convert Chinese characters, English letters, numbers, special characters, etc. to computer-recognizable binary numbers\nEach single character corresponds to a unique, non-repeating binary code Python uses Unicode encoding ord(character)：Convert character to Unicode code\nprint(ord(\u0026#34;1\u0026#34;)) # 49 print(ord(\u0026#34;a\u0026#34;)) # 97 chr(Unicode code)：Convert Unicode code to character\nprint(chr(1010)) # ϲ print(chr(23456)) # 宠 String Processing Methods # Return a list, the original string remains unchanged\nString Splitting .split(\u0026quot; \u0026quot;) # languages = \u0026#34;Python C C++ Java PHP R\u0026#34; languages_list = languages.split(\u0026#34; \u0026#34;)# The parameter in the parentheses is the mark we want to split the target string print(languages_list) print(languages_list) # [\u0026#39;Python\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;C++\u0026#39;, \u0026#39;Java\u0026#39;, \u0026#39;PHP\u0026#39;, \u0026#39;R\u0026#39;] print(languages) # Python C C++ Java PHP R String Aggregation \u0026quot;,\u0026quot;.join(\u0026quot; \u0026quot;) # Iterable type, such as string, list s = \u0026#34;12345\u0026#34; s_join = \u0026#34;,\u0026#34;.join(s) # Take out each element of the iterable object, add the aggregation character between the two s_join # \u0026#39;1,2,3,4,5\u0026#39; The elements of the sequence type must be of character type # s = [1, 2, 3, 4, 5] cannot be used for aggregation s = [\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;3\u0026#34;, \u0026#34;4\u0026#34;, \u0026#34;5\u0026#34;] \u0026#34;*\u0026#34;.join(s) # \u0026#39;1*2*3*4*5\u0026#39; Delete specific characters at both ends .strip(\u0026quot;delete character\u0026quot;) # strip searches from both sides, deletes the specified character when encountered, stops searching when a non-specified character is encountered There are also left deletion lstrip and right deletion rstrip s = \u0026#34; I have many blanks \u0026#34; print(s.strip(\u0026#34; \u0026#34;)) # Search from both sides, delete spaces after encountering the specified character, then stop # I have many blanks print(s.lstrip(\u0026#34; \u0026#34;)) # I have many blanks print(s.rstrip(\u0026#34; \u0026#34;)) # I have many blanks String Replacement .replace(\u0026quot;replaced\u0026quot;, \u0026quot;replaced with\u0026quot;) # s = \u0026#34;Python is coming\u0026#34; s1 = s.replace(\u0026#34;Python\u0026#34;,\u0026#34;Py\u0026#34;) print(s1) # Py is coming String Count .count(\u0026quot;sample string\u0026quot;) # s = \u0026#34;Python is an excellent language\u0026#34; print(\u0026#34;an:\u0026#34;, s.count(\u0026#34;an\u0026#34;)) # an: 2 String Letter Case and First Letter Capital .upper() .lower() .title() # s = \u0026#34;Python\u0026#34; print(s.upper()) # PYTHON print(s.lower()) # python print(s.title()) # Python 3.Boolean Type # Logical Operation Results # any() Data has a non-zero value is True all() Data has a zero value is False (all non-zero) print(any([False,1,0,None])) # 0 False None are all zero # True print(all([False,1,0,None])) # False Mask for numpy array # import numpy as np x = np.array([[1, 3, 2, 5, 7]]) # Define numpy array print(x \u0026gt; 3) # [[False False False True True]] x[x \u0026gt; 3] # array([5, 7]) 4.Type Identification and Type Conversion # Type Identification # type()\nage = 20 name = \u0026#34;Ada\u0026#34; print(type(age)) # \u0026lt;class \u0026#39;int\u0026#39;\u0026gt; print(type(name)) # \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; isinstance(variable, type) Recognize inheritance\nThe variable type is a subtype of the type, then it is true, otherwise it is false print(isinstance(age, int)) # Recognize inheritance, here int is equivalent to a class # True print(isinstance(age, object)) # object is the ancestor of all classes # True String Check Methods # string.isdigit() Character is only composed of numbers string.isalpha() Character is only composed of letters string.isalnum() Character is only composed of numbers and letters Type Conversion # Number type to string str(number type) String composed of only numbers to number int() float() eval() s1 = \u0026#34;20\u0026#34; int(s1) # 20 s2 = \u0026#34;10.1\u0026#34; # int(s2) will error float(s1) # 20.0 eval(s2) # 10.1 "},{"id":46,"href":"/posts/python-basic-syntax-elements/","title":"Python Basic Syntax Elements","section":"Blog","content":"Sometimes you may forget about the basic of Python, so let us take a look at the summary of Python basic summary. The content of this review is as follows: data type, variable, control flow, input \u0026amp; output, PEP8 format.\nCH1 Basic Syntax Elements # 1 Data Type # Basic Type: Number, String, Boolean # Number Type\nint float complex (a+bj) String Type\nstr: use \u0026quot; \u0026quot; or ' ' Boolean Type\nbool y = 2 \u0026lt; 1 y # False Composite Type: List, Tuple, Dictionary, Set # List Type, ordered\na = [1, 2, 3, 4, 5] a[4] # 5 Tuple Type, ordered, elements are not modifiable\nb = (1, 2, 3, 4, 5) b[0] # 1 Dictionary Type, key-value mapping, unordered\nstudent = {201901: \u0026#34;john\u0026#34;, 201902: \u0026#34;howe\u0026#34;, 201903: \u0026#34;timerring\u0026#34;} student[201902] # \u0026#39;howe\u0026#39; Set Type, a collection of unique elements, unordered\ns = {\u0026#34;john\u0026#34;, \u0026#34;howe\u0026#34;, \u0026#34;timerring\u0026#34;, \u0026#34;john\u0026#34;} s # {\u0026#39;john\u0026#39;, \u0026#39;howe\u0026#39;, \u0026#39;timerring\u0026#39;} 2 Variable # Variable Naming # What can be used as variable names?\nuppercase letters, lowercase letters, numbers, underscores, and Chinese characters. strictly case-sensitive What is not allowed?\nthe first character cannot be a number there cannot be spaces in the middle of the variable name cannot be the same as the 33 Python reserved words Variable Name Definition Techniques\nunderscore (variable and function name) variable name consists of multiple words: use _ to connect multiple words age_of_students = [17, 18, 19] Camel Case (class name) variable name consists of multiple words: capitalize the first letter of each word AgeOfStudents Constant (e.g. $\\pi$, e) variable name all letters are uppercase MAX_ITERATION = 1000 Variable Assignment # x, y = 1, 2 # separated by \u0026#34;,\u0026#34; print(x, y) x, y = y, x print(x, y) 3 Control Flow # Loop (for) # res = 0 for i in [1,2,3,4,5]: res += i res # 15 Loop (while) # i = 1 res = 0 while i \u0026lt;= 5: res += i i += 1 res # 15 Branch (if) # if condition: execute statement else: execute statement 4 Input \u0026amp; Output # Data Input # External File Import\nfrom local disk, network, etc. See File, Exception, and Module. Dynamic Interactive Input\nx = input(\u0026#34;please input: \u0026#34;) type(x) # str, so the addition is string concatenation Use eval() to remove the quotes\nx = eval(input(\u0026#34;please input: \u0026#34;)) type(x) # int Data Output # Print # Each print() defaults to a newline\nprint(\u0026#34;timerring\u0026#34;) # timerring print(1) # 1 Line Break Control end= # print(123,end=\u0026#34; \u0026#34;)# also can customize the end content print(456) # 123 456 Combined Output # PI = 3.1415926 E = 2.71828 print(\u0026#34;PI = \u0026#34;, PI, \u0026#34;E = \u0026#34;, E) Formatting Output # # one-to-one correspondence print(\u0026#34;PI = {0},E = {1}\u0026#34;.format(PI, E)) # PI = 3.1415926,E = 2.71828 print(\u0026#34;PI = {0},E = {0}\u0026#34;.format(PI, E)) # PI = 3.1415926,E = 3.1415926 # default order print(\u0026#34;PI = {},E = {}\u0026#34;.format(PI, E)) # PI = 3.1415926,E = 2.71828 Decorative Output # Padding Output\nprint(\u0026#34;{0:_^20}\u0026#34;.format(PI)) # 0 is the variable PI, : is the modifier output, _ is the modifier character, ^ is centered, 20 is the output width # ____3.1415926_____ padding print(\u0026#34;{0:*\u0026lt;30}\u0026#34;.format(PI)) # \u0026lt; is left-aligned # 3.1415926********************* Thousands Separator\nprint(\u0026#34;{0:,}\u0026#34;.format(10000000)) # 10,000,000 Simplified Floating Point Output # keep 2 decimal places print(\u0026#34;{0:.2f}\u0026#34;.format(PI)) # 3.14 Output as a percentage print(\u0026#34;{0:.1%}\u0026#34;.format(0.818727)) # 81.9% Scientific Notation Output print(\u0026#34;{0:.2e}\u0026#34;.format(0.818727)) # 8.19e-01 Integer Base Conversion Output\nDecimal to Binary, Unicode, Decimal, Octal, Hexadecimal \u0026#34;Binary {0:b}, Unicode {0:c}, Decimal {0:d}, Octal {0:o}, Hexadecimal {0:x}\u0026#34;.format(450) # Binary 11100010, Unicode \\u1b6, Decimal 450, Octal 702, Hexadecimal 1c2 Summary # Formatting Output: \u0026quot;character{0:modifier}character{1:modifier}character\u0026quot;.format(v0, v1)\nModifier Output: must be strictly in order.\n5 Program Format (PEP8 Format) # Line Maximum Length # All lines are limited to a maximum of 79 characters\nIndentation # Use indentation to represent the logical relationship between statements, indentation: 4 characters Use Spaces # Add a space on both sides of the binary operator Add spaces around different priority operators x = x*2 - 1 c = (a+b) * (a-b) Use spaces after commas Avoid Using Spaces # Do not add spaces around = when specifying keyword arguments or default parameter values def fun(n=1, m=2): print(n, m) Comments # Single-line comment # comment content\nMulti-line comment \u0026quot;\u0026quot;\u0026quot;comment content, can be split into multiple lines\u0026quot;\u0026quot;\u0026quot;\n"},{"id":47,"href":"/posts/deploy-github-pages-with-gpg-signing/","title":"Deploy Github Pages With GPG Signing","section":"Blog","content":"I have been busy migrating my blog this week. Coincidentally, I learned that there may be cases of commit forgery on GitHub. Therefore, for security reasons, I added a GPG signature. However, when deploying Hugo, I encountered many problems regarding whether GPG signatures can also be used. Fortunately, I finally solved them.\nIf you don\u0026rsquo;t know what GPG is, you can read GPG 101.\nHow to Deploy Github Pages With Gpg Signing and Verify # There are two main ways to deploy:\nPush all source files to GitHub directly, then use the relevant action to complete the entire deployment process. Isolate the blog source files from the built files, push the source files to the private repository of GitHub each time, and then set up the relevant workflow in the private repository to push to the public static repository. To ensure greater security, I chose the second method, deploying Hugo in the workflow of GitHub Pages, and using the actions-gh-pages action. However, due to various reasons, the author of this action does not want to add the GPG signature feature. Therefore, we have to solve the problem ourselves.\nImport GPG Key # First, I found a workflow for importing GPG keys on GitHub. After reading the documentation, my own workflow is as follows:\n- name: Import GPG key # import the gpg key to the github action uses: crazy-max/ghaction-import-gpg@v6 # repository https://github.com/crazy-max/ghaction-import-gpg with: # I use the subkey to sign the commit, if you use the primary key, you can refer to his repository docs. gpg_private_key: ${{ secrets.GPG_PRIVATE_KEY }} # the secret gpg subkey passphrase: ${{ secrets.PASSPHRASE }} # the passphrase of the gpg subkey git_user_signingkey: true git_commit_gpgsign: true fingerprint: ${{ secrets.FINGERPRINT }} # the fingerprint of the public subkey you use If you only use the primary secret key of GPG, you do not need to add the fingerprint, and I generated a dedicated subkey for signing for security reasons. Therefore, you need to specify the fingerprint of the public key of the subkey. Note that the fingerprint should be entered without spaces; otherwise, it will report an error 67108933 Not implemented \u0026lt;GPG Agent\u0026gt;. I added this note to the corresponding issue.\nDon\u0026rsquo;t forget to fill in the corresponding secret variables and values in the repository.\nDeploy # Since the author does not plan to add GPG signature, we need to clone the project and modify it ourselves. Usually, the -S option is used in the commit to specify the use of GPG signature. Therefore, I found the corresponding function in the commit and added the corresponding -S option.\nNote that the modified workflow you created cannot be used directly. The author\u0026rsquo;s instructions are as follows:\nThis action and my other actions do not provide the branch execution. I add the lib/index.js for only each release commit. After releasing, I delete it.\nTherefore, we still need to publish a version ourselves. Run ./release.sh directly in the project, and publish the version you wrote. After that, you can reference your version in the workflow, and my workflow is as follows:\n- name: Deploy Web uses: timerring/actions-gh-pages@v5.0.0 # this is adjusted action from peaceiris/actions-gh-pages, you can use it directly. with: personal_token: ${{ secrets.PERSONAL_TOKEN }} # the personal token of the github action external_repository: your_username/your_repository # your target repository publish_branch: main # the branch you want to deploy publish_dir: ./public # the directory you want to deploy user_name: ${{ secrets.USER_NAME }} # the name of the github action user_email: ${{ secrets.USER_EMAIL }} # the email of the github action # ATTENTION: please add your github verified email commit_message: ${{ github.event.head_commit.message }} Note that please ensure that you add the email verified by GitHub; otherwise, the default parameter ${process.env.GITHUB_ACTOR}@users.noreply.github.com will only generate USERNAME@users.noreply.github.com, not ID+USERNAME@users.noreply.github.com. This is a historical issue with GitHub, details can be found here. However, your private key does not contain this UUID, so it cannot be verified by GPG. (Even if you add this UID to the keys, since the user email has not been verified by GitHub, it will only display unverified in the end.)\nIn short, if your GitHub account was created after July 18, 2017, then your GitHub email address is ID+USERNAME@users.noreply.github.com, not the default USERNAME@users.noreply.github.com. In this case, you need to specify the user_email parameter and fill in the email address you have verified.\nFinally, after pushing to the blogsource repository, the workflow will automatically deploy to the blog repository, and the commit will be signed with GPG and display verified!\nYou can check my result here, every commit pushed from the blogsource repository will be signed with GPG and display verified.\nAppendix # If you also need my hugo deployment method, you can directly use the action version I modified and released, repository address timerring/actions-gh-pages, refer to my complete workflow yaml, and don\u0026rsquo;t forget to fill in the corresponding secret variables and values:\nname: deploy on: push: branches: - main workflow_dispatch: jobs: build: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v3 with: submodules: true fetch-depth: 0 ref: main - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#34;0.108.0\u0026#34; extended: true - name: Build Web run: hugo --minify - name: Import GPG key # import the gpg key to the github action uses: crazy-max/ghaction-import-gpg@v6 # repository https://github.com/crazy-max/ghaction-import-gpg with: # I use the subkey to sign the commit, if you use the primary key, you can refer to his repository docs. gpg_private_key: ${{ secrets.GPG_PRIVATE_KEY }} # the secret gpg subkey passphrase: ${{ secrets.PASSPHRASE }} # the passphrase of the gpg subkey git_user_signingkey: true git_commit_gpgsign: true fingerprint: ${{ secrets.FINGERPRINT }} # the fingerprint of the public subkey you use - name: Deploy Web uses: timerring/actions-gh-pages@v5.0.0 # this is adjusted action from peaceiris/actions-gh-pages, you can use it directly. with: personal_token: ${{ secrets.PERSONAL_TOKEN }} # the personal token of the github action external_repository: your_username/your_repository # your target repository publish_branch: main # the branch you want to deploy publish_dir: ./public # the directory you want to deploy user_name: ${{ secrets.USER_NAME }} # the name of the github action user_email: ${{ secrets.USER_EMAIL }} # the email of the github action # ATTENTION: please add your github verified email commit_message: ${{ github.event.head_commit.message }} "},{"id":48,"href":"/posts/gpg-101/","title":"GPG 101","section":"Blog","content":" GnuPG is a complete and free implementation of the OpenPGP standard as defined by RFC4880(also known as PGP). GnuPG allows you to encrypt and sign your data and communications; it features a versatile key management system, along with access modules for all kinds of public key directories. GnuPG, also known as GPG, is a command line tool with features for easy integration with other applications.\nThis article I will introduce the basic usage of GPG.\nWhat is GPG? # When it comes to GPG, you should know the PGP first. The PGP is a protocol that provides encryption and digital signature services(Pretty Good Privacy). The GPG is the implementation of the PGP protocol. PGP can support many kinds of encryption algorithms, such as AES, RSA, ECC, etc.\nAnd you can add it to your Github to make every commit to your repository is signed.\nDo not upload any message to the public key servers, it\u0026rsquo;s not secure! Do not do any operation related to public key servers! In this article, I will not mention any operation related to public key servers! The public key servers is a centralized service, and will not delete any messages even you have revoked the public key.\nSome abbreviations # Usage # A =\u0026gt; Authentication (eg. ssh) C =\u0026gt; Certify (Only the primary key have this capability) E =\u0026gt; Encrypt S =\u0026gt; Sign (eg. sign the commit) ? =\u0026gt; Unknown capability\nType # sec =\u0026gt; Secret Primary Key ssb =\u0026gt; Secret Subkey pub =\u0026gt; Public Primary Key sub =\u0026gt; Public Subkey\nFormat # armored: asc binary: pgp Fingerprint # Each key or subkey has a line of 10 groups of 4 characters. This is the SHA-1 hash of the entire key, which is 160 bits, 20 bytes, and is usually represented as 40 hexadecimal numbers. This fingerprint can be used to uniquely identify a key pair.\nKey ID # Format:\nLong: the last 16 characters of the fingerprint. Short: the last 8 characters of the fingerprint. UID # UID is the user id which contains the username, comment and email. Name (Comment) \u0026lt;Email\u0026gt;\nOne secret key can have multiple UIDs. UID is used for the whole keys not just for specific subkey. The uid can add easily, but the existing uid cannot be adjusted, only can be revoked. Validity # When import a key, it will default to [unknown]. You can check the fingerprint and the owner\u0026rsquo;s claim to verify the key.\nTrust network # Trust levels:\nultimate: Normally you should only ultimately trust your own keys. The root of the trust chain. full: Full trust the key, also contains the keys signed by this key. marginal: Trust the key, but not fully trust. If three people trust the key, then I trust it. never: Never trust the key along with the keys signed by this key. How to use it? # Installation # brew install gpg Generate Key # You have to do the following steps quickly. If the process is timeout, you will need to re-do the steps.\ngpg --full-generate-key # 1. Then select the key type, it\u0026#39;s fine by default. # 2. Then select the key expiration, I choose `3y`, because you can renew the key later, and that\u0026#39;s make sure you cn still control the key. Expired keys are only invalid for new encryption and signing. But you can still decrypt and verify the existing information, just will be marked as expired. # 3. Then enter you name(uid), I don\u0026#39;t recommend you to use your real name, you can instead of your username. # 4. Then enter your email, make sure to use the verified email in Github, it\u0026#39;s highly recommended to use the `no-reply` email provided by Github to avoid spam. # 5. Then enter the passphrase, it\u0026#39;s used to encrypt the key. # 6. After a rondom move, your key is generated. Generate subkey # It is recommended to generate a subkey for the key. And just use the primary key to sign the new subkeys. Each subkey has its own application scenario.\n# Enter the primary key interactive mode gpg --edit-key yourNameInprimaryKey(uid or keyid) gpg \u0026gt; addkey # Then the process is the same as the previous steps. This time I choose the RSA(sign only). # Btw, before generate the subkey, it will ask you to enter the passphrase of the primary key. # After the subkey is generated, don\u0026#39;t forget to save the key. gpg \u0026gt; save Genrate a revocation certificate # Imagine you forget the passphrase of the key, or you lose the control, you can use the revocation certificate to revoke the public key. If not, you will need to notify your friends that you don\u0026rsquo;t use the key anymore. That will be a big problem. Thus it is necessary to generate a revocation certificate.\ngpg --gen-revoke -ao revoke.pgp uid(or keyid) # Make choices based on your situation Then you will get a revoke.pgp file, you can use it to revoke the key.\nlist the keys # gpg --list-keys # list the public keys, you can also use `gpg -k` gpg --list-secret-keys # list the secret keys, you can also use `gpg -K` The common usage # # The most common usage! -k or -K gpg -K --with-fingerprint --with-subkey-fingerprint --keyid-format long Besides, there are some parameters that you may need to use:\n--with-fingerprint # print the fingerprint of the key --with-subkey-fingerprint # print the fingerprint of the subkey --with-sig-list # print the signature of keys Export the key # gpg -ao public-key.txt --export uid(or keyid) # export the public key # It is better to add your secure path before the secret-key, it will export to the machine directly. gpg -ao secret-key --export-secret-key primarykeyid! # export the primary secret key, remember to add the `!` to export the single key, or you will export the whole secret keys. gpg -ao sign-subkey --export-secret-subkeys subkeyid! # export the sign sub secret key.[S] gpg -ao encrypt-subkey --export-secret-subkeys encryptkeyid! # export the encrypt sub secret key.[E] Most usage # Besides, GPG private key exported as an ASCII armored version or its base64 encoding (often).\ngpg --export-secret-key --armor keyid \u0026gt; secret-key.asc Delete the key # After Export the keys, you can delete then from the machine.\ngpg --delete-secret-keys uid(or keyid) # delete the secret key gpg --delete-keys uid(or keyid) # delete the public key As we know, the keys is stored in the machine in plaintext, it will not delete the keys completely, you can use the wipe or other tools to assist. But there is still the risk of restoring the keys. If you really want to generate and delete the keys in the most secure way, you can try Tails(boum.org).\nImport the key # Strongly discourage any operation related to public key servers!\ngpg --import yourkeysfile(your secret key or others public key) # output # the `#` means the primary key is not imported, so it\u0026#39;s safe. # sec# rsa3072/keyid 2021-01-11 [SC] # ... # the `#` means the subkey is imported. # ssb # rsa3072/keyid 2021-01-11 [E] Sign and verify # # Sign # 1. generate the binary signature file gpg --sign input.txt # 2. generate the ASCII signature file gpg --clearsign input.txt # 3. generate the signature file and original file separately. gpg --armor --detach-sign input.txt # verify gpg --verify input.txt.asc input.txt Encrypt and decrypt # # encrypt # uid(or keyid) is the uid or keyid of the recipient which means you have to import the public key of the recipient in advance. gpg --encrypt --recipient uid(or keyid) input.txt --output output.txt # a simple version gpg -se -o encrypt.txt -r uid(or keyid) input.txt # decrypt gpg --decrypt encrypt.txt --output decrypt.txt Revoke # Even you have revoke the key, if there is still someone sent you message by the outdated public key, you can decrypt the message as well as the hacker can. This operation import the revocation certificate which will make the whole keys invalid. The revoked key is only invalid for new encryption and signing. But you can still decrypt and verify the existing information, but it will be marked as revoked.\nImagine the scenario, Alice\u0026rsquo;s secret key is leaked, she will send a key revocation certificate, but the distribution is not centralized, so she cannot make sure everyone has received the message. Besides, the key revocation certificate is need to be signed by the secret key of the Alice, so if the secret key is lost, she will not be able to revoke the key.\nSo once you have revoked the key, you should push the revoked public key to where you publish the key always, and notify your friends.\n# revoke the primary key # import the public key first gpg --import gpg-linus.asc # then import the revoke certificate which will make the public key invalid directly. gpg --import revoke # gpg -k to check the key is revoked. eg.[revoked: 2024-01-01] # revoke the subkey gpg --edit-key uid(or keyid) # then select the subkey you want to revoke gpg \u0026gt; list gpg \u0026gt; key 1 # 1 is the index of the subkey gpg \u0026gt; revoke gpg \u0026gt; save Config your git # Refer to Github docs and Github docs\nReference # Github docs GnuPG docs ulyc blog Ruanyifeng blog GaoWeiX blog "},{"id":49,"href":"/posts/housewarming-2024/","title":"Housewarming 2024","section":"Blog","content":"So after a long time, I decide to restart my blog program. In my daily development, I have a lot of thoughts and ideas, thus I will write some documents to record them. But I believe that sharing is the best way to learn. A specific example is cryptography, which means the closing source algorithm is never the safest. Only algorithms that have been vetted by the public are truly secure.\nRestart # The reason why I call it restart is that I have been using hexo for a long time, and I wrote the blogs via hexo from the time I entered the university. But there is a serious problem in hexo, which is the deploy speed is too slow. When I have a little blogs, it is not a big problem, but the number of blogs increases with time, the deploy time exceeds 2 minutes, which is unbearable for me and it is hard to check the blog before publishing. And that makes me don\u0026rsquo;t want to publish blogs again, and then I reduced the times of publishing blogs until stopped.\nAfter about 1 year, now I decide to restart it, I came across hugo by chance, and just as the documentation says:\nHugo is a static site generator written in Go, optimized for speed and designed for flexibility. With its advanced templating system and fast asset pipelines, Hugo renders a complete site in seconds, often less.\nSo I finally decide to migrated contents from hexo to hugo.\nMigration # The migration process is smooth. The content types are nearly uniform, so I just need to pay attention to the project structure. And reading the documentation, the hugo mainly contains these parts:\n. ├── archetypes # the template of creating new pages ├── assets # the static files ├── config.toml # the configuration of the blog ├── content # blogs ├── public # hugo build output ├── resources # some resources ├── static # the static files └── themes # the added themes will be here and the theme structure is similar to the main hugo The creation of the blog can be referred to the official documentation.\nThemes # The theme I choose is hugo-book, which is a theme for hugo. Simple and graceful.\nMathematics # For the mathematics, there are two ways in browser to render the formula, the Katex and MathJax. They are all open source javascript libraries. Since I knew the markdown syntax, I use the Typora which supports MathJax library. So I get used to its syntax which accounts for the reason why I choose it.\nDeployment # The deployment is done by github actions, and the workflow file is in the .github/workflows/hugo.yml. BTW, the most contents on the search engine are about the blog deployment in personal github pages, which is not suitable for me, cause the personal github pages is used to store personal homepage. Thus, I will use the private repository blogsource to store the source code and use the blog repository to host the blog.\nAbout the process, I found a toturial at random, you can refer to it. And for more infomation you can refer to the official documentation.\nThere are some key steps to note:\nMake sure the blog repository is public and the Settings -\u0026gt; Pages -\u0026gt; Build and deployment -\u0026gt; Source is set to Deploy from a branch, and the branch is main as well as /(root) is selected.\nGenerate a personal token in github and set it in the blogsource repository. This guarantees you have the permission to operate the target repository.\nSet a deploy.yml file in the .github/workflows directory of blog source code repository which is private. You can refer to this code name: deploy on: push: branches: - main workflow_dispatch: jobs: build: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v3 with: submodules: true fetch-depth: 0 ref: main - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#34;0.108.0\u0026#34; # your hugo version. extended: true - name: Build Web run: hugo --minify - name: Deploy Web uses: peaceiris/actions-gh-pages@v3 with: # Make sure the name is same as what you set in the repo. PERSONAL_TOKEN: ${{ secrets.PERSONAL_TOKEN }} # Set the token in the `blog` repository. EXTERNAL_REPOSITORY: timerring/blogs # Set the target repository. PUBLISH_BRANCH: main PUBLISH_DIR: ./public # Set the publish directory. commit_message: ${{ github.event.head_commit.message }} Add the publishDir = \u0026quot;public\u0026quot; in the config.toml file and set your baseURL according to your target.\nAfter setting these, you can try to push it as normal. You will only need to push the source code to the blogsource repository, and then the blog will be deployed to the blog repository automatically through github actions.\nFonts # From the process of configurate the fonts, I have learned a lot about the fonts.\nStyle # The Style of fonts mainly contains these:\nserif: the fonts with serifs always have a small line at the end of the character. Often used in formal occasions. sans-serif: the fonts focus on simplicity and clarity. Hence it is often used in the web. monospace: the fonts with the same width for all characters. Which is often used in the code display. Weight # Different weights of fonts have different names:\n100 - Thin(Hairline) 200 - Extra Light (Ultra Light) 300 - Light 400 - Regular (Normal,Book,Roman) 500 - Medium 600 - Semi Bold (Demi Bold) 700 - Bold 800 - Extra Bold (Ultra Bold) 900 - Black (Heavy) The Regular and Bold are the most commonly used weights.\nType # The type of fonts mainly contains these:\nttf: TrueType is a font format developed by Apple and Microsoft focusing on the display in the printer in the beginning. otf: OpenType is a extension of TrueType. woff: Web Open Font Format which is a font format compressed to improve the loading speed in web. woff2: Web Open Font Format 2.0. The mostly differ in regard to the compression algorithm. For the fonts part, I have use the production of JetBrains for many years, so I choose the JetBrainsMono-Bold font for the code display.\nAnd for the content part, I use the Helvetica Neue font which is a popular sans-serif font and it is suitable for the web content.\nExtension # In conclusion, I use these extensions in the blog:\ngiscus: the comment system. mathjax: the mathematics system. Google Custom Search Engine: the search system. "},{"id":50,"href":"/about/","title":"About","section":"timerring","content":" About # John Howe GPG Public: 0x26FEE0805E6C9F71 Key fingerprint = B613 FBAF 0822 BEBD FABF 8F65 26FE E080 5E6C 9F71 If you want to know what this blog is about, here are some representative posts:\nNetwork Real Computer Network | A Brief Introduction to DNS ｜ Introduction to the HTTP and HTTPS Protocol Develop CPU can only see the threads | Python series Audio and Video Video Technology 101 | Implement danmaku rendering algorithm from scratch Security The Overview of Security | GPG 101 Container Docker 101 | Docker Cheatsheet Coffee chat # blog topic and coffee chat buy me a coffee Where to find me # RSS: source Github: timerring Mainpage: timerring Email: me[at]timerring.com Footprints # Changelog # 2024-12-15 Transfer from hexo to hugo. 2025-01-03 Add many short codes. 2025-02-04 Make the blog more WordPressized, Refer to terrytao and karpathy. 2025-03-22 Change the font from HelveticaNeueCyr to Georgia, as well as the selection color. "},{"id":51,"href":"/friends/","title":"Friends","section":"timerring","content":" Friends links # Sukka Selfboot zu1k Spencer Woo zhile.io 52txr Colah Bojie Li Bimu Timothy immmmm Harry Chen Qcrao Liam ishell dewx Eryajf Darmau gaoweix limbopro Changqian Yu ice1000 lsvih zhheo jiejoe shuyuej likun only.rs aigonna bulianglin greatdk idoubi Jianlin Su axiaoxin theowenyoung kyth zyzhang naiba lixueduan gatesnotes "}]