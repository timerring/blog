<!doctype html><html lang=en dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="This is a lecture note of Hidden Markov Models."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta name=keywords content="HIDDEN MARKOV MODEL (HMM),WEATHER FORECASTING,WHAT IS AN HMM?,HMM FORMALISM,INFERENCE IN AN HMM,EXAMPLE:SPEECH RECOGNITION,EXAMPLE:WORD RECOGNITION,EXAMPLE HMM: WEATHER FORECASTING,TRELLIS REPRESENTATION OF AN HMM,HMM PROBLEM 1 Evaluation,Example,THE FORWARD ALGORITHM,HMM PROBLEM 2 Decoding,THE VITERBI ALGORITHM,VITERBI IN PSEUDOCODE,HOMEWORK! Sunny or Rainy?,WHY PROBLEM 2?,HMM PROBLEM 3 Learning,FORWARD-BACKWARD,BAUM-WELCH (EM),HMM CAVEATS,BEAM SEARCH,APPLICATIONS FOR HMMS,PREDICT STOCK - MARKET BEHAVIOR WITH MARKOV CHAINS AND PYTHON"><meta property="og:url" content="https://blog.timerring.com/posts/hidden-markov-models/"><meta property="og:site_name" content="timerring"><meta property="og:title" content="Hidden Markov Models"><meta property="og:description" content="This is a lecture note of Hidden Markov Models."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-21T14:49:15+08:00"><meta property="article:tag" content="Random Process"><meta property="article:tag" content="Hidden Markov Models"><meta property="article:tag" content="Math"><meta itemprop=name content="Hidden Markov Models"><meta itemprop=description content="This is a lecture note of Hidden Markov Models."><meta itemprop=datePublished content="2025-04-21T14:49:15+08:00"><meta itemprop=dateModified content="2025-04-21T14:49:15+08:00"><meta itemprop=wordCount content="1631"><meta itemprop=image content="https://blog.timerring.com/favicon.png"><meta itemprop=keywords content="Random Process,Hidden Markov Models,Math"><meta name=twitter:image content="https://blog.timerring.com/favicon.png"><meta name=twitter:card content="summary"><meta name=twitter:title content="Hidden Markov Models"><meta name=twitter:description content="This is a lecture note of Hidden Markov Models."><meta name=twitter:site content="@imjohnhowe"><link rel=canonical href=https://blog.timerring.com/posts/hidden-markov-models/><title>Hidden Markov Models | timerring</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.d7a88d77202b21242efd3641ee0ccf6d2b317c3a57bcd08a48b51662fd988265.css integrity="sha256-16iNdyArISQu/TZB7gzPbSsxfDpXvNCKSLUWYv2YgmU=" crossorigin=anonymous><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.558322723417765e75405c4ee0695a6a2cc1dda27e366ff09c4bde2f67a85793.js integrity="sha256-VYMicjQXdl51QFxO4GlaaizB3aJ+Nm/wnEveL2eoV5M=" crossorigin=anonymous></script><script defer src=/live-photo.min.efe7b2a516404d0718a4c6f90d2eb2f9c0bd8c7bddd382958716bcb24b660970.js integrity="sha256-7+eypRZATQcYpMb5DS6y+cC9jHvd04KVhxa8sktmCXA=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script defer>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1}]})})</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1347694832132708" crossorigin=anonymous></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/favicon.png alt=Logo><span>timerring</span></a></h2><hr><p>Rhythmic trend.</p><div class=book-search><div class=gcse-search></div></div><script async src="https://cse.google.com/cse.js?cx=40782701766144f96"></script><ul><li><a href=/>Dashboard</a></li><li><a href=/posts/>Blogs</a></li><li><a href=/categories/weekly/>Newsletter</a></li><li><a href=/index.xml>RSS</a></li><li><a href=/friends/>Friends</a></li><li><a href=/about/>About</a></li></ul><div style="margin:20px 0;text-align:center"><ins class=adsbygoogle style=display:block data-ad-format=fluid data-ad-layout-key=-c2+7a+0+1 data-ad-client=ca-pub-1347694832132708 data-ad-slot=5778132754></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><script>function goToRandomPost(){const e=["/posts/how-to-keep-your-mac-always-awake/?utm_source=random","/posts/the-way-to-get-cookie/?utm_source=random","/posts/how-to-choose-the-right-software-and-creative-license/?utm_source=random","/posts/0516-add-live-photo-effect-to-hugo/?utm_source=random","/posts/the-practice-of-resolving-domain/?utm_source=random","/posts/hidden-markov-models/?utm_source=random","/posts/markov-chains/?utm_source=random","/posts/markov-processes/?utm_source=random","/posts/0418-find-the-formula-rendering-issue-of-hugo/?utm_source=random","/posts/multi-platform-builds-docker/?utm_source=random","/posts/0404-The-invitation-only-mechanism-from-clubhouse-to-manus/?utm_source=random","/posts/how-to-use-git-submodule/?utm_source=random","/posts/live-streaming-infra-and-protocols/?utm_source=random","/posts/implement-danmaku-rendering-algorithm-from-scratch/?utm_source=random","/posts/is-there-an-rss-renaissance-in-the-ai-era/?utm_source=random","/posts/stock-prices/?utm_source=random","/posts/possion-processes/?utm_source=random","/posts/docker-with-gpu/?utm_source=random","/posts/stochastic-processes/?utm_source=random","/posts/some-useful-tools/?utm_source=random","/posts/a-shopping-experience-in-pangdonglai-supermarket/?utm_source=random","/posts/langchain-and-rag-best-practices/?utm_source=random","/posts/random-walks/?utm_source=random","/posts/sequences-of-random-variables/?utm_source=random","/posts/parameter-estimation/?utm_source=random","/posts/two-random-variables/?utm_source=random","/posts/further-understanding-of-proc/?utm_source=random","/posts/the-iftop/?utm_source=random","/posts/some-useful-commands-to-share/?utm_source=random","/posts/random-variables/?utm_source=random","/posts/repeated-trials/?utm_source=random","/posts/probability-and-its-axioms/?utm_source=random","/posts/reflections-on-trending-topics/?utm_source=random","/posts/cpu-can-only-see-the-threads/?utm_source=random","/posts/go-common-test/?utm_source=random","/posts/go-concurrency-and-parallelism/?utm_source=random","/posts/go-cheatsheet/?utm_source=random","/posts/video-technology-101/?utm_source=random","/posts/the-main-kind-of-message-queue/?utm_source=random","/posts/the-method-to-manage-traffic/?utm_source=random","/posts/the-different-kind-of-api-design/?utm_source=random","/posts/the-encode-and-decode-in-python/?utm_source=random","/posts/about-the-systemd/?utm_source=random","/posts/the-tips-about-dockerfile/?utm_source=random","/posts/docker-cheatsheet/?utm_source=random","/posts/docker-101/?utm_source=random","/posts/the-instance-class-static-magic-method-in-python/?utm_source=random","/posts/the-review-and-plan-for-bilive/?utm_source=random","/posts/the-overview-of-security/?utm_source=random","/posts/how-to-publish-your-code-as-a-pip-module/?utm_source=random","/posts/some-good-things-to-share-about-the-packages/?utm_source=random","/posts/introduction-to-the-http-and-https-protocol/?utm_source=random","/posts/mail-service-and-protocol/?utm_source=random","/posts/understanding-clash-through-configuration/?utm_source=random","/posts/thinking-about-advertisement-from-an-open-source-perspective/?utm_source=random","/posts/a-brief-introduction-to-dns/?utm_source=random","/posts/real-computer-network/?utm_source=random","/posts/python-generator-iterator-and-decorator/?utm_source=random","/posts/python-underlying-mechanism/?utm_source=random","/posts/python-files-exceptions-and-modules/?utm_source=random","/posts/python-object-oriented-programming/?utm_source=random","/posts/python-cheatsheet/?utm_source=random","/posts/python-function-parameter/?utm_source=random","/posts/python-control-structure/?utm_source=random","/posts/python-composite-data-type/?utm_source=random","/posts/python-basic-data-type/?utm_source=random","/posts/python-basic-syntax-elements/?utm_source=random","/posts/deploy-github-pages-with-gpg-signing/?utm_source=random","/posts/gpg-101/?utm_source=random","/posts/housewarming-2024/?utm_source=random"],t=Math.floor(Math.random()*e.length);window.location.href=e[t]}</script><a class=random onclick=goToRandomPost()>Stroll</a></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>timerring</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#hidden-markov-model-hmm>HIDDEN MARKOV MODEL (HMM)</a><ul><li><a href=#weather-forecasting>WEATHER FORECASTING</a></li></ul></li><li><a href=#what-is-an-hmm>WHAT IS AN HMM?</a><ul><li><a href=#hmm-formalism>HMM FORMALISM</a></li><li><a href=#inference-in-an-hmm>INFERENCE IN AN HMM</a></li><li><a href=#examplespeech-recognition>EXAMPLE:SPEECH RECOGNITION</a></li><li><a href=#exampleword-recognition>EXAMPLE:WORD RECOGNITION</a></li><li><a href=#example-hmm-weather-forecasting>EXAMPLE HMM: WEATHER FORECASTING</a></li></ul></li><li><a href=#trellis-representation-of-an-hmm>TRELLIS REPRESENTATION OF AN HMM</a></li><li><a href=#hmm-problem-1-evaluation>HMM PROBLEM 1 Evaluation</a><ul><li><a href=#example>Example</a></li><li><a href=#the-forward-algorithm>THE FORWARD ALGORITHM</a></li></ul></li><li><a href=#hmm-problem-2-decoding>HMM PROBLEM 2 Decoding</a><ul><li><a href=#the-viterbi-algorithm>THE VITERBI ALGORITHM</a></li><li><a href=#viterbi-in-pseudocode>VITERBI IN PSEUDOCODE</a></li><li><a href=#homework-sunny-or-rainy>HOMEWORK! Sunny or Rainy?</a></li><li><a href=#why-problem-2>WHY PROBLEM 2?</a></li></ul></li><li><a href=#hmm-problem-3-learning>HMM PROBLEM 3 Learning</a><ul><li><a href=#forward-backward>FORWARD-BACKWARD</a></li><li><a href=#baum-welch-em>BAUM-WELCH (EM)</a></li></ul></li><li><a href=#hmm-caveats>HMM CAVEATS</a></li><li><a href=#beam-search>BEAM SEARCH</a></li><li><a href=#applications-for-hmms>APPLICATIONS FOR HMMS</a></li><li><a href=#predict-stock---market-behavior-with-markov-chains-and-python>PREDICT STOCK - MARKET BEHAVIOR WITH MARKOV CHAINS AND PYTHON</a></li></ul></nav><style>.book-toc .book-toc-content a{color:var(--body-font-color)}</style></aside></header><article class=markdown><h1>Hidden Markov Models</h1><h5>April 21, 2025
·
8 min read
<span id=busuanzi_container_page_pv>· Page View: <span id=busuanzi_value_page_pv></span></span></h5><div><a href=/categories/tutorial/ style=color:#000>Tutorial</a></div><div><a href=/tags/random-process/ style=color:#000>Random Process</a> <span style=color:#000>| </span><a href=/tags/hidden-markov-models/ style=color:#000>Hidden Markov Models</a> <span style=color:#000>| </span><a href=/tags/math/ style=color:#000>Math</a></div><blockquote class="book-hint info">If you have any questions, feel free to comment below.</blockquote><hr><p>This is a lecture note of Hidden Markov Models.</p><h2 id=hidden-markov-model-hmm>HIDDEN MARKOV MODEL (HMM)
<a class=anchor href=#hidden-markov-model-hmm>#</a></h2><p>Want to recognize patterns (e.g.sequence motifs), we have to learn from the data. Applications include gene prediction and stock price and trading.</p><p>Markov chain can predict the future given the present, dependent only on the previous (or current) state with transition probabilities.</p><ul><li>Many problems can&rsquo;t be solved by a Markov model due to unknown or hard-to-assess states. For example, in speech recognition.</li><li><strong>A hidden Markov model (HMM) is a Markov model where probabilities are based in part on hidden states</strong>. HMMs need to be trained, for example, in gene finding with nucleotide sequences or stock price prediction with known patterns.</li></ul><h3 id=weather-forecasting>WEATHER FORECASTING
<a class=anchor href=#weather-forecasting>#</a></h3><p>On any day, the weather can be rainy/snowy, cloudy or sunny. Given a probability matrix for weather transitions.</p><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-21-15-11-39.png alt></p><p>To compute a sequence, we multiply them together, so if today is sunny then the probability that the next two days will be sunny is $0.8\times0.8$, and the probability that the next two days will be cloudy is $0.1\times0.6\times0.6$.</p><h2 id=what-is-an-hmm>WHAT IS AN HMM?
<a class=anchor href=#what-is-an-hmm>#</a></h2><p>Graphical Model</p><ul><li>Green circles are hidden states, dependent only on the previous state.</li><li>Purple nodes are observed states, dependent only on their corresponding hidden state.</li><li>Arrows indicate probabilistic dependencies between states.</li></ul><h3 id=hmm-formalism>HMM FORMALISM
<a class=anchor href=#hmm-formalism>#</a></h3><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-21-15-21-45.png alt></p><ul><li>An HMM is represented by ${S,K,\Pi,A,B}$:<ul><li>$S={s_{1}&mldr; s_{N}}$ are the values for the hidden states.</li><li>$K={k_{1}&mldr; k_{M}}$ are the values for the observations.</li><li>$\Pi={\pi_{1}}$ are the initial state probabilities.</li><li>$A={a_{ij}}$ are the state transition probabilities.</li><li>$B={b_{ik}}$ are the observation state probabilities (<strong>output probabilities</strong>).</li></ul></li></ul><h3 id=inference-in-an-hmm>INFERENCE IN AN HMM
<a class=anchor href=#inference-in-an-hmm>#</a></h3><ul><li>Problem 1: Evaluation - Compute the probability of a given observation sequence.</li><li>Problem 2: Decoding - Given an observation sequence, compute the most likely hidden state sequence.</li><li>Problem 3: Learning - Given an observation sequence and set of possible models, which model most closely fits the data?</li></ul><h3 id=examplespeech-recognition>EXAMPLE:SPEECH RECOGNITION
<a class=anchor href=#examplespeech-recognition>#</a></h3><ul><li>We have acoustic signal observations, but the intention (phonemes/words) that created the signal is hidden.</li><li>The goal is to identify the actual utterance by adding hidden states and appropriate probabilities for transitions.</li><li>The observables are not states in our network, but transition links.</li></ul><h3 id=exampleword-recognition>EXAMPLE:WORD RECOGNITION
<a class=anchor href=#exampleword-recognition>#</a></h3><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-21-15-27-40.png alt></p><h3 id=example-hmm-weather-forecasting>EXAMPLE HMM: WEATHER FORECASTING
<a class=anchor href=#example-hmm-weather-forecasting>#</a></h3><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-21-15-28-31.png alt></p><h2 id=trellis-representation-of-an-hmm>TRELLIS REPRESENTATION OF AN HMM
<a class=anchor href=#trellis-representation-of-an-hmm>#</a></h2><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-21-15-29-30.png alt></p><h2 id=hmm-problem-1-evaluation>HMM PROBLEM 1 Evaluation
<a class=anchor href=#hmm-problem-1-evaluation>#</a></h2><p>Given an HMM(hidden markov model sequence) and an output sequence(observation sequence), compute the probability of generating that particular output sequence.</p>$$
p_{s1} * b_{s1}(O_1) * a_{s1s2} * b_{s2}(O_2) * a_{s2s3} * b_{s3}(O_3) * ... * a_{s_{n - 1}s_n} * b_{s_n}(O_n)
$$<ul><li>p is the initial state probability.</li><li>$a$ is the state transition probability.</li><li>$b$ is the output probability.</li></ul><h3 id=example>Example
<a class=anchor href=#example>#</a></h3><p>We have:</p><ul><li>3 time units,t1,t2,t3</li><li>each has 2 states, s1,s2<ul><li>p(s1 at t1)=.8</li><li>p(s2 at t1)=.2</li><li>Pi = [0.8 0.2]</li></ul></li><li>3 possible outputs A, B, C</li></ul><p>Our transition probabilities a are:</p><ul><li>p(s1,s1)=.7</li><li>p(s1,s2)=.3</li><li>p(s2,s2)=.6</li><li>p(s2,s1)=.4</li><li>${A} = \begin{bmatrix} 0.7 & 0.3 \ 0.4 & 0.6 \end{bmatrix}$</li></ul><p>Our output probabilities b are:</p><ul><li>p(A|s1)=.5</li><li>p(B|s1)=.4</li><li>p(C|s1)=.1</li><li>p(A|s2)=.7</li><li>p(B|s2)=.3</li><li>p(C|s2)=.0</li><li>${B} = \begin{bmatrix} 0.5 & 0.4 & 0.1 \ 0.7 & 0.3 & 0 \end{bmatrix}$</li></ul><p>What is the probability of generating A, B, C?
<img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-21-15-38-02.png alt></p><p>We want to compute the probability of the output sequence A,B,C</p><p>MORE EFFICIENT SOLUTION</p><p>When we compute s2-s2-s2, we had already computed s1-s2-s2, so the last half of the computation was already done.</p><p>By using dynamic programming, we can reduce the number of computations. We use a dynamic programming algorithm called the <strong>Forward algorithm</strong>.</p><h3 id=the-forward-algorithm>THE FORWARD ALGORITHM
<a class=anchor href=#the-forward-algorithm>#</a></h3><ul><li>Initialization step: $\alpha_{1}(i)=\pi_{i}^{*}b_{i}(O_{1})$ for all states $i$, which is the probabilities of starting at each initial state i at t1.</li><li>Main step (recursive): $\alpha_{t + 1}(j)=[\sum\alpha_{t}(i)*a_{ij}]*b_{j}(O_{t+1})$ for all states $j$ at time $t+1$.<ul><li>$\alpha_{t}(i)$ is the previous probability of being in state $i$ at time $t$.</li><li>$a_{ij}$ is the probability of transitioning from state $i$ to state $j$.</li><li>$b_{j}(O_{t+1})$ is the probability of observing the output $O_{t+1}$ given that we are in state $j$.</li></ul></li><li>Final step: Sum up the probabilities of ending in each of the states at time $n$. $\sum_{i}\alpha_{n}(i)$</li></ul><h2 id=hmm-problem-2-decoding>HMM PROBLEM 2 Decoding
<a class=anchor href=#hmm-problem-2-decoding>#</a></h2><p>Given a sequence of observations, compute the optimal sequence of state transitions that would cause those observations. Alternatively, we could say that <strong>the optimal sequence best explains the observations</strong>.</p><h3 id=the-viterbi-algorithm>THE VITERBI ALGORITHM
<a class=anchor href=#the-viterbi-algorithm>#</a></h3><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-21-15-55-39.png alt></p><ul><li>Use recursion and dynamic programming.</li><li>Initialization step:<ul><li>$\alpha_{i}(i)=\pi_{i}^{*}b_{i}(O_{1})$</li><li>$\psi_{1}(0)=0$ this array will represent the state that maximized our path leading to the prior state.</li></ul></li><li>Recursive step:<ul><li>$\delta_{t + 1}(j)=\max[\alpha_{i}(i)*a_{ij}]*b_{j}(O_{t+1})$, here we are maximizing the probability of the path leading to the current state.</li><li>$\psi_{t + 1}(j)=\arg\max[\alpha_{i}(i)*a_{ij}]$, here we are storing the state that maximizes the probability of the path leading to the current state.</li></ul></li><li>Termination step:<ul><li>$p^*=\max_{i}[\delta_{n}(i)]$，$n$ this the last step. Find the largest probability in the final time step.</li><li>When the optimal path probability $p^<em>$ and the terminating state $q^</em>$ are determined, the path backtracking is used to get the best path. Starting from $\psi[q^*]$, step by step backtracking until the first time step.</li></ul></li></ul><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-21-19-25-25.png alt></p><h3 id=viterbi-in-pseudocode>VITERBI IN PSEUDOCODE
<a class=anchor href=#viterbi-in-pseudocode>#</a></h3><div class=highlight><pre tabindex=0 style=-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:green;font-weight:700>def</span> <span style=color:#00f>forvard_viterb</span>(obs,states,start_p,transp,emit_p):
</span></span><span style=display:flex><span>    T <span style=color:#666>=</span> {}
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>for</span> state <span style=color:#a2f;font-weight:700>in</span> states:
</span></span><span style=display:flex><span>        T[state]<span style=color:#666>=</span>(start_p[state],[state],start_p[state])
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>for</span> output <span style=color:#a2f;font-weight:700>in</span> obs:
</span></span><span style=display:flex><span>        U <span style=color:#666>=</span> {}
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>for</span> next_state <span style=color:#a2f;font-weight:700>in</span> states:
</span></span><span style=display:flex><span>            total <span style=color:#666>=</span> <span style=color:#666>0</span>
</span></span><span style=display:flex><span>            argmax <span style=color:#666>=</span> <span style=color:green;font-weight:700>None</span>
</span></span><span style=display:flex><span>            valmax <span style=color:#666>=</span> <span style=color:#666>0</span>
</span></span><span style=display:flex><span>            <span style=color:green;font-weight:700>for</span> source_state <span style=color:#a2f;font-weight:700>in</span> states:
</span></span><span style=display:flex><span>                p <span style=color:#666>=</span> emit_p[source_state][output]
</span></span><span style=display:flex><span>                prob_t <span style=color:#666>=</span> p <span style=color:#666>*</span> T[source_state][<span style=color:#666>2</span>] <span style=color:#666>*</span> transp[source_state][next_state]
</span></span><span style=display:flex><span>                total <span style=color:#666>+=</span> prob_t
</span></span><span style=display:flex><span>                <span style=color:green;font-weight:700>if</span> T[source_state][<span style=color:#666>2</span>]<span style=color:#666>*</span>transp[source_state][next_state]<span style=color:#666>&gt;</span>valmax:
</span></span><span style=display:flex><span>                    argmax <span style=color:#666>=</span> T[source_state][<span style=color:#666>1</span>]<span style=color:#666>+</span>[next_state]
</span></span><span style=display:flex><span>                    valmax <span style=color:#666>=</span> T[source_state][<span style=color:#666>2</span>]<span style=color:#666>*</span>transp[source_state][next_state]
</span></span><span style=display:flex><span>            U[next_state]<span style=color:#666>=</span>(total,argmax,valmax)
</span></span><span style=display:flex><span>        T <span style=color:#666>=</span> U
</span></span><span style=display:flex><span>    total <span style=color:#666>=</span> <span style=color:#666>0</span>
</span></span><span style=display:flex><span>    valmax <span style=color:#666>=</span> <span style=color:#666>0</span>
</span></span><span style=display:flex><span>    argmax <span style=color:#666>=</span> <span style=color:green;font-weight:700>None</span>
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>for</span> state <span style=color:#a2f;font-weight:700>in</span> states:
</span></span><span style=display:flex><span>        prob <span style=color:#666>=</span> T[state][<span style=color:#666>0</span>]
</span></span><span style=display:flex><span>        total <span style=color:#666>+=</span> prob
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>if</span> T[state][<span style=color:#666>2</span>]<span style=color:#666>&gt;</span>valmax:
</span></span><span style=display:flex><span>            argmax <span style=color:#666>=</span> T[state][<span style=color:#666>1</span>]
</span></span><span style=display:flex><span>            valmax <span style=color:#666>=</span> T[state][<span style=color:#666>2</span>]
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>return</span> (total,argmax,valmax)
</span></span></code></pre></div><h3 id=homework-sunny-or-rainy>HOMEWORK! Sunny or Rainy?
<a class=anchor href=#homework-sunny-or-rainy>#</a></h3><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-21-19-26-56.png alt></p><h3 id=why-problem-2>WHY PROBLEM 2?
<a class=anchor href=#why-problem-2>#</a></h3><ul><li>Applications, The HMM and Viterbi give us the ability to generate the best explanation where the term best means the most likely sequence through all of the states:<ul><li>given a speech signal, what was uttered what phonemes or words were uttered)?</li><li>given a set of symptoms, what disease(s) is the patient suffering from?</li><li>given a misspelled word, which word was intended?</li><li>given a series of events, what caused them?</li></ul></li><li>HOW DO WE OBTAIN OUR PROBABILITIES?<ul><li>Need prior probabilities, transition probabilities, and output (emission) probabilities.</li><li>Can accumulate probabilities through counting, but it may be difficult to get accurate probabilities, especially output probabilities.</li><li>This leads to HMM problem 3.</li></ul></li></ul><h2 id=hmm-problem-3-learning>HMM PROBLEM 3 Learning
<a class=anchor href=#hmm-problem-3-learning>#</a></h2><ul><li>Given HMM and output sequence, update the various probabilities. Solved by the <strong>Baum-Welch algorithm</strong> (also known as the <strong>Estimation-Modification</strong> or EM algorithm), which uses the forward-backward algorithm.</li></ul><h3 id=forward-backward>FORWARD-BACKWARD
<a class=anchor href=#forward-backward>#</a></h3><ul><li>Compute forward probabilities as in the Forward algorithm.</li><li>Backward portion:<ul><li>Initialization step: $\beta_{t}(i)=1$, also start from time t, but as initial point.</li><li>$\beta_{t}(i)=1= P(O_{t+1}, O_{t+2}&mldr; O_{K} | \text{state is i at t})$, Joint probability of the partial observation sequence $O_{t+1}, O_{t+2}&mldr; O_{K}$ given that the hidden state at time k is state i. <strong>The Backward probability.</strong></li><li>Recursive step:
$$
\beta_{t}(i)=\sum a_{ij}*b_{j}(O_{t + 1})*\beta_{t+1}(j)
$$</li><li>$b_{j}(O_{t + 1})$ is the probability of state j given out $O_{t + 1}$.</li></ul></li></ul><h3 id=baum-welch-em>BAUM-WELCH (EM)
<a class=anchor href=#baum-welch-em>#</a></h3><p>Compute</p>$$
\xi_{t}(i,j)=\frac{\alpha_{t}(i)*a_{ij}*b_{j}(O_{t+1})*\beta_{t+1}(j)}{\text{denominator}}
$$<p>where the denominator normalizes the probabilities.</p><ul><li>forward $\alpha_{t}(i)$</li><li>backward $\beta_{t+1}(j)$</li><li>trans $a_{ij}$</li><li>output $O_{t+1}$</li><li>Compute $\gamma_{t}(i)=\sum\xi_{t}(i,j)$ for all $j$ at time $t$.<ul><li>This represents the <strong>expected number of times</strong> we are <strong>at state i at time t</strong></li><li>If we sum up $\gamma_{t}(i)$ for all times t, we have the number of expected times we are i state i.</li></ul></li><li>Re-estimation: Update probabilities based on the expected number of times in a state or transition. Start with estimated probabilities, use training examples to adjust, and continue until probabilities stabilize.</li><li><strong>Our observation probabilities</strong> $b_{i}(O_{t})$: p(observation i | state j )=(expected number of times we saw observation i in the test case / number of times we achieved state j )</li><li><strong>Our transition probabilities</strong> $a_{ij}$: p(state i | state j) (expected number of transitions from i to j / number of times we were in state j )</li><li>this is the prior probability: $\pi(\text{state } i)=\alpha_{1}(i) * \beta_{1}(i)/\sum_{i}[\alpha_{1}(i) * \beta_{1}(i)] \text{ for all states } i$</li></ul><p>Some advantages:</p><ul><li>Start with estimated probabilities (they don&rsquo;t even have to be very good)<ul><li>The better the initial probabilities, the more likely it will be that the algorithm will converge to a stable state <strong>quickly</strong>, the worse the initial probabilities, the longer it will take.</li></ul></li><li>Use training examples to adjust the probabilities</li><li>And continue until the probabilities stabilize: that is, between iterations of Baum-Welch, they do not change (or their change is less than a given error rate)<ul><li><strong>So HMMs can be said to learn the proper probabilities through training examples</strong></li></ul></li></ul><h2 id=hmm-caveats>HMM CAVEATS
<a class=anchor href=#hmm-caveats>#</a></h2><ul><li>States may not be independent.</li><li>If a probability (output or transition) is 0, it can cause issues that the path will never be selected. Replace 0 probabilities with a minimum value (e.g., 0.001).</li><li>Need to be mindful of overfitting and require a good training set. More training data doesn&rsquo;t always mean a better model.</li><li>Recall that the complexity for the forward algorithm is $O(T*N^T)$ where N is 30(hidden states 30 phonemes) and T is 100(utterance around 100 phonemes-20 words)!</li><li>High complexity in search, especially for long sequences. Can use beam search to reduce the number of possible paths searched.</li></ul><h2 id=beam-search>BEAM SEARCH
<a class=anchor href=#beam-search>#</a></h2><ul><li>A combination of heuristic and breadth-first search. Examine all next states, evaluate them (using $\alpha$ or $\beta$ in HMMs), and retain only some states (either <strong>top $k$</strong> or <strong>above a threshold value</strong>).</li></ul><h2 id=applications-for-hmms>APPLICATIONS FOR HMMS
<a class=anchor href=#applications-for-hmms>#</a></h2><ul><li>Speech recognition (1980s)</li><li>Hand - written character recognition</li><li>Natural language understanding<ul><li>Word sense disambiguation</li><li>Machine translation</li><li>Word matching (for misspelled words)</li><li>Semantic tagging of words</li></ul></li><li>Bioinformatics (protein structure predictions, gene analysis and sequencing predictions)</li><li>Market predictions</li><li>Diagnosis of mechanical systems</li></ul><h2 id=predict-stock---market-behavior-with-markov-chains-and-python>PREDICT STOCK - MARKET BEHAVIOR WITH MARKOV CHAINS AND PYTHON
<a class=anchor href=#predict-stock---market-behavior-with-markov-chains-and-python>#</a></h2><ul><li>Markov Chain offers a probabilistic approach to predict the likelihood of an event based on prior behavior.</li><li>Analyze market behavior, create transition matrices, bin values, combine event features, and create two Markov Chains (for volume jumps and drops) to predict future stock-market outcomes.</li></ul><h2>Related readings</h2><ul><li><a href="/posts/markov-chains/?utm_source=related">Markov Chains</a></li><li><a href="/posts/markov-processes/?utm_source=related">Markov Processes</a></li><li><a href="/posts/stock-prices/?utm_source=related">Stock Prices</a></li><li><a href="/posts/possion-processes/?utm_source=related">Possion Processes</a></li><li><a href="/posts/stochastic-processes/?utm_source=related">Stochastic Processes</a></li></ul><hr><div class=post-nav><a href="https://blog.timerring.com/posts/markov-chains/?utm_source=nav">&lt;&lt; prev | Markov Chains</a>
<a onclick=goToRandomPost() style=cursor:pointer>Continue strolling
</a><a href="https://blog.timerring.com/posts/the-practice-of-resolving-domain/?utm_source=nav">The Practice of... ｜ next >></a></div><hr>If you find this blog useful and want to support my blog, need my skill for something, or have a coffee chat with me, feel free to:<div class=support><a href=https://ko-fi.com/polls/What-is-the-custom-topic-for-the-next-month-C0C219LHQJ class=book-btn target=_blank>Subscribe to participate in blog topics and custom services
</a><a href=https://ko-fi.com/timerring class=book-btn target=_blank>Buy me a coffee</a></div><div style=margin-bottom:8px></div><script>function goToRandomPost(){const e=["/posts/how-to-keep-your-mac-always-awake/?utm_source=random","/posts/the-way-to-get-cookie/?utm_source=random","/posts/how-to-choose-the-right-software-and-creative-license/?utm_source=random","/posts/0516-add-live-photo-effect-to-hugo/?utm_source=random","/posts/the-practice-of-resolving-domain/?utm_source=random","/posts/hidden-markov-models/?utm_source=random","/posts/markov-chains/?utm_source=random","/posts/markov-processes/?utm_source=random","/posts/0418-find-the-formula-rendering-issue-of-hugo/?utm_source=random","/posts/multi-platform-builds-docker/?utm_source=random","/posts/0404-The-invitation-only-mechanism-from-clubhouse-to-manus/?utm_source=random","/posts/how-to-use-git-submodule/?utm_source=random","/posts/live-streaming-infra-and-protocols/?utm_source=random","/posts/implement-danmaku-rendering-algorithm-from-scratch/?utm_source=random","/posts/is-there-an-rss-renaissance-in-the-ai-era/?utm_source=random","/posts/stock-prices/?utm_source=random","/posts/possion-processes/?utm_source=random","/posts/docker-with-gpu/?utm_source=random","/posts/stochastic-processes/?utm_source=random","/posts/some-useful-tools/?utm_source=random","/posts/a-shopping-experience-in-pangdonglai-supermarket/?utm_source=random","/posts/langchain-and-rag-best-practices/?utm_source=random","/posts/random-walks/?utm_source=random","/posts/sequences-of-random-variables/?utm_source=random","/posts/parameter-estimation/?utm_source=random","/posts/two-random-variables/?utm_source=random","/posts/further-understanding-of-proc/?utm_source=random","/posts/the-iftop/?utm_source=random","/posts/some-useful-commands-to-share/?utm_source=random","/posts/random-variables/?utm_source=random","/posts/repeated-trials/?utm_source=random","/posts/probability-and-its-axioms/?utm_source=random","/posts/reflections-on-trending-topics/?utm_source=random","/posts/cpu-can-only-see-the-threads/?utm_source=random","/posts/go-common-test/?utm_source=random","/posts/go-concurrency-and-parallelism/?utm_source=random","/posts/go-cheatsheet/?utm_source=random","/posts/video-technology-101/?utm_source=random","/posts/the-main-kind-of-message-queue/?utm_source=random","/posts/the-method-to-manage-traffic/?utm_source=random","/posts/the-different-kind-of-api-design/?utm_source=random","/posts/the-encode-and-decode-in-python/?utm_source=random","/posts/about-the-systemd/?utm_source=random","/posts/the-tips-about-dockerfile/?utm_source=random","/posts/docker-cheatsheet/?utm_source=random","/posts/docker-101/?utm_source=random","/posts/the-instance-class-static-magic-method-in-python/?utm_source=random","/posts/the-review-and-plan-for-bilive/?utm_source=random","/posts/the-overview-of-security/?utm_source=random","/posts/how-to-publish-your-code-as-a-pip-module/?utm_source=random","/posts/some-good-things-to-share-about-the-packages/?utm_source=random","/posts/introduction-to-the-http-and-https-protocol/?utm_source=random","/posts/mail-service-and-protocol/?utm_source=random","/posts/understanding-clash-through-configuration/?utm_source=random","/posts/thinking-about-advertisement-from-an-open-source-perspective/?utm_source=random","/posts/a-brief-introduction-to-dns/?utm_source=random","/posts/real-computer-network/?utm_source=random","/posts/python-generator-iterator-and-decorator/?utm_source=random","/posts/python-underlying-mechanism/?utm_source=random","/posts/python-files-exceptions-and-modules/?utm_source=random","/posts/python-object-oriented-programming/?utm_source=random","/posts/python-cheatsheet/?utm_source=random","/posts/python-function-parameter/?utm_source=random","/posts/python-control-structure/?utm_source=random","/posts/python-composite-data-type/?utm_source=random","/posts/python-basic-data-type/?utm_source=random","/posts/python-basic-syntax-elements/?utm_source=random","/posts/deploy-github-pages-with-gpg-signing/?utm_source=random","/posts/gpg-101/?utm_source=random","/posts/housewarming-2024/?utm_source=random"],t=Math.floor(Math.random()*e.length);window.location.href=e[t]}</script></article><ins class=adsbygoogle style=display:block data-ad-format=fluid data-ad-layout-key=-c2+7a+0+1 data-ad-client=ca-pub-1347694832132708 data-ad-slot=5778132754></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script><div class=book-comments><div class="book-tabs-content markdown-inner"><div id=tcomment></div><script src=https://giscus.app/client.js data-repo=timerring/blog data-repo-id=R_kgDONeZYZg data-category=Announcements data-category-id=DIC_kwDONeZYZs4ClRWs data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=en crossorigin=anonymous async></script></div><ins class=adsbygoogle style=display:block data-ad-client=ca-pub-1347694832132708 data-ad-slot=1676758726 data-ad-format=auto data-full-width-responsive=true></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script><div class=busuanzi-footer><p style=text-align:center>&copy; 2022 - present <a href=https://github.com/timerring>timerring</a> | PV: <span id=busuanzi_value_site_pv></span> | UV: <span id=busuanzi_value_site_uv></span></p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#hidden-markov-model-hmm>HIDDEN MARKOV MODEL (HMM)</a><ul><li><a href=#weather-forecasting>WEATHER FORECASTING</a></li></ul></li><li><a href=#what-is-an-hmm>WHAT IS AN HMM?</a><ul><li><a href=#hmm-formalism>HMM FORMALISM</a></li><li><a href=#inference-in-an-hmm>INFERENCE IN AN HMM</a></li><li><a href=#examplespeech-recognition>EXAMPLE:SPEECH RECOGNITION</a></li><li><a href=#exampleword-recognition>EXAMPLE:WORD RECOGNITION</a></li><li><a href=#example-hmm-weather-forecasting>EXAMPLE HMM: WEATHER FORECASTING</a></li></ul></li><li><a href=#trellis-representation-of-an-hmm>TRELLIS REPRESENTATION OF AN HMM</a></li><li><a href=#hmm-problem-1-evaluation>HMM PROBLEM 1 Evaluation</a><ul><li><a href=#example>Example</a></li><li><a href=#the-forward-algorithm>THE FORWARD ALGORITHM</a></li></ul></li><li><a href=#hmm-problem-2-decoding>HMM PROBLEM 2 Decoding</a><ul><li><a href=#the-viterbi-algorithm>THE VITERBI ALGORITHM</a></li><li><a href=#viterbi-in-pseudocode>VITERBI IN PSEUDOCODE</a></li><li><a href=#homework-sunny-or-rainy>HOMEWORK! Sunny or Rainy?</a></li><li><a href=#why-problem-2>WHY PROBLEM 2?</a></li></ul></li><li><a href=#hmm-problem-3-learning>HMM PROBLEM 3 Learning</a><ul><li><a href=#forward-backward>FORWARD-BACKWARD</a></li><li><a href=#baum-welch-em>BAUM-WELCH (EM)</a></li></ul></li><li><a href=#hmm-caveats>HMM CAVEATS</a></li><li><a href=#beam-search>BEAM SEARCH</a></li><li><a href=#applications-for-hmms>APPLICATIONS FOR HMMS</a></li><li><a href=#predict-stock---market-behavior-with-markov-chains-and-python>PREDICT STOCK - MARKET BEHAVIOR WITH MARKOV CHAINS AND PYTHON</a></li></ul></nav><style>.book-toc .book-toc-content a{color:var(--body-font-color)}</style></div></aside></main></body></html>