<!doctype html><html lang=en dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="This is a quick-start essay for LangChain and RAG which mainly refers to the Langchain chat with your data course which are taught by Harrison Chase and Andrew Ng."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta name=keywords content="Introduction,LangChain,Prompt,Models,Indexes,Chains,Agents,RAG process,Vector Store Loading,Retrieval-Augmented Generation,Loaders,PDF,Web Base Loader,Splitters,Why split?,Type of splitters,Example CharacterTextSplitter and RecursiveCharacterTextSplitter,Vectorstores and Embeddings,Embeddings,Vector Stores,Practice,Embeddings,Vector Stores,Retrieval,Similarity Search,Practice,Maximum Marginal Relevance (MMR),Practice,Metadata,Practice,LLM Aided Retrieval,SelfQueryRetriever,Practice,Compression,Practice,Question Answering,RetrievalQA Chain,Map_reduce,Refine,Map_rerank,Practice,Conversational Retrieval Chain,Memory,Practice"><meta property="og:url" content="https://blog.timerring.com/posts/langchain-and-rag-best-practices/"><meta property="og:site_name" content="timerring"><meta property="og:title" content="Langchain and RAG Best Practices"><meta property="og:description" content="This is a quick-start essay for LangChain and RAG which mainly refers to the Langchain chat with your data course which are taught by Harrison Chase and Andrew Ng."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-05T00:05:24+08:00"><meta property="article:tag" content="Langchain"><meta property="article:tag" content="RAG"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-03-04-23-37-20.png"><meta itemprop=name content="Langchain and RAG Best Practices"><meta itemprop=description content="This is a quick-start essay for LangChain and RAG which mainly refers to the Langchain chat with your data course which are taught by Harrison Chase and Andrew Ng."><meta itemprop=datePublished content="2025-03-05T00:05:24+08:00"><meta itemprop=dateModified content="2025-03-05T00:05:24+08:00"><meta itemprop=wordCount content="3380"><meta itemprop=image content="https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-03-04-23-37-20.png"><meta itemprop=keywords content="Langchain,RAG"><meta name=twitter:image content="https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-03-04-23-37-20.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Langchain and RAG Best Practices"><meta name=twitter:description content="This is a quick-start essay for LangChain and RAG which mainly refers to the Langchain chat with your data course which are taught by Harrison Chase and Andrew Ng."><meta name=twitter:site content="@imjohnhowe"><link rel=canonical href=https://blog.timerring.com/posts/langchain-and-rag-best-practices/><title>Langchain and RAG Best Practices | timerring</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.d7a88d77202b21242efd3641ee0ccf6d2b317c3a57bcd08a48b51662fd988265.css integrity="sha256-16iNdyArISQu/TZB7gzPbSsxfDpXvNCKSLUWYv2YgmU=" crossorigin=anonymous><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.152b0dce9788c010c47c3acfdcfd813fbf0c5e764b5d8db5c403ec45fd205b57.js integrity="sha256-FSsNzpeIwBDEfDrP3P2BP78MXnZLXY21xAPsRf0gW1c=" crossorigin=anonymous></script><script defer src=/live-photo.min.efe7b2a516404d0718a4c6f90d2eb2f9c0bd8c7bddd382958716bcb24b660970.js integrity="sha256-7+eypRZATQcYpMb5DS6y+cC9jHvd04KVhxa8sktmCXA=" crossorigin=anonymous></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/favicon.png alt=Logo><span>timerring</span></a></h2><hr><p>Rhythmic trend.</p><div class=book-search><div class=gcse-search></div></div><script async src="https://cse.google.com/cse.js?cx=40782701766144f96"></script><ul><li><a href=/>Dashboard</a></li><li><a href=/posts/>Blogs</a></li><li><a href=/categories/weekly/>Newsletter</a></li><li><a href=/index.xml>RSS</a></li><li><a href=/friends/>Friends</a></li><li><a href=/about/>About</a></li></ul><script>function goToRandomPost(){const e=["/posts/0516-add-live-photo-effect-to-hugo/?utm_source=random","/posts/the-practice-of-resolving-domain/?utm_source=random","/posts/hidden-markov-models/?utm_source=random","/posts/markov-chains/?utm_source=random","/posts/markov-processes/?utm_source=random","/posts/0418-find-the-formula-rendering-issue-of-hugo/?utm_source=random","/posts/multi-platform-builds-docker/?utm_source=random","/posts/0404-The-invitation-only-mechanism-from-clubhouse-to-manus/?utm_source=random","/posts/how-to-use-git-submodule/?utm_source=random","/posts/live-streaming-infra-and-protocols/?utm_source=random","/posts/implement-danmaku-rendering-algorithm-from-scratch/?utm_source=random","/posts/is-there-an-rss-renaissance-in-the-ai-era/?utm_source=random","/posts/stock-prices/?utm_source=random","/posts/possion-processes/?utm_source=random","/posts/docker-with-gpu/?utm_source=random","/posts/stochastic-processes/?utm_source=random","/posts/some-useful-tools/?utm_source=random","/posts/a-shopping-experience-in-pangdonglai-supermarket/?utm_source=random","/posts/langchain-and-rag-best-practices/?utm_source=random","/posts/random-walks/?utm_source=random","/posts/sequences-of-random-variables/?utm_source=random","/posts/parameter-estimation/?utm_source=random","/posts/two-random-variables/?utm_source=random","/posts/further-understanding-of-proc/?utm_source=random","/posts/the-iftop/?utm_source=random","/posts/some-useful-commands-to-share/?utm_source=random","/posts/random-variables/?utm_source=random","/posts/repeated-trials/?utm_source=random","/posts/probability-and-its-axioms/?utm_source=random","/posts/reflections-on-trending-topics/?utm_source=random","/posts/cpu-can-only-see-the-threads/?utm_source=random","/posts/go-common-test/?utm_source=random","/posts/go-concurrency-and-parallelism/?utm_source=random","/posts/go-cheatsheet/?utm_source=random","/posts/video-technology-101/?utm_source=random","/posts/the-main-kind-of-message-queue/?utm_source=random","/posts/the-method-to-manage-traffic/?utm_source=random","/posts/the-different-kind-of-api-design/?utm_source=random","/posts/the-encode-and-decode-in-python/?utm_source=random","/posts/about-the-systemd/?utm_source=random","/posts/the-tips-about-dockerfile/?utm_source=random","/posts/docker-cheatsheet/?utm_source=random","/posts/docker-101/?utm_source=random","/posts/the-instance-class-static-magic-method-in-python/?utm_source=random","/posts/the-review-and-plan-for-bilive/?utm_source=random","/posts/the-overview-of-security/?utm_source=random","/posts/how-to-publish-your-code-as-a-pip-module/?utm_source=random","/posts/some-good-things-to-share-about-the-packages/?utm_source=random","/posts/introduction-to-the-http-and-https-protocol/?utm_source=random","/posts/mail-service-and-protocol/?utm_source=random","/posts/understanding-clash-through-configuration/?utm_source=random","/posts/thinking-about-advertisement-from-an-open-source-perspective/?utm_source=random","/posts/a-brief-introduction-to-dns/?utm_source=random","/posts/real-computer-network/?utm_source=random","/posts/python-generator-iterator-and-decorator/?utm_source=random","/posts/python-underlying-mechanism/?utm_source=random","/posts/python-files-exceptions-and-modules/?utm_source=random","/posts/python-object-oriented-programming/?utm_source=random","/posts/python-cheatsheet/?utm_source=random","/posts/python-function-parameter/?utm_source=random","/posts/python-control-structure/?utm_source=random","/posts/python-composite-data-type/?utm_source=random","/posts/python-basic-data-type/?utm_source=random","/posts/python-basic-syntax-elements/?utm_source=random","/posts/deploy-github-pages-with-gpg-signing/?utm_source=random","/posts/gpg-101/?utm_source=random","/posts/housewarming-2024/?utm_source=random"],t=Math.floor(Math.random()*e.length);window.location.href=e[t]}</script><a class=random onclick=goToRandomPost()>Stroll</a></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>timerring</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a><ul><li><a href=#langchain>LangChain</a><ul><li><a href=#prompt>Prompt</a></li><li><a href=#models>Models</a></li><li><a href=#indexes>Indexes</a></li><li><a href=#chains>Chains</a></li><li><a href=#agents>Agents</a></li></ul></li><li><a href=#rag-process>RAG process</a><ul><li><a href=#vector-store-loading>Vector Store Loading</a></li><li><a href=#retrieval-augmented-generation>Retrieval-Augmented Generation</a></li></ul></li></ul></li><li><a href=#loaders>Loaders</a><ul><li><a href=#pdf>PDF</a></li><li><a href=#web-base-loader>Web Base Loader</a></li></ul></li><li><a href=#splitters>Splitters</a><ul><li><a href=#why-split>Why split?</a></li><li><a href=#type-of-splitters>Type of splitters</a></li><li><a href=#example-charactertextsplitter-and-recursivecharactertextsplitter>Example CharacterTextSplitter and RecursiveCharacterTextSplitter</a></li></ul></li><li><a href=#vectorstores-and-embeddings>Vectorstores and Embeddings</a><ul><li><a href=#embeddings>Embeddings</a></li><li><a href=#vector-stores>Vector Stores</a></li><li><a href=#practice>Practice</a><ul><li><a href=#embeddings-1>Embeddings</a></li><li><a href=#vector-stores-1>Vector Stores</a></li></ul></li></ul></li><li><a href=#retrieval>Retrieval</a><ul><li><a href=#similarity-search>Similarity Search</a><ul><li><a href=#practice-1>Practice</a></li></ul></li><li><a href=#maximum-marginal-relevance-mmr>Maximum Marginal Relevance (MMR)</a><ul><li><a href=#practice-2>Practice</a></li></ul></li><li><a href=#metadata>Metadata</a><ul><li><a href=#practice-3>Practice</a></li></ul></li><li><a href=#llm-aided-retrieval>LLM Aided Retrieval</a><ul><li><a href=#selfqueryretriever>SelfQueryRetriever</a></li><li><a href=#practice-4>Practice</a></li></ul></li><li><a href=#compression>Compression</a><ul><li><a href=#practice-5>Practice</a></li></ul></li></ul></li><li><a href=#question-answering>Question Answering</a><ul><li><a href=#retrievalqa-chain>RetrievalQA Chain</a></li><li><a href=#map_reduce>Map_reduce</a></li><li><a href=#refine>Refine</a></li><li><a href=#map_rerank>Map_rerank</a></li><li><a href=#practice-6>Practice</a></li></ul></li><li><a href=#conversational-retrieval-chain>Conversational Retrieval Chain</a><ul><li><a href=#memory>Memory</a></li><li><a href=#practice-7>Practice</a></li></ul></li></ul></nav><style>.book-toc .book-toc-content a{color:var(--body-font-color)}</style></aside></header><article class=markdown><h1>Langchain and RAG Best Practices</h1><h5>March 5, 2025
·
16 min read
<span id=busuanzi_container_page_pv>· Page View: <span id=busuanzi_value_page_pv></span></span></h5><div><a href=/categories/tutorial/ style=color:#000>Tutorial</a></div><div><a href=/tags/langchain/ style=color:#000>Langchain</a> <span style=color:#000>| </span><a href=/tags/rag/ style=color:#000>RAG</a></div><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-03-04-23-37-20.png></p><p class=title-image><i>The whole process of RAG</i></p><blockquote class="book-hint info">If you have any questions, feel free to comment below.</blockquote><hr><p>This is a quick-start essay for LangChain and RAG which mainly refers to the <a href="https://learn.deeplearning.ai/courses/langchain-chat-with-your-data/lesson/snupv/introduction?courseName=langchain-chat-with-your-data" target=_blank rel="nofollow noopener">Langchain chat with your data<span style=white-space:nowrap><svg width=".7em" height=".7em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a> course which are taught by Harrison Chase and Andrew Ng.</p><p>You can check the entire code in the <a href=https://github.com/timerring/rag101/ target=_blank rel="nofollow noopener">rag101 repository<span style=white-space:nowrap><svg width=".7em" height=".7em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a>.</p><h1 id=langchain-and-rag-best-practices>LangChain and RAG best practices
<a class=anchor href=#langchain-and-rag-best-practices>#</a></h1><h2 id=introduction>Introduction
<a class=anchor href=#introduction>#</a></h2><h3 id=langchain>LangChain
<a class=anchor href=#langchain>#</a></h3><p>LangChain is an Open-source developer framework for building LLM applications.</p><p>It components are as below:</p><h4 id=prompt>Prompt
<a class=anchor href=#prompt>#</a></h4><ul><li>Prompt Templates: used for generating model input.</li><li>Output Parsers: implementations for processing generated results.</li><li>Example Selectors: selecting appropriate input examples.</li></ul><h4 id=models>Models
<a class=anchor href=#models>#</a></h4><ul><li>LLMs</li><li>Chat Models</li><li>Text Embedding Models</li></ul><h4 id=indexes>Indexes
<a class=anchor href=#indexes>#</a></h4><ul><li>Document Loaders</li><li>Text Splitters</li><li>Vector Stores</li><li>Retrievers</li></ul><h4 id=chains>Chains
<a class=anchor href=#chains>#</a></h4><ul><li>Can be used as a building block for other chains.</li><li>Provides over 20 types of application-specific chains.</li></ul><h4 id=agents>Agents
<a class=anchor href=#agents>#</a></h4><ul><li>Supports 5 types of agents to help language models use external tools.</li><li>Agent Toolkits: provides over 10 implementations, agents execute tasks through specific tools.</li></ul><h3 id=rag-process>RAG process
<a class=anchor href=#rag-process>#</a></h3><p>The whole RAG process lays on the Vector Store Loading and Retrieval-Augmented Generation.</p><h4 id=vector-store-loading>Vector Store Loading
<a class=anchor href=#vector-store-loading>#</a></h4><p>Load the data from different sources, split and convert them into vector embeddings.</p><h4 id=retrieval-augmented-generation>Retrieval-Augmented Generation
<a class=anchor href=#retrieval-augmented-generation>#</a></h4><ol><li>After the user&rsquo;s input <strong>Query</strong>, the system will retrieve the most relevant document fragments (Relevant Splits) from the vector store.</li><li>The retrieved relevant fragments will be combined into a <strong>Prompt</strong>, which will be passed along with the context to the large language model (LLM).</li><li>Finally, the language model will generate an answer based on the retrieved fragments and return it to the user.</li></ol><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-03-03-20-33-44.png alt></p><h2 id=loaders>Loaders
<a class=anchor href=#loaders>#</a></h2><p>You can use loaders to deal with different kind and format of data.</p><p>Some are public and some are proprietary. Some are structured and some are not.</p><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-03-03-20-37-44.png alt></p><p>Some useful lib:</p><ul><li>pdf: pypdf</li><li>youtube audio: yt_dlp pydub</li><li>web page: beautifulsoup4</li></ul><p>For more loaders, you can check the <a href=https://python.langchain.com/api_reference/community/document_loaders.html#module-langchain_community.document_loaders target=_blank rel="nofollow noopener">official docs<span style=white-space:nowrap><svg width=".7em" height=".7em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a>.</p><p>You can check the entire code <a href=https://github.com/timerring/rag101/tree/main/loader target=_blank rel="nofollow noopener">here<span style=white-space:nowrap><svg width=".7em" height=".7em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a>.</p><h3 id=pdf>PDF
<a class=anchor href=#pdf>#</a></h3><p>Now, we can practice:</p><p>First, install the lib:</p><div class=highlight><pre tabindex=0 style=-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install langchain-community 
</span></span><span style=display:flex><span>pip install pypdf
</span></span></code></pre></div><p>You can check the demo in the</p><div class=highlight><pre tabindex=0 style=-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain.document_loaders</span> <span style=color:green;font-weight:700>import</span> PyPDFLoader
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># In fact, the langchain calls the pypdf lib to load the pdf file</span>
</span></span><span style=display:flex><span>loader <span style=color:#666>=</span> PyPDFLoader(<span style=color:#ba2121>&#34;ProbRandProc_Notes2004_JWBerkeley.pdf&#34;</span>)
</span></span><span style=display:flex><span>pages <span style=color:#666>=</span> loader<span style=color:#666>.</span>load()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>print</span>(<span style=color:green>type</span>(pages))
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># &lt;class &#39;list&#39;&gt;</span>
</span></span><span style=display:flex><span><span style=color:green>print</span>(<span style=color:green>len</span>(pages))
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># Print the total num of pages</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># Using the first page as an example</span>
</span></span><span style=display:flex><span>page <span style=color:#666>=</span> pages[<span style=color:#666>0</span>]
</span></span><span style=display:flex><span><span style=color:green>print</span>(<span style=color:green>type</span>(page))
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># &lt;class &#39;langchain_core.documents.base.Document&#39;&gt;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># What is inside the page:</span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># 1. page_content</span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># 2. meta_data: the description of the page</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>print</span>(page<span style=color:#666>.</span>page_content[<span style=color:#666>0</span>:<span style=color:#666>500</span>])
</span></span><span style=display:flex><span><span style=color:green>print</span>(page<span style=color:#666>.</span>metadata)
</span></span></code></pre></div><h3 id=web-base-loader>Web Base Loader
<a class=anchor href=#web-base-loader>#</a></h3><p>Also we install the lib first:</p><div class=highlight><pre tabindex=0 style=-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install beautifulsoup4
</span></span></code></pre></div><p>The WebBaseLoader is based on the beautifulsoup4 lib.</p><div class=highlight><pre tabindex=0 style=-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain_community.document_loaders</span> <span style=color:green;font-weight:700>import</span> WebBaseLoader
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loader <span style=color:#666>=</span> WebBaseLoader(<span style=color:#ba2121>&#34;https://zh.d2l.ai/&#34;</span>)
</span></span><span style=display:flex><span>pages <span style=color:#666>=</span> loader<span style=color:#666>.</span>load()
</span></span><span style=display:flex><span><span style=color:green>print</span>(pages[<span style=color:#666>0</span>]<span style=color:#666>.</span>page_content[:<span style=color:#666>500</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># You can also use json as the post processing</span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># import json</span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># convert_to_json = json.loads(pages[0].page_content)</span>
</span></span></code></pre></div><h2 id=splitters>Splitters
<a class=anchor href=#splitters>#</a></h2><p>Splitting Documents into smaller chunks. Retaining the meaningful relationships.</p><h3 id=why-split>Why split?
<a class=anchor href=#why-split>#</a></h3><ul><li>The limitation of GPU: the GPT model with more than 1B parameters. The forward propagation cannot process such a large parameters. So the split is necessary.</li><li>More efficient computation.</li><li>Some fixed size of sequence.</li><li>Better generalization.</li></ul><blockquote><p>However, the split points may lose some information. So we split should consider the semantic.</p></blockquote><h3 id=type-of-splitters>Type of splitters
<a class=anchor href=#type-of-splitters>#</a></h3><ul><li>CharacterTextSplitter</li><li>MarkdownHeaderTextSplitter</li><li>TokenTextsplitter</li><li>SentenceTransformersTokenTextSplitter</li><li><strong>RecursiveCharacterTextSplitter</strong>: Recursively tries to split by different characters to find one that works.</li><li>Language: for CPP, Python, Ruby, Markdown etc</li><li>NLTKTextSplitter: sentences using NLTK(Natural Language Tool Kit)</li><li>SpacyTextSplitter: sentences using Spacy</li></ul><p>For more, check the <a href=https://python.langchain.com/api_reference/text_splitters/index.html#module-langchain_text_splitters target=_blank rel="nofollow noopener">docs<span style=white-space:nowrap><svg width=".7em" height=".7em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a>.</p><h3 id=example-charactertextsplitter-and-recursivecharactertextsplitter>Example CharacterTextSplitter and RecursiveCharacterTextSplitter
<a class=anchor href=#example-charactertextsplitter-and-recursivecharactertextsplitter>#</a></h3><p>You can check the entire code <a href=https://github.com/timerring/rag101/blob/main/splitter/text_splitter.py target=_blank rel="nofollow noopener">here<span style=white-space:nowrap><svg width=".7em" height=".7em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a>.</p><div class=highlight><pre tabindex=0 style=-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain.text_splitter</span> <span style=color:green;font-weight:700>import</span> RecursiveCharacterTextSplitter, CharacterTextSplitter
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>example_text <span style=color:#666>=</span> <span style=color:#ba2121>&#34;&#34;&#34;When writing documents, writers will use document structure to group content. </span><span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span><span style=color:#ba2121>This can convey to the reader, which idea&#39;s are related. For example, closely related ideas </span><span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span><span style=color:#ba2121>are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. </span><span style=color:#b62;font-weight:700>\n\n</span><span style=color:#ba2121>  </span><span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span><span style=color:#ba2121>Paragraphs are often delimited with a carriage return or two carriage returns. </span><span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span><span style=color:#ba2121>Carriage returns are the &#34;backslash n&#34; you see embedded in this string. </span><span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span><span style=color:#ba2121>Sentences have a period at the end, but also, have a space.</span><span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span><span style=color:#ba2121>and words are separated by space.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>c_splitter <span style=color:#666>=</span> CharacterTextSplitter(
</span></span><span style=display:flex><span>    chunk_size<span style=color:#666>=</span><span style=color:#666>450</span>, <span style=color:#408080;font-style:italic># the size of the chunk</span>
</span></span><span style=display:flex><span>    chunk_overlap<span style=color:#666>=</span><span style=color:#666>0</span>, <span style=color:#408080;font-style:italic># the overlap of the chunk, which can be shared with the previous chunk</span>
</span></span><span style=display:flex><span>    separator <span style=color:#666>=</span> <span style=color:#ba2121>&#39; &#39;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>r_splitter <span style=color:#666>=</span> RecursiveCharacterTextSplitter(
</span></span><span style=display:flex><span>    chunk_size<span style=color:#666>=</span><span style=color:#666>450</span>,
</span></span><span style=display:flex><span>    chunk_overlap<span style=color:#666>=</span><span style=color:#666>0</span>, 
</span></span><span style=display:flex><span>    separators<span style=color:#666>=</span>[<span style=color:#ba2121>&#34;</span><span style=color:#b62;font-weight:700>\n\n</span><span style=color:#ba2121>&#34;</span>, <span style=color:#ba2121>&#34;</span><span style=color:#b62;font-weight:700>\n</span><span style=color:#ba2121>&#34;</span>, <span style=color:#ba2121>&#34; &#34;</span>, <span style=color:#ba2121>&#34;&#34;</span>] <span style=color:#408080;font-style:italic># priority of the separators</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>print</span>(c_splitter<span style=color:#666>.</span>split_text(example_text))
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># split at 450 characters</span>
</span></span><span style=display:flex><span><span style=color:green>print</span>(r_splitter<span style=color:#666>.</span>split_text(example_text))
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># split at first \n\n</span>
</span></span></code></pre></div><h2 id=vectorstores-and-embeddings>Vectorstores and Embeddings
<a class=anchor href=#vectorstores-and-embeddings>#</a></h2><p>Review the RAG process:</p><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-03-03-22-30-38.png alt></p><p>Benefits:</p><ol><li>Improve the accuracy of the query. When query the similar chunks, the accuracy will be higher.</li><li>Improve the efficiency of the query. Minimize the computation when query the similar chunks.</li><li>Improve the coverage of the query. The chunks can cover every point of the document.</li><li>Facilitate the Embeddings.</li></ol><h3 id=embeddings>Embeddings
<a class=anchor href=#embeddings>#</a></h3><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-03-03-22-35-38.png alt>
<img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-03-03-22-36-48.png alt></p><p>If two sentences have similar meanings, then they will be closer in the high-dimensional semantic space.</p><h3 id=vector-stores>Vector Stores
<a class=anchor href=#vector-stores>#</a></h3><p>Store every chunk in a vector store. When customer query, the query will be embedded and then find the most similar vectors which means the index of these chunks, and then return the chunks.</p><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-03-03-22-37-35.png alt>
<img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-03-03-22-41-22.png alt></p><h3 id=practice>Practice
<a class=anchor href=#practice>#</a></h3><h4 id=embeddings-1>Embeddings
<a class=anchor href=#embeddings-1>#</a></h4><p>You can check the entire code <a href=https://github.com/timerring/rag101/blob/main/embeddings/zhipu.py target=_blank rel="nofollow noopener">here<span style=white-space:nowrap><svg width=".7em" height=".7em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a>.</p><p>First, install the lib:</p><p>The <code>chromadb</code> is a lightweight vector database.</p><div class=highlight><pre tabindex=0 style=-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install chromadb
</span></span></code></pre></div><p>What we need is a good embedding model, you can select what you like. Refer to the <a href=https://python.langchain.com/api_reference/community/embeddings.html#module-langchain_community.embeddings target=_blank rel="nofollow noopener">docs<span style=white-space:nowrap><svg width=".7em" height=".7em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a>.</p><p>Here I use the <code>ZhipuAIEmbeddings</code>. So you should install the lib:</p><div class=highlight><pre tabindex=0 style=-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install zhipuai
</span></span></code></pre></div><p>Here is the test code:</p><div class=highlight><pre tabindex=0 style=-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain_community.embeddings</span> <span style=color:green;font-weight:700>import</span> ZhipuAIEmbeddings
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>embed <span style=color:#666>=</span> ZhipuAIEmbeddings(
</span></span><span style=display:flex><span>    model<span style=color:#666>=</span><span style=color:#ba2121>&#34;embedding-3&#34;</span>,
</span></span><span style=display:flex><span>    api_key<span style=color:#666>=</span><span style=color:#ba2121>&#34;Entry your own api key&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>input_texts <span style=color:#666>=</span> [<span style=color:#ba2121>&#34;This is a test query1.&#34;</span>, <span style=color:#ba2121>&#34;This is a test query2.&#34;</span>]
</span></span><span style=display:flex><span><span style=color:green>print</span>(embed<span style=color:#666>.</span>embed_documents(input_texts))
</span></span></code></pre></div><h4 id=vector-stores-1>Vector Stores
<a class=anchor href=#vector-stores-1>#</a></h4><p>You can check the entire code <a href=https://github.com/timerring/rag101/blob/main/vectorstores/chroma.py target=_blank rel="nofollow noopener">here<span style=white-space:nowrap><svg width=".7em" height=".7em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a>.</p><div class=highlight><pre tabindex=0 style=-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install langchain-chroma
</span></span></code></pre></div><p>Then we can use the <code>Chroma</code> to store the embeddings.</p><div class=highlight><pre tabindex=0 style=-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain_chroma</span> <span style=color:green;font-weight:700>import</span> Chroma
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain_community.document_loaders</span> <span style=color:green;font-weight:700>import</span> WebBaseLoader
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain.text_splitter</span> <span style=color:green;font-weight:700>import</span> RecursiveCharacterTextSplitter
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain_community.embeddings</span> <span style=color:green;font-weight:700>import</span> ZhipuAIEmbeddings
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># load the web page</span>
</span></span><span style=display:flex><span>loader <span style=color:#666>=</span> WebBaseLoader(<span style=color:#ba2121>&#34;https://en.d2l.ai/&#34;</span>)
</span></span><span style=display:flex><span>docs <span style=color:#666>=</span> loader<span style=color:#666>.</span>load()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># split the text into chunks</span>
</span></span><span style=display:flex><span>text_splitter <span style=color:#666>=</span> RecursiveCharacterTextSplitter(
</span></span><span style=display:flex><span>    chunk_size <span style=color:#666>=</span> <span style=color:#666>1500</span>,
</span></span><span style=display:flex><span>    chunk_overlap <span style=color:#666>=</span> <span style=color:#666>150</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>splits <span style=color:#666>=</span> text_splitter<span style=color:#666>.</span>split_documents(docs)
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># print(len(splits))</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># set the embeddings models</span>
</span></span><span style=display:flex><span>embeddings <span style=color:#666>=</span> ZhipuAIEmbeddings(
</span></span><span style=display:flex><span>    model<span style=color:#666>=</span><span style=color:#ba2121>&#34;embedding-3&#34;</span>,
</span></span><span style=display:flex><span>    api_key<span style=color:#666>=</span><span style=color:#ba2121>&#34;your own api key&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># set the persist directory</span>
</span></span><span style=display:flex><span>persist_directory <span style=color:#666>=</span> <span style=color:#ba2121>r</span><span style=color:#ba2121>&#39;.&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># create the vector database</span>
</span></span><span style=display:flex><span>vectordb <span style=color:#666>=</span> Chroma<span style=color:#666>.</span>from_documents(
</span></span><span style=display:flex><span>    documents<span style=color:#666>=</span>splits,
</span></span><span style=display:flex><span>    embedding<span style=color:#666>=</span>embeddings,
</span></span><span style=display:flex><span>    persist_directory<span style=color:#666>=</span>persist_directory
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># print(vectordb._collection.count())</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># query the vector database</span>
</span></span><span style=display:flex><span>question <span style=color:#666>=</span> <span style=color:#ba2121>&#34;Recurrent&#34;</span>
</span></span><span style=display:flex><span>docs <span style=color:#666>=</span> vectordb<span style=color:#666>.</span>similarity_search(question, k<span style=color:#666>=</span><span style=color:#666>3</span>)
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># print(len(docs))</span>
</span></span><span style=display:flex><span><span style=color:green>print</span>(docs[<span style=color:#666>0</span>]<span style=color:#666>.</span>page_content)
</span></span></code></pre></div><p>Then you can find the <code>chorma.sqlite3</code> file in the specific directory.</p><h2 id=retrieval>Retrieval
<a class=anchor href=#retrieval>#</a></h2><p>This part is the core part of the RAG.</p><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-03-04-20-58-14.png alt></p><p>Last part we have already used the <code>similarity_search</code> method. On top of that, we also have other methods.</p><ul><li>Basic semantic similarity</li><li>Maximum Marginal Relevance(MMR)</li><li>Metadata</li><li>LLM Aided Retrieval</li></ul><h3 id=similarity-search>Similarity Search
<a class=anchor href=#similarity-search>#</a></h3><p>Similarity Search calculates the similarity between the query vector and all document vectors in the database to find the most relevant document.</p><p>The similarity measurement methods include <strong>cosine similarity</strong> and <strong>Euclidean distance</strong>, which can effectively measure the closeness of two vectors in a high-dimensional space.</p><p>However, relying solely on similarity search may result in insufficient diversity, as it only focuses on the match between the query and the content, ignoring the differences between different pieces of information. In some applications, especially when it is necessary to cover <strong>multiple different aspects of information</strong>, the extended method of Maximum Marginal Relevance (MMR) can better balance relevance and diversity.</p><h4 id=practice-1>Practice
<a class=anchor href=#practice-1>#</a></h4><p>The practice part is on the pervious part.</p><h3 id=maximum-marginal-relevance-mmr>Maximum Marginal Relevance (MMR)
<a class=anchor href=#maximum-marginal-relevance-mmr>#</a></h3><p>Retrieving only the most relevant documents may overlook the diversity of information. For example, if only the most similar response is selected, the <strong>results may be very similar or even contain duplicate content</strong>. The core idea of MMR is to balance relevance and diversity, that is, to select the information most relevant to the query while ensuring that the information is diverse in content. <strong>By reducing the repetition of information between different pieces</strong>, MMR can provide a more comprehensive and diverse set of results.</p><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-03-04-21-14-54.png alt></p><p>The process of MMR is as follows:</p><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-03-04-21-21-02.png alt></p><ol><li>Query the Vector Store: First convert the query into vectors using the embedding model.</li><li>Choose the <code>fetch_k</code> most similar responses. Find the top <code>k</code> most similar vectors from the vector store.</li><li>Within those responses choose the <code>k</code> most diverse. By calculating the similarity between each response, MMR will prefer results that are <strong>more different from each other</strong>, thus increasing the coverage of information. This process ensures that the returned results are not only &ldquo;most similar&rdquo;, but also &ldquo;complementary&rdquo;.</li></ol><p>The key parameter is the <code>lambda</code> which is the weight of the relevance and diversity.</p><ul><li>When lambda is close to 1, MMR will be more like the similarity search.</li><li>When lambda is close to 0, MMR will be more like the random search.</li></ul><h4 id=practice-2>Practice
<a class=anchor href=#practice-2>#</a></h4><p>We can adjust the code in <code>Vector stores</code> part to use the MMR method. The full code is in the <a href=https://github.com/timerring/rag101/blob/main/retrieval/mmr.py target=_blank rel="nofollow noopener"><code>retrieval/mmr.py</code> file<span style=white-space:nowrap><svg width=".7em" height=".7em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a>.</p><div class=highlight><pre tabindex=0 style=-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#408080;font-style:italic># query the vector database with MMR</span>
</span></span><span style=display:flex><span>question <span style=color:#666>=</span> <span style=color:#ba2121>&#34;How the neural network works?&#34;</span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># fetch the 8 most similar documents, and then choose the 2 most relevant documents</span>
</span></span><span style=display:flex><span>docs_mmr <span style=color:#666>=</span> vectordb<span style=color:#666>.</span>max_marginal_relevance_search(question, fetch_k<span style=color:#666>=</span><span style=color:#666>8</span>, k<span style=color:#666>=</span><span style=color:#666>2</span>)
</span></span><span style=display:flex><span><span style=color:green>print</span>(docs_mmr[<span style=color:#666>0</span>]<span style=color:#666>.</span>page_content[:<span style=color:#666>100</span>])
</span></span><span style=display:flex><span><span style=color:green>print</span>(docs_mmr[<span style=color:#666>1</span>]<span style=color:#666>.</span>page_content[:<span style=color:#666>100</span>])
</span></span></code></pre></div><h3 id=metadata>Metadata
<a class=anchor href=#metadata>#</a></h3><p>When our query is under some specific conditions, we can use the metadata to filter the results.</p><p>For example, the information such as page numbers, authors, timestamps, etc. These information can be used as filtering conditions during retrieval, thus improving the accuracy of the query.</p><h4 id=practice-3>Practice
<a class=anchor href=#practice-3>#</a></h4><p>You can check the entire code <a href=https://github.com/timerring/rag101/blob/main/retrieval/metadata.py target=_blank rel="nofollow noopener">here<span style=white-space:nowrap><svg width=".7em" height=".7em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a>.</p><p>Add new documents from another website, and then filter the results from the specific website.</p><div class=highlight><pre tabindex=0 style=-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain_chroma</span> <span style=color:green;font-weight:700>import</span> Chroma
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain_community.document_loaders</span> <span style=color:green;font-weight:700>import</span> WebBaseLoader
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain.text_splitter</span> <span style=color:green;font-weight:700>import</span> RecursiveCharacterTextSplitter
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain_community.embeddings</span> <span style=color:green;font-weight:700>import</span> ZhipuAIEmbeddings
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># load the web page</span>
</span></span><span style=display:flex><span>loader <span style=color:#666>=</span> WebBaseLoader(<span style=color:#ba2121>&#34;https://en.d2l.ai/&#34;</span>)
</span></span><span style=display:flex><span>docs <span style=color:#666>=</span> loader<span style=color:#666>.</span>load()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># split the text into chunks</span>
</span></span><span style=display:flex><span>text_splitter <span style=color:#666>=</span> RecursiveCharacterTextSplitter(
</span></span><span style=display:flex><span>    chunk_size <span style=color:#666>=</span> <span style=color:#666>1500</span>,
</span></span><span style=display:flex><span>    chunk_overlap <span style=color:#666>=</span> <span style=color:#666>150</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>splits <span style=color:#666>=</span> text_splitter<span style=color:#666>.</span>split_documents(docs)
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># print(len(splits))</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># set the embeddings models</span>
</span></span><span style=display:flex><span>embeddings <span style=color:#666>=</span> ZhipuAIEmbeddings(
</span></span><span style=display:flex><span>    model<span style=color:#666>=</span><span style=color:#ba2121>&#34;embedding-3&#34;</span>,
</span></span><span style=display:flex><span>    api_key<span style=color:#666>=</span><span style=color:#ba2121>&#34;your_api_key&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># set the persist directory</span>
</span></span><span style=display:flex><span>persist_directory <span style=color:#666>=</span> <span style=color:#ba2121>r</span><span style=color:#ba2121>&#39;.&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># create the vector database</span>
</span></span><span style=display:flex><span>vectordb <span style=color:#666>=</span> Chroma<span style=color:#666>.</span>from_documents(
</span></span><span style=display:flex><span>    documents<span style=color:#666>=</span>splits,
</span></span><span style=display:flex><span>    embedding<span style=color:#666>=</span>embeddings,
</span></span><span style=display:flex><span>    persist_directory<span style=color:#666>=</span>persist_directory
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># print(vectordb._collection.count())</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># add new documents from another website</span>
</span></span><span style=display:flex><span>new_loader <span style=color:#666>=</span> WebBaseLoader(<span style=color:#ba2121>&#34;https://www.deeplearning.ai/&#34;</span>)
</span></span><span style=display:flex><span>new_docs <span style=color:#666>=</span> new_loader<span style=color:#666>.</span>load()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># split the text into chunks</span>
</span></span><span style=display:flex><span>new_splits <span style=color:#666>=</span> text_splitter<span style=color:#666>.</span>split_documents(new_docs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># add to the existing vector database</span>
</span></span><span style=display:flex><span>vectordb<span style=color:#666>.</span>add_documents(new_splits)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># Get all documents</span>
</span></span><span style=display:flex><span>all_docs <span style=color:#666>=</span> vectordb<span style=color:#666>.</span>similarity_search(<span style=color:#ba2121>&#34;What is the difference between a neural network and a deep learning model?&#34;</span>, k<span style=color:#666>=</span><span style=color:#666>20</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># Print the metadata of the documents</span>
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>for</span> i, doc <span style=color:#a2f;font-weight:700>in</span> <span style=color:green>enumerate</span>(all_docs):
</span></span><span style=display:flex><span>    <span style=color:green>print</span>(<span style=color:#ba2121>f</span><span style=color:#ba2121>&#34;Document </span><span style=color:#b68;font-weight:700>{</span>i<span style=color:#666>+</span><span style=color:#666>1</span><span style=color:#b68;font-weight:700>}</span><span style=color:#ba2121> metadata: </span><span style=color:#b68;font-weight:700>{</span>doc<span style=color:#666>.</span>metadata<span style=color:#b68;font-weight:700>}</span><span style=color:#ba2121>&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># Document 1 metadata: {&#39;language&#39;: &#39;en&#39;, &#39;source&#39;: &#39;https://en.d2l.ai/&#39;, &#39;title&#39;: &#39;Dive into Deep Learning — Dive into Deep Learning 1.0.3 documentation&#39;}</span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># Document 2 metadata: {&#39;language&#39;: &#39;en&#39;, &#39;source&#39;: &#39;https://en.d2l.ai/&#39;, &#39;title&#39;: &#39;Dive into Deep Learning — Dive into Deep Learning 1.0.3 documentation&#39;}</span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># Document 3 metadata: {&#39;language&#39;: &#39;en&#39;, &#39;source&#39;: &#39;https://en.d2l.ai/&#39;, &#39;title&#39;: &#39;Dive into Deep Learning — Dive into Deep Learning 1.0.3 documentation&#39;}</span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># Document 4 metadata: {&#39;description&#39;: &#39;DeepLearning.AI | Andrew Ng | Join over 7 million people learning how to use and build AI through our online courses. Earn certifications, level up your skills, and stay ahead of the industry.&#39;, &#39;language&#39;: &#39;en&#39;, &#39;source&#39;: &#39;https://www.deeplearning.ai/&#39;, &#39;title&#39;: &#39;DeepLearning.AI: Start or Advance Your Career in AI&#39;}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>question <span style=color:#666>=</span> <span style=color:#ba2121>&#34;how the neural network works?&#34;</span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># filter the documents from the specific website</span>
</span></span><span style=display:flex><span>docs_meta <span style=color:#666>=</span> vectordb<span style=color:#666>.</span>similarity_search(question, k<span style=color:#666>=</span><span style=color:#666>1</span>, <span style=color:green>filter</span><span style=color:#666>=</span>{<span style=color:#ba2121>&#34;source&#34;</span>: <span style=color:#ba2121>&#34;https://www.deeplearning.ai/&#34;</span>})
</span></span><span style=display:flex><span><span style=color:green>print</span>(docs_meta[<span style=color:#666>0</span>]<span style=color:#666>.</span>page_content[:<span style=color:#666>100</span>])
</span></span></code></pre></div><h3 id=llm-aided-retrieval>LLM Aided Retrieval
<a class=anchor href=#llm-aided-retrieval>#</a></h3><p>It uses language models to automatically parse sentence semantics, extract filtering information.</p><h4 id=selfqueryretriever>SelfQueryRetriever
<a class=anchor href=#selfqueryretriever>#</a></h4><p>LangChain provides the SelfQueryRetriever module, which can analyze the semantics of the question sentence from the language model, and extract the search term and filter conditions.</p><ul><li>The <strong>search term</strong> of the vector search</li><li>The <strong>filter conditions</strong> of the document metadata</li></ul><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-03-04-23-44-01.png alt></p><p>For example, for the question &ldquo;Besides Wikipedia, which health websites are there?&rdquo;, SelfQueryRetriever can infer that &ldquo;Wikipedia&rdquo; represents the filter condition, that is, to exclude the documents from Wikipedia.</p><h4 id=practice-4>Practice
<a class=anchor href=#practice-4>#</a></h4><div class=highlight><pre tabindex=0 style=-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain.llms</span> <span style=color:green;font-weight:700>import</span> OpenAI
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain.retrievers.self_query.base</span> <span style=color:green;font-weight:700>import</span> SelfQueryRetriever
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain.chains.query_constructor.base</span> <span style=color:green;font-weight:700>import</span> AttributeInfo
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>llm <span style=color:#666>=</span> OpenAI(temperature<span style=color:#666>=</span><span style=color:#666>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>metadata_field_info <span style=color:#666>=</span> [
</span></span><span style=display:flex><span>    AttributeInfo(
</span></span><span style=display:flex><span>        name<span style=color:#666>=</span><span style=color:#ba2121>&#34;source&#34;</span>, <span style=color:#408080;font-style:italic>#  source is to tell the LLM the data is from which document</span>
</span></span><span style=display:flex><span>        description<span style=color:#666>=</span><span style=color:#ba2121>&#34;The lecture the chunk is from, should be one of `docs/loaders.pdf`, `docs/text_splitters.pdf`, or `docs/vectorstores.pdf`&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:green>type</span><span style=color:#666>=</span><span style=color:#ba2121>&#34;string&#34;</span>,
</span></span><span style=display:flex><span>    ),
</span></span><span style=display:flex><span>    AttributeInfo(
</span></span><span style=display:flex><span>        name<span style=color:#666>=</span><span style=color:#ba2121>&#34;page&#34;</span>, <span style=color:#408080;font-style:italic># page is to tell the LLM the data is from which page</span>
</span></span><span style=display:flex><span>        description<span style=color:#666>=</span><span style=color:#ba2121>&#34;The page from the lecture&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:green>type</span><span style=color:#666>=</span><span style=color:#ba2121>&#34;integer&#34;</span>,
</span></span><span style=display:flex><span>    ),
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>document_content_description <span style=color:#666>=</span> <span style=color:#ba2121>&#34;the lectures of retrieval augmentation generation&#34;</span>
</span></span><span style=display:flex><span>retriever <span style=color:#666>=</span> SelfQueryRetriever<span style=color:#666>.</span>from_llm(
</span></span><span style=display:flex><span>    llm,
</span></span><span style=display:flex><span>    vectordb,
</span></span><span style=display:flex><span>    document_content_description,
</span></span><span style=display:flex><span>    metadata_field_info,
</span></span><span style=display:flex><span>    verbose<span style=color:#666>=</span><span style=color:green;font-weight:700>True</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>question <span style=color:#666>=</span> <span style=color:#ba2121>&#34;What is the main topic of second lecture?&#34;</span>  
</span></span></code></pre></div><h3 id=compression>Compression
<a class=anchor href=#compression>#</a></h3><p>When using vector retrieval to get relevant documents, directly returning the entire document fragment may lead to resource waste, as the actual relevant part is only a small part of the document. To improve this, LangChain provides a &ldquo;compression&rdquo; retrieval mechanism.</p><p>Its working principle is to first use standard vector retrieval to obtain candidate documents, and then <strong>use a language model to compress these documents</strong> based on the semantic meaning of the query sentence, only retaining the relevant part of the document.</p><p>For example, for the query &ldquo;the nutritional value of mushrooms&rdquo;, the retrieval may return a long document about mushrooms. After compression, only the sentences related to &ldquo;nutritional value&rdquo; are extracted from the document.</p><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-03-04-23-50-24.png alt></p><h4 id=practice-5>Practice
<a class=anchor href=#practice-5>#</a></h4><div class=highlight><pre tabindex=0 style=-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain.retrievers</span> <span style=color:green;font-weight:700>import</span> ContextualCompressionRetriever
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain.retrievers.document_compressors</span> <span style=color:green;font-weight:700>import</span> LLMChainExtractor
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>def</span> <span style=color:#00f>pretty_print_docs</span>(docs):
</span></span><span style=display:flex><span>    <span style=color:green>print</span>(<span style=color:#ba2121>f</span><span style=color:#ba2121>&#34;</span><span style=color:#b62;font-weight:700>\n</span><span style=color:#b68;font-weight:700>{</span><span style=color:#ba2121>&#39;-&#39;</span> <span style=color:#666>*</span> <span style=color:#666>100</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b62;font-weight:700>\n</span><span style=color:#ba2121>&#34;</span><span style=color:#666>.</span>join([<span style=color:#ba2121>f</span><span style=color:#ba2121>&#34;Document </span><span style=color:#b68;font-weight:700>{</span>i<span style=color:#666>+</span><span style=color:#666>1</span><span style=color:#b68;font-weight:700>}</span><span style=color:#ba2121>:</span><span style=color:#b62;font-weight:700>\n\n</span><span style=color:#ba2121>&#34;</span> <span style=color:#666>+</span> d<span style=color:#666>.</span>page_content <span style=color:green;font-weight:700>for</span> i, d <span style=color:#a2f;font-weight:700>in</span> <span style=color:green>enumerate</span>(docs)]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>llm <span style=color:#666>=</span> OpenAI(temperature<span style=color:#666>=</span><span style=color:#666>0</span>)
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># initialize the compressor</span>
</span></span><span style=display:flex><span>compressor <span style=color:#666>=</span> LLMChainExtractor<span style=color:#666>.</span>from_llm(llm)
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># initialize the compression retriever</span>
</span></span><span style=display:flex><span>compression_retriever <span style=color:#666>=</span> ContextualCompressionRetriever(
</span></span><span style=display:flex><span>    base_compressor<span style=color:#666>=</span>compressor, <span style=color:#408080;font-style:italic># llm chain extractor</span>
</span></span><span style=display:flex><span>    base_retriever<span style=color:#666>=</span>vectordb<span style=color:#666>.</span>as_retriever() <span style=color:#408080;font-style:italic># vector database retriever</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># compress the source documents</span>
</span></span><span style=display:flex><span>question <span style=color:#666>=</span> <span style=color:#ba2121>&#34;What is the main topic of second lecture?&#34;</span>
</span></span><span style=display:flex><span>compressed_docs <span style=color:#666>=</span> compression_retriever<span style=color:#666>.</span>get_relevant_documents(question)
</span></span><span style=display:flex><span>pretty_print_docs(compressed_docs)
</span></span></code></pre></div><h2 id=question-answering>Question Answering
<a class=anchor href=#question-answering>#</a></h2><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-03-04-22-03-35.png alt></p><ol><li>Multiple relevant documents have been retrieved from the vector store</li><li>Potentially compress the relevant splits to fit into the LLM context. The system will generate the necessary background information (System Prompt) and keep the user&rsquo;s question (Human Question), and then integrate all the information into a complete context input.</li><li>Send the information along with our question to an LLM to select and format an answer</li></ol><h3 id=retrievalqa-chain>RetrievalQA Chain
<a class=anchor href=#retrievalqa-chain>#</a></h3><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-03-04-22-09-33.png alt></p><p>We need to use Langchain to combine the prompts into the desired format and pass them to the large language model to generate the desired reply. This solution is better than the traditional method of inputting the question into the large language model because:</p><ul><li><strong>Enhance the accuracy of the answer</strong>: By combining the retrieval results with the generation ability of the large language model, the relevance and accuracy of the answer are greatly improved.</li><li><strong>Support real-time update of the knowledge base</strong>: The retrieval process depends on the data in the vector store, which can be updated in real time according to needs, ensuring that the answer reflects the latest knowledge.</li><li><strong>Reduce the memory burden of the model</strong>: By using the information in the knowledge base as the input context, the dependence on the model&rsquo;s internal parameters for storing knowledge is reduced.</li></ul><p>In addition to the RetrievalQA Chain, there are other methods, such as <code>Map_reduce</code>, <code>Refine</code> and <code>Map_rerank</code>.</p><h3 id=map_reduce>Map_reduce
<a class=anchor href=#map_reduce>#</a></h3><p><code>Map_reduce</code> method divides the documents into multiple chunks, and then passes each chunk to the language model (LLM) to generate an independent answer. After that, all the generated answers will be merged into the final answer, and the merging process (reduce) may include summarizing, voting, etc.</p><p>This method is suitable for <strong>the large amount of documents parallel processing</strong>, also with quick response.</p><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-03-04-22-15-16.png alt></p><h3 id=refine>Refine
<a class=anchor href=#refine>#</a></h3><p>Refine method generates an initial answer from the first document chunk, and then processes each subsequent document one by one. Each block will supplement or correct the existing answer, and finally <strong>obtain an optimized and improved answer</strong> after all chunks are processed.</p><p>This method is suitable for the most quality answer.</p><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-03-04-22-18-13.png alt></p><h3 id=map_rerank>Map_rerank
<a class=anchor href=#map_rerank>#</a></h3><p>Map_rerank divides the documents into multiple chunks, and then generates an independent answer for each chunk. The scoring is based on the relevance and quality of the answer. Finally, the answer with the highest score will be selected as the final output.</p><p>This method is suitable for the most match answer rather than combine with all the information.</p><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-03-04-22-18-58.png alt></p><h3 id=practice-6>Practice
<a class=anchor href=#practice-6>#</a></h3><p>You can check the entire code <a href=https://github.com/timerring/rag101/tree/main/question_answering target=_blank rel="nofollow noopener">here<span style=white-space:nowrap><svg width=".7em" height=".7em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a>.</p><p>First, install the lib:</p><div class=highlight><pre tabindex=0 style=-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install pyjwt
</span></span></code></pre></div><p>You can use the demo to check the model performance.</p><div class=highlight><pre tabindex=0 style=-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain_community.chat_models</span> <span style=color:green;font-weight:700>import</span> ChatZhipuAI
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain_core.messages</span> <span style=color:green;font-weight:700>import</span> AIMessage, HumanMessage, SystemMessage
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>chat <span style=color:#666>=</span> ChatZhipuAI(
</span></span><span style=display:flex><span>    model<span style=color:#666>=</span><span style=color:#ba2121>&#34;glm-4-flash&#34;</span>,
</span></span><span style=display:flex><span>    temperature<span style=color:#666>=</span><span style=color:#666>0.5</span>, <span style=color:#408080;font-style:italic># the temperature of the model</span>
</span></span><span style=display:flex><span>    api_key<span style=color:#666>=</span><span style=color:#ba2121>&#34;your_api_key&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>messages <span style=color:#666>=</span> [
</span></span><span style=display:flex><span>    AIMessage(content<span style=color:#666>=</span><span style=color:#ba2121>&#34;Hi.&#34;</span>),  <span style=color:#408080;font-style:italic># AI generated message</span>
</span></span><span style=display:flex><span>    SystemMessage(content<span style=color:#666>=</span><span style=color:#ba2121>&#34;Your role is a poet.&#34;</span>),  <span style=color:#408080;font-style:italic># the role of the model</span>
</span></span><span style=display:flex><span>    HumanMessage(content<span style=color:#666>=</span><span style=color:#ba2121>&#34;Write a short poem about AI in four lines.&#34;</span>),  <span style=color:#408080;font-style:italic># the message from the user</span>
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># get the answer from the model</span>
</span></span><span style=display:flex><span>response <span style=color:#666>=</span> chat<span style=color:#666>.</span>invoke(messages)
</span></span><span style=display:flex><span><span style=color:green>print</span>(response<span style=color:#666>.</span>content)
</span></span></code></pre></div><p>Then we can use the <code>RetrievalQA chain</code> to get the answer from the model.</p><div class=highlight><pre tabindex=0 style=-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain_community.chat_models</span> <span style=color:green;font-weight:700>import</span> ChatZhipuAI
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain_core.messages</span> <span style=color:green;font-weight:700>import</span> AIMessage, HumanMessage, SystemMessage
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain_community.embeddings</span> <span style=color:green;font-weight:700>import</span> ZhipuAIEmbeddings
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain_chroma</span> <span style=color:green;font-weight:700>import</span> Chroma
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain_community.document_loaders</span> <span style=color:green;font-weight:700>import</span> WebBaseLoader
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain.text_splitter</span> <span style=color:green;font-weight:700>import</span> RecursiveCharacterTextSplitter
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain.chains</span> <span style=color:green;font-weight:700>import</span> RetrievalQA
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain.prompts</span> <span style=color:green;font-weight:700>import</span> PromptTemplate <span style=color:#408080;font-style:italic># You can also import the PromptTemplate</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loader <span style=color:#666>=</span> WebBaseLoader(<span style=color:#ba2121>&#34;https://en.d2l.ai/&#34;</span>)
</span></span><span style=display:flex><span>docs <span style=color:#666>=</span> loader<span style=color:#666>.</span>load()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>text_splitter <span style=color:#666>=</span> RecursiveCharacterTextSplitter(
</span></span><span style=display:flex><span>    chunk_size <span style=color:#666>=</span> <span style=color:#666>1500</span>,
</span></span><span style=display:flex><span>    chunk_overlap <span style=color:#666>=</span> <span style=color:#666>150</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>splits <span style=color:#666>=</span> text_splitter<span style=color:#666>.</span>split_documents(docs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>persist_directory <span style=color:#666>=</span> <span style=color:#ba2121>&#39;.&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># initialize the embeddings</span>
</span></span><span style=display:flex><span>embeddings <span style=color:#666>=</span> ZhipuAIEmbeddings(
</span></span><span style=display:flex><span>    model<span style=color:#666>=</span><span style=color:#ba2121>&#34;embedding-3&#34;</span>,
</span></span><span style=display:flex><span>    api_key<span style=color:#666>=</span><span style=color:#ba2121>&#34;your_api_key&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># initialize the vector database</span>
</span></span><span style=display:flex><span>vectordb <span style=color:#666>=</span> Chroma<span style=color:#666>.</span>from_documents(
</span></span><span style=display:flex><span>    documents<span style=color:#666>=</span>splits,
</span></span><span style=display:flex><span>    embedding<span style=color:#666>=</span>embeddings,
</span></span><span style=display:flex><span>    persist_directory<span style=color:#666>=</span>persist_directory
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>chat <span style=color:#666>=</span> ChatZhipuAI(
</span></span><span style=display:flex><span>    model<span style=color:#666>=</span><span style=color:#ba2121>&#34;glm-4-flash&#34;</span>,
</span></span><span style=display:flex><span>    temperature<span style=color:#666>=</span><span style=color:#666>0.5</span>,
</span></span><span style=display:flex><span>    api_key <span style=color:#666>=</span>  <span style=color:#ba2121>&#34;your_api_key&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># Now you can ask the question about the web to the model</span>
</span></span><span style=display:flex><span>question <span style=color:#666>=</span> <span style=color:#ba2121>&#34;What is this book about?&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># You can also create a prompt template</span>
</span></span><span style=display:flex><span>template <span style=color:#666>=</span> <span style=color:#ba2121>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#ba2121>Please answer the question based on the following context.
</span></span></span><span style=display:flex><span><span style=color:#ba2121>If you don&#39;t know the answer, just say you don&#39;t know, don&#39;t try to make up an answer.
</span></span></span><span style=display:flex><span><span style=color:#ba2121>Answer in at most three sentences. Please answer as concisely as possible. Finally, always say &#34;Thank you for asking!&#34;
</span></span></span><span style=display:flex><span><span style=color:#ba2121>Context: </span><span style=color:#b68;font-weight:700>{context}</span><span style=color:#ba2121>
</span></span></span><span style=display:flex><span><span style=color:#ba2121>Question: </span><span style=color:#b68;font-weight:700>{question}</span><span style=color:#ba2121>
</span></span></span><span style=display:flex><span><span style=color:#ba2121>Helpful answer:
</span></span></span><span style=display:flex><span><span style=color:#ba2121>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>QA_CHAIN_PROMPT <span style=color:#666>=</span> PromptTemplate<span style=color:#666>.</span>from_template(template)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>qa_chain <span style=color:#666>=</span> RetrievalQA<span style=color:#666>.</span>from_chain_type(
</span></span><span style=display:flex><span>    chat,
</span></span><span style=display:flex><span>    retriever<span style=color:#666>=</span>vectordb<span style=color:#666>.</span>as_retriever(),
</span></span><span style=display:flex><span>    return_source_documents<span style=color:#666>=</span><span style=color:green;font-weight:700>True</span>, <span style=color:#408080;font-style:italic># Return the source documents(optional)</span>
</span></span><span style=display:flex><span>    chain_type_kwargs<span style=color:#666>=</span>{<span style=color:#ba2121>&#34;prompt&#34;</span>: QA_CHAIN_PROMPT} <span style=color:#408080;font-style:italic># Add the prompt template to the chain</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>result <span style=color:#666>=</span> qa_chain({<span style=color:#ba2121>&#34;query&#34;</span>: question})
</span></span><span style=display:flex><span><span style=color:green>print</span>(result[<span style=color:#ba2121>&#34;result&#34;</span>])
</span></span><span style=display:flex><span><span style=color:green>print</span>(result[<span style=color:#ba2121>&#34;source_documents&#34;</span>][<span style=color:#666>0</span>]) <span style=color:#408080;font-style:italic># If you set return_source_documents to True, you can get the source documents</span>
</span></span></code></pre></div><h2 id=conversational-retrieval-chain>Conversational Retrieval Chain
<a class=anchor href=#conversational-retrieval-chain>#</a></h2><p>The whole process of RAG is as follows:</p><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-03-04-23-37-20.png alt></p><p>Conversational Retrieval Chain is a technical architecture that combines dialogue history and intelligent retrieval capabilities.</p><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-03-04-23-40-11.png alt></p><ol><li>Chat History: The system will record the user&rsquo;s dialogue context as an important input for subsequent question processing.</li><li>Question: The user&rsquo;s question is sent to the retrieval module.</li><li>Retriever: The system retrieves the content related to the question from the vector database through the retriever.</li><li>System & Human: The system integrates the user&rsquo;s question and the extracted relevant information into the Prompt, providing structured input to the language model.</li><li>LLM: The language model generates the answer based on the context, and then returns the answer to the user.</li></ol><h3 id=memory>Memory
<a class=anchor href=#memory>#</a></h3><p><code>ConversationBufferMemory</code> is a memory module in the LangChain framework, which is used to manage the dialogue history. Its main function is to store the dialogue content between users and AI in the form of a buffer, and then return these records when needed, so that the model can generate responses in a consistent context.</p><p>The demo of it is as follows:</p><div class=highlight><pre tabindex=0 style=-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain.memory</span> <span style=color:green;font-weight:700>import</span> ConversationBufferMemory
</span></span><span style=display:flex><span>memory <span style=color:#666>=</span> ConversationBufferMemory(
</span></span><span style=display:flex><span>    memory_key<span style=color:#666>=</span><span style=color:#ba2121>&#34;chat_history&#34;</span>, <span style=color:#408080;font-style:italic># This key can be referenced in other modules (such as chains or tools).</span>
</span></span><span style=display:flex><span>    return_messages<span style=color:#666>=</span><span style=color:green;font-weight:700>True</span> <span style=color:#408080;font-style:italic># whether to return the messages in list, otherwise return the messages in block.</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Besides, we also need the corresponding RA module. Then we can test the memory.</p><div class=highlight><pre tabindex=0 style=-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain.chains</span> <span style=color:green;font-weight:700>import</span> ConversationalRetrievalChain
</span></span><span style=display:flex><span>retriever<span style=color:#666>=</span>vectordb<span style=color:#666>.</span>as_retriever()
</span></span><span style=display:flex><span>qa <span style=color:#666>=</span> ConversationalRetrievalChain<span style=color:#666>.</span>from_llm(
</span></span><span style=display:flex><span>    chat,
</span></span><span style=display:flex><span>    retriever<span style=color:#666>=</span>retriever,
</span></span><span style=display:flex><span>    memory<span style=color:#666>=</span>memory
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>question <span style=color:#666>=</span> <span style=color:#ba2121>&#34;What is the main topic of this book?&#34;</span>
</span></span><span style=display:flex><span>result <span style=color:#666>=</span> qa<span style=color:#666>.</span>invoke({<span style=color:#ba2121>&#34;question&#34;</span>: question})
</span></span><span style=display:flex><span><span style=color:green>print</span>(result[<span style=color:#ba2121>&#39;answer&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>question <span style=color:#666>=</span> <span style=color:#ba2121>&#34;What is my last question?&#34;</span>
</span></span><span style=display:flex><span>result <span style=color:#666>=</span> qa<span style=color:#666>.</span>invoke({<span style=color:#ba2121>&#34;question&#34;</span>: question})
</span></span><span style=display:flex><span><span style=color:green>print</span>(result[<span style=color:#ba2121>&#39;answer&#39;</span>])
</span></span></code></pre></div><h3 id=practice-7>Practice
<a class=anchor href=#practice-7>#</a></h3><p>You can check the entire code <a href=https://github.com/timerring/rag101/blob/main/conversational_retrieval_chain/conversational_retrieval_chain.py target=_blank rel="nofollow noopener">here<span style=white-space:nowrap><svg width=".7em" height=".7em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a>.</p><p>The best practice is as follows:</p><div class=highlight><pre tabindex=0 style=-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain_community.chat_models</span> <span style=color:green;font-weight:700>import</span> ChatZhipuAI
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain_core.messages</span> <span style=color:green;font-weight:700>import</span> AIMessage, HumanMessage, SystemMessage
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain_community.embeddings</span> <span style=color:green;font-weight:700>import</span> ZhipuAIEmbeddings
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain_chroma</span> <span style=color:green;font-weight:700>import</span> Chroma
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain_community.document_loaders</span> <span style=color:green;font-weight:700>import</span> WebBaseLoader
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain.text_splitter</span> <span style=color:green;font-weight:700>import</span> RecursiveCharacterTextSplitter
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain.memory</span> <span style=color:green;font-weight:700>import</span> ConversationBufferMemory
</span></span><span style=display:flex><span><span style=color:green;font-weight:700>from</span> <span style=color:#00f;font-weight:700>langchain.chains</span> <span style=color:green;font-weight:700>import</span> ConversationalRetrievalChain
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loader <span style=color:#666>=</span> WebBaseLoader(<span style=color:#ba2121>&#34;https://en.d2l.ai/&#34;</span>)
</span></span><span style=display:flex><span>docs <span style=color:#666>=</span> loader<span style=color:#666>.</span>load()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>text_splitter <span style=color:#666>=</span> RecursiveCharacterTextSplitter(
</span></span><span style=display:flex><span>    chunk_size <span style=color:#666>=</span> <span style=color:#666>1500</span>,
</span></span><span style=display:flex><span>    chunk_overlap <span style=color:#666>=</span> <span style=color:#666>150</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>splits <span style=color:#666>=</span> text_splitter<span style=color:#666>.</span>split_documents(docs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>persist_directory <span style=color:#666>=</span> <span style=color:#ba2121>&#39;.&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># initialize the embeddings</span>
</span></span><span style=display:flex><span>embeddings <span style=color:#666>=</span> ZhipuAIEmbeddings(
</span></span><span style=display:flex><span>    model<span style=color:#666>=</span><span style=color:#ba2121>&#34;embedding-3&#34;</span>,
</span></span><span style=display:flex><span>    api_key<span style=color:#666>=</span><span style=color:#ba2121>&#34;your_api_key&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># initialize the vector database</span>
</span></span><span style=display:flex><span>vectordb <span style=color:#666>=</span> Chroma<span style=color:#666>.</span>from_documents(
</span></span><span style=display:flex><span>    documents<span style=color:#666>=</span>splits,
</span></span><span style=display:flex><span>    embedding<span style=color:#666>=</span>embeddings,
</span></span><span style=display:flex><span>    persist_directory<span style=color:#666>=</span>persist_directory
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>chat <span style=color:#666>=</span> ChatZhipuAI(
</span></span><span style=display:flex><span>    model<span style=color:#666>=</span><span style=color:#ba2121>&#34;glm-4-flash&#34;</span>,
</span></span><span style=display:flex><span>    temperature<span style=color:#666>=</span><span style=color:#666>0.5</span>,
</span></span><span style=display:flex><span>    api_key <span style=color:#666>=</span>  <span style=color:#ba2121>&#34;your_api_key&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># initialize the memory</span>
</span></span><span style=display:flex><span>memory <span style=color:#666>=</span> ConversationBufferMemory(
</span></span><span style=display:flex><span>    memory_key<span style=color:#666>=</span><span style=color:#ba2121>&#34;chat_history&#34;</span>,
</span></span><span style=display:flex><span>    return_messages<span style=color:#666>=</span><span style=color:green;font-weight:700>True</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># create the ConversationalRetrievalChain</span>
</span></span><span style=display:flex><span>retriever <span style=color:#666>=</span> vectordb<span style=color:#666>.</span>as_retriever()
</span></span><span style=display:flex><span>qa <span style=color:#666>=</span> ConversationalRetrievalChain<span style=color:#666>.</span>from_llm(
</span></span><span style=display:flex><span>    chat,
</span></span><span style=display:flex><span>    retriever<span style=color:#666>=</span>retriever,
</span></span><span style=display:flex><span>    memory<span style=color:#666>=</span>memory
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># First question</span>
</span></span><span style=display:flex><span>question <span style=color:#666>=</span> <span style=color:#ba2121>&#34;What is the main topic of this book?&#34;</span>
</span></span><span style=display:flex><span>result <span style=color:#666>=</span> qa<span style=color:#666>.</span>invoke({<span style=color:#ba2121>&#34;question&#34;</span>: question})
</span></span><span style=display:flex><span><span style=color:green>print</span>(result[<span style=color:#ba2121>&#39;answer&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#408080;font-style:italic># Second question</span>
</span></span><span style=display:flex><span>question <span style=color:#666>=</span> <span style=color:#ba2121>&#34;Can you tell me more about it?&#34;</span>
</span></span><span style=display:flex><span>result <span style=color:#666>=</span> qa<span style=color:#666>.</span>invoke({<span style=color:#ba2121>&#34;question&#34;</span>: question})
</span></span><span style=display:flex><span><span style=color:green>print</span>(result[<span style=color:#ba2121>&#39;answer&#39;</span>])
</span></span></code></pre></div><hr><div class=post-nav><a href="https://blog.timerring.com/posts/random-walks/?utm_source=nav">&lt;&lt; prev | Random Walks</a>
<a onclick=goToRandomPost() style=cursor:pointer>Continue strolling
</a><a href="https://blog.timerring.com/posts/a-shopping-experience-in-pangdonglai-supermarket/?utm_source=nav">A Shopping... ｜ next >></a></div><hr>If you find this blog useful and want to support my blog, need my skill for something, or have a coffee chat with me, feel free to:<div class=support><a href=https://ko-fi.com/polls/What-is-the-custom-topic-for-the-next-month-C0C219LHQJ class=book-btn target=_blank>Subscribe to participate in blog topics and custom services
</a><a href=https://ko-fi.com/timerring class=book-btn target=_blank>Buy me a coffee</a></div><div style=margin-bottom:8px></div><script>function goToRandomPost(){const e=["/posts/0516-add-live-photo-effect-to-hugo/?utm_source=random","/posts/the-practice-of-resolving-domain/?utm_source=random","/posts/hidden-markov-models/?utm_source=random","/posts/markov-chains/?utm_source=random","/posts/markov-processes/?utm_source=random","/posts/0418-find-the-formula-rendering-issue-of-hugo/?utm_source=random","/posts/multi-platform-builds-docker/?utm_source=random","/posts/0404-The-invitation-only-mechanism-from-clubhouse-to-manus/?utm_source=random","/posts/how-to-use-git-submodule/?utm_source=random","/posts/live-streaming-infra-and-protocols/?utm_source=random","/posts/implement-danmaku-rendering-algorithm-from-scratch/?utm_source=random","/posts/is-there-an-rss-renaissance-in-the-ai-era/?utm_source=random","/posts/stock-prices/?utm_source=random","/posts/possion-processes/?utm_source=random","/posts/docker-with-gpu/?utm_source=random","/posts/stochastic-processes/?utm_source=random","/posts/some-useful-tools/?utm_source=random","/posts/a-shopping-experience-in-pangdonglai-supermarket/?utm_source=random","/posts/langchain-and-rag-best-practices/?utm_source=random","/posts/random-walks/?utm_source=random","/posts/sequences-of-random-variables/?utm_source=random","/posts/parameter-estimation/?utm_source=random","/posts/two-random-variables/?utm_source=random","/posts/further-understanding-of-proc/?utm_source=random","/posts/the-iftop/?utm_source=random","/posts/some-useful-commands-to-share/?utm_source=random","/posts/random-variables/?utm_source=random","/posts/repeated-trials/?utm_source=random","/posts/probability-and-its-axioms/?utm_source=random","/posts/reflections-on-trending-topics/?utm_source=random","/posts/cpu-can-only-see-the-threads/?utm_source=random","/posts/go-common-test/?utm_source=random","/posts/go-concurrency-and-parallelism/?utm_source=random","/posts/go-cheatsheet/?utm_source=random","/posts/video-technology-101/?utm_source=random","/posts/the-main-kind-of-message-queue/?utm_source=random","/posts/the-method-to-manage-traffic/?utm_source=random","/posts/the-different-kind-of-api-design/?utm_source=random","/posts/the-encode-and-decode-in-python/?utm_source=random","/posts/about-the-systemd/?utm_source=random","/posts/the-tips-about-dockerfile/?utm_source=random","/posts/docker-cheatsheet/?utm_source=random","/posts/docker-101/?utm_source=random","/posts/the-instance-class-static-magic-method-in-python/?utm_source=random","/posts/the-review-and-plan-for-bilive/?utm_source=random","/posts/the-overview-of-security/?utm_source=random","/posts/how-to-publish-your-code-as-a-pip-module/?utm_source=random","/posts/some-good-things-to-share-about-the-packages/?utm_source=random","/posts/introduction-to-the-http-and-https-protocol/?utm_source=random","/posts/mail-service-and-protocol/?utm_source=random","/posts/understanding-clash-through-configuration/?utm_source=random","/posts/thinking-about-advertisement-from-an-open-source-perspective/?utm_source=random","/posts/a-brief-introduction-to-dns/?utm_source=random","/posts/real-computer-network/?utm_source=random","/posts/python-generator-iterator-and-decorator/?utm_source=random","/posts/python-underlying-mechanism/?utm_source=random","/posts/python-files-exceptions-and-modules/?utm_source=random","/posts/python-object-oriented-programming/?utm_source=random","/posts/python-cheatsheet/?utm_source=random","/posts/python-function-parameter/?utm_source=random","/posts/python-control-structure/?utm_source=random","/posts/python-composite-data-type/?utm_source=random","/posts/python-basic-data-type/?utm_source=random","/posts/python-basic-syntax-elements/?utm_source=random","/posts/deploy-github-pages-with-gpg-signing/?utm_source=random","/posts/gpg-101/?utm_source=random","/posts/housewarming-2024/?utm_source=random"],t=Math.floor(Math.random()*e.length);window.location.href=e[t]}</script></article><div class=book-comments><div class="book-tabs-content markdown-inner"><div id=tcomment></div><script src=https://giscus.app/client.js data-repo=timerring/blog data-repo-id=R_kgDONeZYZg data-category=Announcements data-category-id=DIC_kwDONeZYZs4ClRWs data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=en crossorigin=anonymous async></script></div></div><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script><div class=busuanzi-footer><p style=text-align:center>&copy; 2022 - present <a href=https://github.com/timerring>timerring</a> | PV: <span id=busuanzi_value_site_pv></span> | UV: <span id=busuanzi_value_site_uv></span></p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a><ul><li><a href=#langchain>LangChain</a><ul><li><a href=#prompt>Prompt</a></li><li><a href=#models>Models</a></li><li><a href=#indexes>Indexes</a></li><li><a href=#chains>Chains</a></li><li><a href=#agents>Agents</a></li></ul></li><li><a href=#rag-process>RAG process</a><ul><li><a href=#vector-store-loading>Vector Store Loading</a></li><li><a href=#retrieval-augmented-generation>Retrieval-Augmented Generation</a></li></ul></li></ul></li><li><a href=#loaders>Loaders</a><ul><li><a href=#pdf>PDF</a></li><li><a href=#web-base-loader>Web Base Loader</a></li></ul></li><li><a href=#splitters>Splitters</a><ul><li><a href=#why-split>Why split?</a></li><li><a href=#type-of-splitters>Type of splitters</a></li><li><a href=#example-charactertextsplitter-and-recursivecharactertextsplitter>Example CharacterTextSplitter and RecursiveCharacterTextSplitter</a></li></ul></li><li><a href=#vectorstores-and-embeddings>Vectorstores and Embeddings</a><ul><li><a href=#embeddings>Embeddings</a></li><li><a href=#vector-stores>Vector Stores</a></li><li><a href=#practice>Practice</a><ul><li><a href=#embeddings-1>Embeddings</a></li><li><a href=#vector-stores-1>Vector Stores</a></li></ul></li></ul></li><li><a href=#retrieval>Retrieval</a><ul><li><a href=#similarity-search>Similarity Search</a><ul><li><a href=#practice-1>Practice</a></li></ul></li><li><a href=#maximum-marginal-relevance-mmr>Maximum Marginal Relevance (MMR)</a><ul><li><a href=#practice-2>Practice</a></li></ul></li><li><a href=#metadata>Metadata</a><ul><li><a href=#practice-3>Practice</a></li></ul></li><li><a href=#llm-aided-retrieval>LLM Aided Retrieval</a><ul><li><a href=#selfqueryretriever>SelfQueryRetriever</a></li><li><a href=#practice-4>Practice</a></li></ul></li><li><a href=#compression>Compression</a><ul><li><a href=#practice-5>Practice</a></li></ul></li></ul></li><li><a href=#question-answering>Question Answering</a><ul><li><a href=#retrievalqa-chain>RetrievalQA Chain</a></li><li><a href=#map_reduce>Map_reduce</a></li><li><a href=#refine>Refine</a></li><li><a href=#map_rerank>Map_rerank</a></li><li><a href=#practice-6>Practice</a></li></ul></li><li><a href=#conversational-retrieval-chain>Conversational Retrieval Chain</a><ul><li><a href=#memory>Memory</a></li><li><a href=#practice-7>Practice</a></li></ul></li></ul></nav><style>.book-toc .book-toc-content a{color:var(--body-font-color)}</style></div></aside></main></body></html>