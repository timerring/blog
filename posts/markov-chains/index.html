<!doctype html><html lang=en dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="This is the notes for the lecture of Markov Chains."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta name=keywords content="MARKOV CHAIN,MARKOV PROPERTY,Example,1-D RANDOM WALK,BRANCHING PROCESSES,NUCLEAR CHAIN REACTION,WAITING LINES/QUEUES,TRANSITION PROBABILITIES,HOMOGENEOUS MARKOV CHAIN,STOCHASTIC MATRIX,EXAMPLE - BINARY COMMUNICATION CHANNEL,EXAMPLE - Weather Forecasting,EXAMPLE – Mood of Gary,EXAMPLE - Transforming a (non-Markov) Process into a Markov Chain,EXAMPLE - BACK TO RANDOM WALK,RANDOM WALK WITH ABSORBING BARRIERS in the edge nodes,RANDOM WALK WITH REFLECTING BARRIERS,CYCLIC RANDOM WALKS,EHRENFEST&amp;rsquo;S DIFFUSION MODEL (NON-UNIFORM RANDOM WALK),SUCCESS RUNS (RANDOM WALK),RANDOM OCCUPANCY,IMBEDDED MARKOV CHAINS,CHAPMAN KOLMOGOROV EQUATION,N-STEP TRANSITION PROBABILITIES,Example - Weather Forecasting,a two-state Markov chain:,Example - Transforming a (non-Markov) Process into a Markov Chain (cont.) Given that it rained on Monday and Tuesday,what is the probability that it will rain on Thursday?,SOLVING FOR P(N),The calculation process,EXAMPLE – EIGENVALUES AND EIGENVECTORS,Example - BINARY COMMUNICATION CHANNEL,CLOSED SETS,IRREDUCIBLE MARKOV CHAIN,PERSISTENT (RECURRENT) STATE,periodicity,Definition of Period,Examples,Positive Recurrence AND Ergodicity,Examples,KEY FACTS FOR A MARKOV CHAIN,Examples,ONE-DIMENSIONAL RANDOM WALK,2-D RANDOM WALK,3 - D RANDOM WALK,STATIONARY DISTRIBUTIONS,Example,STEADY - STATE MATRIX,VECTOR/MATRIX NOTATION: MATRIX LIMIT,VECTOR/MATRIX NOTATION: EIGENVECTOR,Example: Weather forecast again,Example,ERGODICITY,Example,FUNCTION&amp;rsquo;S ERGODIC AVERAGE,ERGODICITY IN PERIODIC MARKOV CHAINS,Example An perlodic Irreducible Markov chain,REDUCIBLE MARKOV CHAINS,Example - Reducible Markov chain"><meta property="og:url" content="https://blog.timerring.com/posts/markov-chains/"><meta property="og:site_name" content="timerring"><meta property="og:title" content="Markov Chains"><meta property="og:description" content="This is the notes for the lecture of Markov Chains."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-20T18:38:25+08:00"><meta property="article:tag" content="Random Process"><meta property="article:tag" content="Math"><meta itemprop=name content="Markov Chains"><meta itemprop=description content="This is the notes for the lecture of Markov Chains."><meta itemprop=datePublished content="2025-04-20T18:38:25+08:00"><meta itemprop=dateModified content="2025-04-20T18:38:25+08:00"><meta itemprop=wordCount content="5488"><meta itemprop=image content="https://blog.timerring.com/favicon.png"><meta itemprop=keywords content="Random Process,Math"><meta name=twitter:image content="https://blog.timerring.com/favicon.png"><meta name=twitter:card content="summary"><meta name=twitter:title content="Markov Chains"><meta name=twitter:description content="This is the notes for the lecture of Markov Chains."><meta name=twitter:site content="@imjohnhowe"><link rel=canonical href=https://blog.timerring.com/posts/markov-chains/><title>Markov Chains | timerring</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.b2496f568c7d479e460b580e0e81d4b97841f83539eba8c770dc9f8e1aa7682e.css integrity="sha256-sklvVox9R55GC1gODoHUuXhB+DU566jHcNyfjhqnaC4=" crossorigin=anonymous><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.742e4ee0b666ffe79a755355899bffc21e557a3b432abb81e82aa2195ae569ad.js integrity="sha256-dC5O4LZm/+eadVNViZv/wh5VejtDKruB6CqiGVrlaa0=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script defer>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1}]})})</script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/favicon.png alt=Logo><span>timerring</span></a></h2><hr><p>Rhythmic trend.</p><div class=book-search><div class=gcse-search></div></div><script async src="https://cse.google.com/cse.js?cx=40782701766144f96"></script><ul><li><a href=/>Dashboard</a></li><li><a href=/posts/>Blogs</a></li><li><a href=/categories/weekly/>Newsletter</a></li><li><a href=/index.xml>RSS</a></li><li><a href=/friends/>Friends</a></li><li><a href=/about/>About</a></li></ul><script>function goToRandomPost(){const e=["/posts/hidden-markov-models/?utm_source=random","/posts/markov-chains/?utm_source=random","/posts/markov-processes/?utm_source=random","/posts/stock-prices/?utm_source=random","/posts/possion-processes/?utm_source=random","/posts/random-walks/?utm_source=random","/posts/0418-find-the-formula-rendering-issue-of-hugo/?utm_source=random","/posts/stochastic-processes/?utm_source=random","/posts/multi-platform-builds-docker/?utm_source=random","/posts/0404-The-invitation-only-mechanism-from-clubhouse-to-manus/?utm_source=random","/posts/how-to-use-git-submodule/?utm_source=random","/posts/live-streaming-infra-and-protocols/?utm_source=random","/posts/implement-danmaku-rendering-algorithm-from-scratch/?utm_source=random","/posts/is-there-an-rss-renaissance-in-the-ai-era/?utm_source=random","/posts/docker-with-gpu/?utm_source=random","/posts/some-useful-tools/?utm_source=random","/posts/a-shopping-experience-in-pangdonglai-supermarket/?utm_source=random","/posts/langchain-and-rag-best-practices/?utm_source=random","/posts/sequences-of-random-variables/?utm_source=random","/posts/random-variables/?utm_source=random","/posts/repeated-trials/?utm_source=random","/posts/probability-and-its-axioms/?utm_source=random","/posts/further-understanding-of-proc/?utm_source=random","/posts/the-iftop/?utm_source=random","/posts/some-useful-commands-to-share/?utm_source=random","/posts/reflections-on-trending-topics/?utm_source=random","/posts/cpu-can-only-see-the-threads/?utm_source=random","/posts/go-common-test/?utm_source=random","/posts/go-concurrency-and-parallelism/?utm_source=random","/posts/go-cheatsheet/?utm_source=random","/posts/video-technology-101/?utm_source=random","/posts/the-main-kind-of-message-queue/?utm_source=random","/posts/the-method-to-manage-traffic/?utm_source=random","/posts/the-different-kind-of-api-design/?utm_source=random","/posts/the-encode-and-decode-in-python/?utm_source=random","/posts/about-the-systemd/?utm_source=random","/posts/the-tips-about-dockerfile/?utm_source=random","/posts/docker-cheatsheet/?utm_source=random","/posts/docker-101/?utm_source=random","/posts/the-instance-class-static-magic-method-in-python/?utm_source=random","/posts/the-review-and-plan-for-bilive/?utm_source=random","/posts/the-overview-of-security/?utm_source=random","/posts/how-to-publish-your-code-as-a-pip-module/?utm_source=random","/posts/some-good-things-to-share-about-the-packages/?utm_source=random","/posts/introduction-to-the-http-and-https-protocol/?utm_source=random","/posts/mail-service-and-protocol/?utm_source=random","/posts/understanding-clash-through-configuration/?utm_source=random","/posts/thinking-about-advertisement-from-an-open-source-perspective/?utm_source=random","/posts/a-brief-introduction-to-dns/?utm_source=random","/posts/real-computer-network/?utm_source=random","/posts/python-generator-iterator-and-decorator/?utm_source=random","/posts/python-underlying-mechanism/?utm_source=random","/posts/python-files-exceptions-and-modules/?utm_source=random","/posts/python-object-oriented-programming/?utm_source=random","/posts/python-cheatsheet/?utm_source=random","/posts/python-function-parameter/?utm_source=random","/posts/python-control-structure/?utm_source=random","/posts/python-composite-data-type/?utm_source=random","/posts/python-basic-data-type/?utm_source=random","/posts/python-basic-syntax-elements/?utm_source=random","/posts/deploy-github-pages-with-gpg-signing/?utm_source=random","/posts/gpg-101/?utm_source=random","/posts/housewarming-2024/?utm_source=random"],t=Math.floor(Math.random()*e.length);window.location.href=e[t]}</script><a class=random onclick=goToRandomPost()>Stroll</a></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>timerring</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#markov-chain>MARKOV CHAIN</a><ul><li><a href=#markov-property>MARKOV PROPERTY</a></li><li><a href=#example>Example</a><ul><li><a href=#1-d-random-walk>1-D RANDOM WALK</a></li><li><a href=#branching-processes>BRANCHING PROCESSES</a></li><li><a href=#nuclear-chain-reaction>NUCLEAR CHAIN REACTION</a></li><li><a href=#waiting-linesqueues>WAITING LINES/QUEUES</a></li></ul></li><li><a href=#transition-probabilities>TRANSITION PROBABILITIES</a></li><li><a href=#homogeneous-markov-chain>HOMOGENEOUS MARKOV CHAIN</a></li><li><a href=#stochastic-matrix>STOCHASTIC MATRIX</a><ul><li><a href=#example---binary-communication-channel>EXAMPLE - BINARY COMMUNICATION CHANNEL</a></li><li><a href=#example---weather-forecasting>EXAMPLE - Weather Forecasting</a></li><li><a href=#example--mood-of-gary>EXAMPLE – Mood of Gary</a></li><li><a href=#example---transforming-a-non-markov-process-into-a-markov-chain>EXAMPLE - Transforming a (non-Markov) Process into a Markov Chain</a></li></ul></li><li><a href=#example---back-to-random-walk>EXAMPLE - BACK TO RANDOM WALK</a><ul><li><a href=#random-walk-with-absorbing-barriers-in-the-edge-nodes>RANDOM WALK WITH ABSORBING BARRIERS in the edge nodes</a></li><li><a href=#random-walk-with-reflecting-barriers>RANDOM WALK WITH REFLECTING BARRIERS</a></li><li><a href=#cyclic-random-walks>CYCLIC RANDOM WALKS</a></li><li><a href=#ehrenfests-diffusion-model-non-uniform-random-walk>EHRENFEST&rsquo;S DIFFUSION MODEL (NON-UNIFORM RANDOM WALK)</a></li><li><a href=#success-runs-random-walk>SUCCESS RUNS (RANDOM WALK)</a></li></ul></li><li><a href=#random-occupancy>RANDOM OCCUPANCY</a></li><li><a href=#imbedded-markov-chains>IMBEDDED MARKOV CHAINS</a></li><li><a href=#chapman-kolmogorov-equation>CHAPMAN KOLMOGOROV EQUATION</a></li><li><a href=#n-step-transition-probabilities>N-STEP TRANSITION PROBABILITIES</a><ul><li><a href=#example---weather-forecasting-a-two-state-markov-chain>Example - Weather Forecasting, a two-state Markov chain:</a></li><li><a href=#example---transforming-a-non-markov-process-into-a-markov-chain-cont-given-that-it-rained-on-monday-and-tuesday-what-is-the-probability-that-it-will-rain-on-thursday>Example - Transforming a (non-Markov) Process into a Markov Chain (cont.) Given that it rained on Monday and Tuesday, what is the probability that it will rain on Thursday?</a></li></ul></li><li><a href=#solving-for-pn>SOLVING FOR P(N)</a><ul><li><a href=#the-calculation-process>The calculation process</a></li><li><a href=#example--eigenvalues-and-eigenvectors>EXAMPLE – EIGENVALUES AND EIGENVECTORS</a></li><li><a href=#example---binary-communication-channel-1>Example - BINARY COMMUNICATION CHANNEL</a></li></ul></li><li><a href=#closed-sets>CLOSED SETS</a></li><li><a href=#irreducible-markov-chain>IRREDUCIBLE MARKOV CHAIN</a></li><li><a href=#persistent-recurrent-state>PERSISTENT (RECURRENT) STATE</a></li><li><a href=#periodicity>periodicity</a><ul><li><a href=#definition-of-period>Definition of Period</a></li><li><a href=#examples>Examples</a></li></ul></li><li><a href=#positive-recurrence-and-ergodicity>Positive Recurrence AND Ergodicity</a><ul><li><a href=#examples-1>Examples</a></li></ul></li><li><a href=#key-facts-for-a-markov-chain>KEY FACTS FOR A MARKOV CHAIN</a></li><li><a href=#examples-2>Examples</a></li><li><a href=#one-dimensional-random-walk>ONE-DIMENSIONAL RANDOM WALK</a></li><li><a href=#2-d-random-walk>2-D RANDOM WALK</a></li><li><a href=#3---d-random-walk>3 - D RANDOM WALK</a></li><li><a href=#stationary-distributions>STATIONARY DISTRIBUTIONS</a><ul><li><a href=#example-1>Example</a></li></ul></li><li><a href=#steady---state-matrix>STEADY - STATE MATRIX</a></li><li><a href=#vectormatrix-notation-matrix-limit>VECTOR/MATRIX NOTATION: MATRIX LIMIT</a></li><li><a href=#vectormatrix-notation-eigenvector>VECTOR/MATRIX NOTATION: EIGENVECTOR</a><ul><li><a href=#example-weather-forecast-again>Example: Weather forecast again</a></li><li><a href=#example-2>Example</a></li></ul></li><li><a href=#ergodicity>ERGODICITY</a><ul><li><a href=#example-3>Example</a></li></ul></li><li><a href=#functions-ergodic-average>FUNCTION&rsquo;S ERGODIC AVERAGE</a></li><li><a href=#ergodicity-in-periodic-markov-chains>ERGODICITY IN PERIODIC MARKOV CHAINS</a></li><li><a href=#example-an-perlodic-irreducible-markov-chain>Example An perlodic Irreducible Markov chain</a></li><li><a href=#reducible-markov-chains>REDUCIBLE MARKOV CHAINS</a><ul><li><a href=#example---reducible-markov-chain>Example - Reducible Markov chain</a></li></ul></li></ul></li></ul></nav><style>.book-toc .book-toc-content a{color:var(--body-font-color)}</style></aside></header><article class=markdown><h1>Markov Chains</h1><h5>April 20, 2025
·
27 min read
<span id=busuanzi_container_page_pv>· Page View: <span id=busuanzi_value_page_pv></span></span></h5><div><a href=/categories/tutorial/ style=color:#000>Tutorial</a></div><div><a href=/tags/random-process/ style=color:#000>Random Process</a> <span style=color:#000>| </span><a href=/tags/math/ style=color:#000>Math</a></div><blockquote class="book-hint info">If you have any questions, feel free to comment below.</blockquote><hr><p>This is the notes for the lecture of Markov Chains.</p><ol><li><strong>OUTLINE</strong><ul><li>Markov Chain<ul><li>Transition Probability</li><li>Examples and applications</li><li>CHAPMAN-KOLMOGOROV EQUATION</li></ul></li><li>Classification of States</li><li>Stationary Distribution and Limiting Prob</li><li>Ergodicity</li></ul></li></ol><h2 id=markov-chain>MARKOV CHAIN
<a class=anchor href=#markov-chain>#</a></h2><p>A special kind of Markov process, where the system can occupy a finite or countably infinite number of states $e_0,e_1,e_2,\cdots$ such that the future evolution of the process, once it is in a given state, <strong>depends only on the present state</strong> and not on how it arrived at that state. It can be discrete or continuous.</p><h3 id=markov-property>MARKOV PROPERTY
<a class=anchor href=#markov-property>#</a></h3><p>A sequence of random variables $(X_t)$ is called a Markov chain if it has the Markov property: $P(X_t = x_t|X_{t - 1}=x_{t - 1},X_{t - 2}=x_{t - 2},\cdots,X_0=x_0)=P(X_t = x_t|X_{t - 1}=x_{t - 1})$</p><ul><li>States are usually labelled ${x_0,x_1,x_2,\cdots}$ and state space can be finite or infinite.</li><li><strong>First order Markov assumption (memoryless)</strong>.</li><li><strong>Homogeneity</strong>: $P(X_{m + n}=x_j|X_m=x_i)=P(X_n=x_j|X_0=x_i)$</li></ul><h3 id=example>Example
<a class=anchor href=#example>#</a></h3><h4 id=1-d-random-walk>1-D RANDOM WALK
<a class=anchor href=#1-d-random-walk>#</a></h4><blockquote><p>Time is slotted.</p></blockquote><p>The walker flips a coin every time slot to decide which way to go: $X(t + 1)=\begin{cases}X(t)+1& \text{with probability }p\X(t)-1& \text{with probability }(1 - p)\end{cases}$</p><ul><li>The sequence of Bernoulli trials $Y_1,Y_2,\cdots$ at each stage are independent, and the accumulated partial sum $S_n=X_1 + X_2+\cdots+X_n$, $S_{n + 1}=S_n+X_{n + 1}$</li><li>$P(S_{n + 1}=j + 1|S_n=j)=p$</li><li>$P(S_{n + 1}=j - 1|S_n=j)=q$</li><li>Conditional probabilities for $S_n$ depend only on the values of $S_n$ and are not affected by the values of $S_1,S_2,\cdots,S_{n - 1}$</li></ul><h4 id=branching-processes>BRANCHING PROCESSES
<a class=anchor href=#branching-processes>#</a></h4><blockquote><p>Time is discrete and represents successive generations.
<img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-20-19-23-30.png alt></p></blockquote><ul><li>Each individual has a unit lifetime, at the end of which it might give birth to one or more offsprings simultaneously.</li><li>The offspring distribution is described by a random variable $\xi$ taking non-negative integer values with corresponding probabilities $p_k = P[\xi=k], k\geq0$</li><li>All individuals behave independently of each other.</li><li>The population size at generation $n$ is denoted by $Z_n$, where $\xi_{i,n}$ are iid. copies of $\xi$, representing the number of offspring of the $i$ - th member of the $(n - 1)$ generation.</li><li>The process ${Z_n,n\geq0}$ is a discrete-time Markov chain on the state space ${0,1,2,3,\cdots}$ where state $0$ is absorbing and all other states are transient.</li></ul><h4 id=nuclear-chain-reaction>NUCLEAR CHAIN REACTION
<a class=anchor href=#nuclear-chain-reaction>#</a></h4><ul><li>A particle such as a neutron scores a hit with probability $p$, creating $m$ new particles, and $q = 1 - p$ represents the probability that it remains inactive with no descendants.</li><li>The only possible number of descendants is zero and $m$ with probabilities $q$ and $p$.</li><li>If $p$ is close to one, the number of particles is likely to increase indefinitely, leading to an explosion, whereas if $p$ is close to zero the process may never start.</li></ul><h4 id=waiting-linesqueues>WAITING LINES/QUEUES
<a class=anchor href=#waiting-linesqueues>#</a></h4><ul><li>Customers (jobs) arrive randomly and wait for service.</li><li>A customer arriving when the server is free receives immediate service; otherwise the customer joins the queue and waits for service.</li><li>The server continues service according to some schedule such as first in, first out.</li><li>Busy period: total duration of uninterrupted service at some $t = t_0$</li></ul><h3 id=transition-probabilities>TRANSITION PROBABILITIES
<a class=anchor href=#transition-probabilities>#</a></h3><p>In a discrete-time Markov chain ${X_n}$ with a finite or infinite set of states $e_1,e_2,\cdots,e_N$</p><ul><li>Let $X_n = X(t_n)$ represent the state of the system at $t=t_n$.</li><li>For $n\geq m\geq0$, the sequence $X_m,X_{m + 1},\cdots,X_n$ represents the evolution of the system.</li><li>Let $p_i(m)=P(X_m = e_i)$ be the probability that at time $t = t_m$ the system is in state $e_i$.</li><li>$p_{ij}(m,n)=P(X_n = e_j|X_m = e_i)$ be the probability that the system goes into state $e_j$ at $t=t_n$ given that it was in state $e_i$ at $t=t_m$ (transition probabilities of the Markov chain from state $e_i$ at $t_m$ to $e_j$ at $t_n$).</li><li>For $m\lt n\lt r$, $P{X_r = e_i,X_n = e_j,X_m = e_k}=P{X_r = e_i|X_n = e_j}P{X_n = e_j|X_m = e_k}P{X_m = e_k}=p_{ji}(n,r)p_{kj}(m,n)p_k(m)$</li></ul><h3 id=homogeneous-markov-chain>HOMOGENEOUS MARKOV CHAIN
<a class=anchor href=#homogeneous-markov-chain>#</a></h3><ul><li>Homogeneous: if $p_{ij}(m,n)$ depends only on the difference $n-m$.</li><li>The transition probabilities are stationary and $P{X_{m + n}=e_j|X_m = e_i}\triangleq p_{ij}(n)=p_{ij}^{(n)}$</li><li>One-step transition probabilities: $p_{ij}=P(X_{n+1}=e_j|X_n = e_i)$ is the basic transition.</li><li>The time duration $Y$ that a homogeneous Markov process spends in a given state (interarrival time) must be memoryless. $P(y>m + n|y>m)=P(y>n)$, y is a geometric random variable.</li></ul><h3 id=stochastic-matrix>STOCHASTIC MATRIX
<a class=anchor href=#stochastic-matrix>#</a></h3><ul><li>Arrange the transition probabilities $p_{ij}(m,n)$ in a matrix form $P(m,n)$
$$
P(m,n)=\begin{pmatrix}p_{11}(m,n)&p_{12}(m,n)&\cdots&p_{1N}(m,n)&\cdots\\p_{21}(m,n)&p_{22}(m,n)&\cdots&\cdots&\cdots\\\vdots&\vdots&\vdots&\vdots&\vdots\\p_{N1}(m,n)&\cdots&\cdots&p_{NN}(m,n)&\cdots\\\cdots&\cdots&\cdots&\cdots&\cdots\end{pmatrix}
$$</li></ul><p>where $\sum_{j}p_{ij}(m,n)=\sum_{j}P[X_n = e_j|X_m = e_i]=1$ (elements of each row sums to 1).</p><p><strong>With the initial distribution $p(m)=P(X_m = e_i)$ for some $m$, $P(m,n)$ completely define the Markov chain.</strong></p><p>For a homogeneous MC,(refer to the definition of homogeneous Markov chain)</p>$$
P=\begin{pmatrix}p_{11}&p_{12}&p_{13}&\cdots&p_{1N}&\cdots\\p_{21}&p_{22}&p_{23}&\cdots&\cdots&\cdots\\p_{31}&p_{32}&p_{23}&\cdots&\cdots&\cdots\\\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\p_{N1}&p_{N2}&p_{N3}&\cdots&p_{NN}&\cdots\\\cdots&\cdots&\cdots&\cdots&\cdots&\cdots\end{pmatrix}
$$<h4 id=example---binary-communication-channel>EXAMPLE - BINARY COMMUNICATION CHANNEL
<a class=anchor href=#example---binary-communication-channel>#</a></h4><p>The channel delivers the input symbol to the output with a certain error probability that may depend on the symbol being transmitted.</p><p>Let $\alpha\lt\frac{1}{2}$ and $\beta\lt\frac{1}{2}$ represent the two kinds of channel error probabilities.</p><ul><li>For a time-invariant channel, these error probabilities remain constant over various transmitted Symbols so that $P{X_{n + 1}=1|X_n = 0}=p_{01}=\alpha$, $P{X_{n + 1}=0|X_n = 1}=p_{10}=\beta$</li><li>The corresponding Markov chain is homogeneous.</li><li>$2\times2$ homogeneous probability transition matrix
$$
P=\begin{pmatrix}P_{00}&P_{01}\\p_{10}&p_{11}\end{pmatrix}=\begin{pmatrix}1-\alpha&\alpha\\\beta&1 - \beta\end{pmatrix}
$$</li></ul><h4 id=example---weather-forecasting>EXAMPLE - Weather Forecasting
<a class=anchor href=#example---weather-forecasting>#</a></h4><p>Suppose that the chance of rain tomorrow depends on previous weather conditions only through whether or not it is raining today and not on past weather conditions.</p><p>Suppose also that if it rains today, then it will rain tomorrow with probability $\alpha$; and if it does not rain today, then it will rain tomorrow with probability $\beta$.</p><ul><li>Let’s denote that the process is in state $0$ (the first state) when it rains and state $1$ (the second state) when it does not.</li><li>$P=\begin{pmatrix}\alpha&amp;1-\alpha\\beta&amp;1 - \beta\end{pmatrix}$</li></ul><h4 id=example--mood-of-gary>EXAMPLE – Mood of Gary
<a class=anchor href=#example--mood-of-gary>#</a></h4><p>On any given day Gary is either cheerful (C), so-so (S), or glum (G).</p><ul><li>If he is cheerful today, then he will be C, S, or G tomorrow with respective probabilities $0.5$, $0.4$, $0.1$.</li><li>If he feels so-so today, then he will be C, S, or G tomorrow with probabilities $0.3$, $0.4$, $0.3$.</li><li>If he is glum today, then he will be C, S, or G tomorrow with probabilities $0.2$, $0.3$, $0.5$.</li></ul><p>Let $X_n$ denote Gary’s mood on the $n$-th day, then ${X_n,n\geq0}$ is a three - state Markov chain (state $0 = C$, state $1 = S$, state $2 = G$) with transition probability matrix</p>$$
P=\begin{pmatrix}0.5&0.4&0.1\\0.3&0.4&0.3\\0.2&0.3&0.5\end{pmatrix}
$$<h4 id=example---transforming-a-non-markov-process-into-a-markov-chain>EXAMPLE - Transforming a (non-Markov) Process into a Markov Chain
<a class=anchor href=#example---transforming-a-non-markov-process-into-a-markov-chain>#</a></h4><p>Suppose that whether or not it rains today depends on previous weather conditions through the last two days.</p><p>Specifically, suppose that if it has rained for the past two days, then it will rain tomorrow with probability $0.7$; if it rained today but not yesterday, then it will rain tomorrow with probability $0.5$; if it rained yesterday but not today, then it will rain tomorrow with probability $0.4$; if it has not rained in the past two days, then it will rain tomorrow with probability $0.2$.</p><p>If we let the state at time $n$ depend only on whether or not it is raining at time $n - 1$, then the preceding model is not a Markov chain. But we can transform this model into a Markov chain by saying that the state at any time is determined by the weather conditions during both that day and the previous day.</p><ul><li>state $0$ if it rained both today and yesterday</li><li>state $1$ if it rained today but not yesterday</li><li>state $2$ if it rained yesterday but not today</li><li>state $3$ if it did not rain either yesterday or today.</li></ul><p>The transition probability matrix</p>$$
P=\begin{pmatrix}0.7&0&0.3&0\\0.5&0&0.5&0\\0&0.4&0&0.6\\0&0.2&0&0.8\end{pmatrix}
$$<p>eg. $p_{02}=0.3$ means the probability changes from state 0 to state 2 is 0.3.</p><h3 id=example---back-to-random-walk>EXAMPLE - BACK TO RANDOM WALK
<a class=anchor href=#example---back-to-random-walk>#</a></h3><p>At each interior state $e_i$ the particle move to:</p><ul><li>the right to $e_{i + 1}$ with probability $p$</li><li>to the left to $e_{i - 1}$ with probability $q$</li><li>remains at $e_i$ with probability $r$.</li></ul><p>Let $n$ represent the location of the particle at time $n$ on a straight line.</p>$$
P=\begin{pmatrix}r_0&p_0&0&0&0&\cdots\\q_1&r_1&p_1&0&0&\cdots\\0&q_2&r_2&p_2&0&\cdots\\0&0&q_3&r_3&p_3&\cdots\\\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\end{pmatrix}
$$<ul><li>With $p_i = p$, $q_i = 1 - p$, $r_i = 0$ for $i>1$, and $r_0 = 1$ → the gambler&rsquo;s ruin problem</li></ul><h4 id=random-walk-with-absorbing-barriers-in-the-edge-nodes>RANDOM WALK WITH ABSORBING BARRIERS in the edge nodes
<a class=anchor href=#random-walk-with-absorbing-barriers-in-the-edge-nodes>#</a></h4><p>Special case: if the number of states in a random walk be finite $(e_0,e_1,\cdots,e_N)$</p><p>From the interior states $e_1,e_2,\cdots,e_{N - 1}$ transitions to the left and right neighbors are with probabilities $q$ and $p$ respectively, while no transition is possible from $e_0$ and $e_N$ to any other state.</p>$$
P=\begin{pmatrix}1&0&0&0&\cdots&0\\q&0&p&0&0&\vdots\\0&q&0&p&0&\cdots\\\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\0&0&\cdots&\cdots&q&0&p\\0&0&\cdots&\cdots&0&0&1\end{pmatrix}
$$<h4 id=random-walk-with-reflecting-barriers>RANDOM WALK WITH REFLECTING BARRIERS
<a class=anchor href=#random-walk-with-reflecting-barriers>#</a></h4><p>If the two boundaries in the previous example reflect the particle back to the adjacent state instead of absorbing it, with $e_1,e_2,\cdots,e_N$ representing the $N$ states.</p><p>The end reflection probabilities to the right and left are given by $p_{10}=p$ and $p_{N,N - 1}=q$</p>$$
P=\begin{pmatrix}q&p&0&0&\cdots&0\\q&0&p&0&0&\cdots\\0&q&0&p&0&\cdots\\\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\0&0&0&\cdots&0&p\\0&0&0&\cdots&0&q&p\end{pmatrix}
$$<p>A fun game where every time a player loses the game, his adversary returns just the stake amount so that the game is kept alive and it continues forever</p><h4 id=cyclic-random-walks>CYCLIC RANDOM WALKS
<a class=anchor href=#cyclic-random-walks>#</a></h4><p>The two end-boundary states $e_1$ and $e_N$ loop together to form a circle so that $e_N$ has neighbors $e_1$ and $e_{N - 1}$.</p>$$
P=\begin{pmatrix}0&p&0&0&\cdots&0&q\\q&0&p&0&\cdots&0&0\\0&q&0&p&\cdots&0&0\\\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\0&0&0&\cdots&0&p&0\\p&0&0&\cdots&0&q&0\end{pmatrix}
$$<p>If we permit transition between any two states $e_i$ and $e_j$, then since moving $k$ steps to the right on a circle is the same as moving $N - k$ to the left, the transition probability matrix is symmetric.</p>$$
P=\begin{pmatrix}q_0&q_1&q_2&\cdots&q_{N - 1}\\q_{N - 1}&q_0&q_1&\cdots&q_{N - 2}\\q_{N - 2}&q_{N - 1}&q_0&\cdots&q_{N - 3}\\\vdots&\vdots&\vdots&\vdots&\vdots\\q_1&q_2&\cdots&q_{N - 1}&q_0\end{pmatrix}
$$$$
q_{k}=P\left\{x_{n+1}=e_{i + k} | x_{k}=e_{i}\right\}=P\left\{x_{n+1}=e_{i-(N - k)} | x_{n}=e_{i}\right\}
$$<h4 id=ehrenfests-diffusion-model-non-uniform-random-walk>EHRENFEST&rsquo;S DIFFUSION MODEL (NON-UNIFORM RANDOM WALK)
<a class=anchor href=#ehrenfests-diffusion-model-non-uniform-random-walk>#</a></h4><p>Let $N$ represent the combined population of two cities $A$ and $B$.
Suppose migration occurs between the cities one at a time with probability proportional to the population of the city, and let the population of $A$ determine the state of the system.
Then $e_0,e_1,\cdots,e_N$ represent the possible states, and from state $e_k$ at the next step, $A$ can move into either $e_{k + 1}$ or $e_{k - 1}$ with probabilities $k/N$ or $1 - k/N$ respectively</p>$$
P=\begin{pmatrix}0&1&0&0&\cdots&0&0\\p&0&1-p&0&\cdots&0&0\\0&2p&0&1-2p&\cdots&0&0\\0&0&3p&0&1-3p&\cdots&0\\\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\0&0&0&\cdots&0&1-p&0\\0&0&0&\cdots&0&1-p&0\\0&0&0&\cdots&0&0&1\end{pmatrix}
$$<p>A random walk with totally reflective barriers where the probability of a step varies with the position of state. If $k\leq N/2$, the particle is more likely to move to the right, while if $k > N/2$, it is more likely to move to the left.</p><p><strong>The particle has a tendency to move toward the center, an equilibrium distribution.</strong></p><h4 id=success-runs-random-walk>SUCCESS RUNS (RANDOM WALK)
<a class=anchor href=#success-runs-random-walk>#</a></h4><p>$p$ - success, $q$ - failure</p><p>The particle moves from $i$ to $i + 1$ with probability $p$ or <strong>moves back to the origin with probability $q$.</strong></p>$$
p_{ij}= \begin{cases}p & j = i + 1 \\ q & j = 0 \\ 0 & otherwise \end{cases}
$$$$
\prod p_i=p
$$<p>$e_i$ represents the number of uninterrupted successes up to the $n$th trial</p>$$
More generally p_{ij}= \begin{cases}p_i & j = i + 1 \\ q_i & j = 0 \\ 0 & otherwise \end{cases}
$$<p>The probability that the time between two successive returns to zero equals $k$ is given by the product $p_1p_2\cdots p_{k - 1}q$.</p><h3 id=random-occupancy>RANDOM OCCUPANCY
<a class=anchor href=#random-occupancy>#</a></h3><blockquote><p>A sequence of independent trials is conducted where a new ball is placed at random into one of the $N$ cells at each trial. Each cell can hold multiple balls.</p></blockquote><p>Let $e_k$, $k = 0,1,\cdots,N$ represent the state where $k$ cells are occupied and $N - k$ cells are empty.</p><p>At the next trial, the next ball can go into one of the occupied cells ($e_k$ to $e_k$) with probability $k/N$ or an empty cell ($e_k$ to $e_{k+1}$) with probability $(N - k)/N$.</p><p>The transition probabilities:</p>$$
p_{kk}=\frac{k}{N} \quad \text{and} \quad p_{k,k + 1}=1-\frac{k}{N}, \quad p_{kj}=0\ (j\neq k,j\neq k + 1)
$$<p>Birthday statistics problem: What is the minimum number of people required in a random group so that every day is a birthday for someone in that group (with probability $p$)?</p><h3 id=imbedded-markov-chains>IMBEDDED MARKOV CHAINS
<a class=anchor href=#imbedded-markov-chains>#</a></h3><p>An imbedded Markov chain $(X_n)$ that represents the number of customers or jobs waiting for service in a queue at the departure of the $n$th customer.</p><p>Let $X_n$ be the number of customers waiting for service at the departure of the $n$-th customer.</p><p>The number of customers waiting for service at the departure of the $(n + 1)$st customer is</p>$$
x_{n+1}= \begin{cases}x_{n}+y_{n+1}-1 & x_{n} \neq 0\\y_{n+1} & x_{n}=0\end{cases}
$$<p>Let $Y_n$ denote the number of customers arriving at the queue during the service time of the $n$th customer.</p><p>If the queue was empty at the departure of the $n$th customer, then $X_n = 0$, and the next customer to arrive is the $(n + 1)$st. During that service, $Y_{n+1}$ customers arrive, so that $X_{n+1}=Y_{n+1}$; otherwise the number of customers left behind by the $(n + 1)$st customer is $X_n-1+Y_{n+1}$.
The transition probabilities</p>$$
\alpha_{j}=P\left(x_{n+1}=j | x_{n}=i\right)= \begin{cases}P\left(y_{n+1}=j\right) & i = 0\\P\left(y_{n+1}=j - i + 1\right) & i\geq1\end{cases}
$$<p>Note that the underlying stochastic process $X_t$ need not be Markovian, whereas the sequence $X_n = X(t_n)$ generated at the random departure time instants $t_n$ is Markovian.</p><p>The method of imbedded Markov chains converts a non-Markovian problem into Markovian, and the limiting behavior of the imbedded chain can be used to study the underlying process.</p>$$
P=\begin{pmatrix}a_0&a_1&a_2&a_3&\ddots&\ddots\\a_0&a_1&a_2&a_3&\ddots&\ddots\\0&a_0&a_1&a_2&\ddots&\ddots\\0&0&a_0&a_1&\ddots&\ddots\\\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\end{pmatrix}
$$$$
\lim _{t \to \infty} P(x(t)=k)=\lim _{n \to \infty} P\left(x_{n}=k\right)
$$<p>for certain queues after a long time, i.e., the limiting behavior at a stationary state is the same as those at the random departure instant.</p><h3 id=chapman-kolmogorov-equation>CHAPMAN KOLMOGOROV EQUATION
<a class=anchor href=#chapman-kolmogorov-equation>#</a></h3><p>Remember?</p>$$
\begin{aligned} P\left(x_{r}=e_{i}, x_{n}=e_{j}, x_{m}=e_{k}\right) &=P\left(x_{r}=e_{i} | x_{m}=e_{j}\right)P\left(x_{n}=e_{j} | x_{m}=e_{k}\right)P\left(x_{m}=e_{k}\right) \\ &=p_{ji}(n,r)p_{kj}(m,n)p_{k}(m) \end{aligned}
$$<p>For $n>m$</p>$$
\begin{aligned} P\left(x_{n}=e_{j}, x_{m}=e_{i}\right) &=\sum_{k} P\left(x_{n}=e_{j}, x_{k}=e_{k}, x_{m}=e_{i}\right) \\ &=\sum_{k} P\left(x_{n}=e_{j} | x_{k}=e_{k}, x_{m}=e_{i}\right)P\left(x_{k}=e_{k}, x_{m}=e_{i}\right) \\ &=\sum_{k} P\left(x_{n}=e_{j} | x_{k}=e_{k}\right)P\left(x_{k}=e_{k} | x_{m}=e_{i}\right) \end{aligned}
$$$$
\begin{aligned} p_{ij}(m,n) &=P\left(x_{n}=e_{j} | x_{m}=e_{i}\right) \\ &=\sum_{k} P\left(x_{n}=e_{j} | x_{k}=e_{k}\right)P\left(x_{k}=e_{k} | x_{m}=e_{i}\right) \end{aligned}
$$$$
p_{ij}(m,n)=\sum_{k} p_{ik}(m,r)p_{kj}(r,n)
$$<p>By letting $r=m + 1,m + 2,\cdots$</p>$$
P(m,n)=P(m,m + 1)P(m + 1,m + 2)\cdots P(n - 1,n)
$$<ul><li><p>For all $n>m$, it is sufficient to know the one-step transition probability matrices $P(0,1),P(1,2),\cdots,P(n - 1,n)$.</p></li><li><p>For a <strong>homogeneous Markov chain</strong>, $P(i,i + 1)=P$, for all $i$ → $P(m,n)=P^{n - m}$.</p></li></ul><h3 id=n-step-transition-probabilities>N-STEP TRANSITION PROBABILITIES
<a class=anchor href=#n-step-transition-probabilities>#</a></h3><p>For a homogeneous chain $p_{ij}^{(n)}$ represents the $(i,j)$th entry of $P(0,n)=P^n$. As $p_{ij}^{(m + n)}=p_{ik}^{(m)}p_{kj}^{(n)}=p_{ik}^{(n)}p_{kj}^{(m)}$, we have:</p>$$
p_{ij}^{(m + n)}=\sum_{k} p_{ik}^{(m)}p_{kj}^{(n)}=\sum_{k} p_{ik}^{(n)}p_{kj}^{(m)}
$$<p>The one-step recursion relation</p>$$
p_{ij}^{(n+1)}=\sum_{k} p_{ik}p_{kj}^{(n)}=\sum_{k} p_{ik}^{(n)}p_{kj}
$$<p>The unconditional probability distribution at $t=nT$, or $p(n)=p(m)p(m,n)$, where</p>$$
p(n) \triangleq\left[p_1(n),p_2(n),\cdots,p_j(n),\cdots\right]
$$$$
\begin{aligned} p_j(n) &=P\left(x_{n}=e_{j}\right)=\sum_{i} P\left(x_{n}=e_{j} | x_{n}=e_{j}\right)P\left(x_{n}=e_{j}\right) \\ &=\sum_{i} p_{ij}(m,n)p_1(m) \end{aligned}
$$<p>For a homogeneous chain, $P(n)=p(0)P^n$.</p><p>In general, it is difficult to obtain explicit formulas for the $n$-step transition probabilities $p_{ij}^{(n)}$.</p><p>For a homogeneous Markov chain with finitely many states $e_1,e_2,\cdots,e_N$, the transition matrix $P$ is $N\times N$ with certain possible simplifications.</p>$$
p_{ij}^{(m + n)}=\sum_{k} p_{ik}^{(m)}p_{kj}^{(n)}=\sum_{k} p_{ik}^{(n)}p_{kj}^{(m)}
$$<p>Consider the case when \(m = 1\)</p>$$
p_{ij}^{(n)}=\sum_{k = 0}^{M} p_{ik}p_{kj}^{(n - 1)}=P\cdot P^{(n - 1)}
$$<p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-20-21-30-02.png alt></p><h4 id=example---weather-forecasting-a-two-state-markov-chain>Example - Weather Forecasting, a two-state Markov chain:
<a class=anchor href=#example---weather-forecasting-a-two-state-markov-chain>#</a></h4>$$
P=\left[\begin{array}{ll} \alpha & 1-\alpha \\ \beta & 1-\beta \end{array}\right]
$$<p>If $\alpha = 0.7$ and $\beta = 0.4$, what is the probability that it will rain four days from today given that it is raining today?</p><p>The one-step transition probability matrix is given by</p>$$
P=\left|\begin{array}{ll} 0.7 & 0.3 \\ 0.4 & 0.6 \end{array}\right|
$$<p>Hence</p>$$
\begin{aligned} P^{(2)}=P^{2} &=\left|\begin{array}{ll} 0.7 & 0.3 \\ 0.4 & 0.6 \end{array}\right| \cdot\left|\begin{array}{ll} 0.7 & 0.3 \\ 0.4 & 0.6 \end{array}\right| \\ &=\left|\begin{array}{ll} 0.61 & 0.39 \\ 0.52 & 0.48 \end{array}\right| \end{aligned}
$$$$
\begin{aligned} P^{(4)}=\left(P^{2}\right)^{2} &=\left|\begin{array}{ll} 0.61 & 0.39 \\ 0.52 & 0.48 \end{array}\right| \cdot\left|\begin{array}{ll} 0.61 & 0.39 \\ 0.52 & 0.48 \end{array}\right| \\ &=\left|\begin{array}{ll} 0.5749 & 0.4251 \\ 0.5668 & 0.4332 \end{array}\right| \end{aligned}
$$<p>The desired probability $P_{00}^{(4)} = 0.5749$</p><h4 id=example---transforming-a-non-markov-process-into-a-markov-chain-cont-given-that-it-rained-on-monday-and-tuesday-what-is-the-probability-that-it-will-rain-on-thursday>Example - Transforming a (non-Markov) Process into a Markov Chain (cont.) Given that it rained on Monday and Tuesday, what is the probability that it will rain on Thursday?
<a class=anchor href=#example---transforming-a-non-markov-process-into-a-markov-chain-cont-given-that-it-rained-on-monday-and-tuesday-what-is-the-probability-that-it-will-rain-on-thursday>#</a></h4><p>The two-step transition matrix is given by</p>$$
\begin{aligned} P^{(2)}=P^{2} &=\left[\begin{array}{llll} 0.7 & 0 & 0.3 & 0 \\ 0.5 & 0 & 0.5 & 0 \\ 0 & 0.4 & 0 & 0.6 \\ 0 & 0.2 & 0 & 0.8 \end{array}\right] \cdot\left[\begin{array}{llll} 0.7 & 0 & 0.3 & 0 \\ 0.5 & 0 & 0.5 & 0 \\ 0 & 0.4 & 0 & 0.6 \\ 0 & 0.2 & 0 & 0.8 \end{array}\right] \\ &=\left[\begin{array}{llll} 0.49 & 0.12 & 0.21 & 0.18 \\ 0.35 & 0.20 & 0.15 & 0.30 \\ 0.20 & 0.12 & 0.20 & 0.48 \\ 0.10 & 0.16 & 0.10 & 0.64 \end{array}\right] \end{aligned}
$$<p>“Rain on Thursday” is equivalent to the process being in either state 0 or state 1 on Thursday. The desired probability is given by $P_{00}^{2}+P_{01}^{2}=0.49 + 0.12=0.61$.</p><h3 id=solving-for-pn>SOLVING FOR P(N)
<a class=anchor href=#solving-for-pn>#</a></h3>$$
PU = U\Lambda
$$<p>where $U \triangleq[u_1,u_2,\cdots,u_N]$ and</p>$$
\Lambda \triangleq\left(\begin{array}{llll} \lambda_1 & \lambda_2 & & 0 \\ 0 & \ddots & \lambda_N \end{array}\right)
$$<p>$U$ is non-singular(full rank and not zero determinant), $V = U^{-1}$ exists</p>$$
V \triangleq U^{-1}=\left[\begin{array}{c} v_1 \\ v_2 \\ \vdots \\ v_N \end{array}\right]
$$<p>Finally we can get</p>$$
P^{n}=U\Lambda^{n}V=\sum_{k = 1}^{N} \lambda_k^{n} u_k v_k^T
$$<p>or</p>$$p_{ij}^{(n)}=\sum_{k = 1}^{N}\lambda_{k}^{n}u_{ki}v_{kj}$$<h4 id=the-calculation-process>The calculation process
<a class=anchor href=#the-calculation-process>#</a></h4><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-20-21-54-35.png alt></p><h4 id=example--eigenvalues-and-eigenvectors>EXAMPLE – EIGENVALUES AND EIGENVECTORS
<a class=anchor href=#example--eigenvalues-and-eigenvectors>#</a></h4><ol><li><strong>Calculating Eigenvalues</strong>:</li></ol><p>Given a matrix $A$, we calculate its eigenvalues by solving the characteristic equation $c(\lambda)=\text{det}(A - \lambda I_{2})$. For $A=\begin{bmatrix}1&amp;1\ - 2&amp;4\end{bmatrix}$, we have:</p>$$c(\lambda)=\text{det}\begin{pmatrix}\begin{bmatrix}1-\lambda&1\\ - 2&4-\lambda\end{bmatrix}\end{pmatrix}=(1 - \lambda)(4 - \lambda)+2
$$<p>Expand the expression: $(1 - \lambda)(4 - \lambda)+2 = 4-\lambda - 4\lambda+\lambda^{2}+2=\lambda^{2}-5\lambda + 6$, set $\lambda^{2}-5\lambda + 6 = 0$. Factoring gives $(\lambda - 3)(\lambda - 2)=0$, so $\lambda_{1}=3$ and $\lambda_{2}=2$.</p><ol start=2><li><strong>Finding Eigenvectors</strong>:</li></ol><ul><li><strong>For $\lambda_{1}=3$</strong>:<ul><li>We solve the system $(A - 3I_{2})x = 0$. Here, $A - 3I_{2}=\begin{bmatrix}1 - 3&amp;1\ - 2&amp;4 - 3\end{bmatrix}=\begin{bmatrix}-2&amp;1\ - 2&amp;1\end{bmatrix}$.</li><li>The augmented matrix is $\begin{bmatrix}-2&amp;1&amp;0\ - 2&amp;1&amp;0\end{bmatrix}$, which row - reduces to
$$
\begin{bmatrix}1&-\frac{1}{2}&0\\0&0&0\end{bmatrix}
$$
Let
$$
x=\begin{bmatrix}x_{1}\\x_{2}\end{bmatrix}
$$
, then $x_{1}=\frac{1}{2}x_{2}$, so
$$
x = x_{2}\begin{bmatrix}\frac{1}{2}\\1\end{bmatrix}
$$
Choosing $x_{2}=2$, we get the eigenvector
$$
x=\begin{bmatrix}1\\2\end{bmatrix}
$$</li></ul></li><li><strong>For $\lambda_{2}=2$</strong>:<ul><li>We solve the system $(A - 2I_{2})x = 0$. Here, $A - 2I_{2}=\begin{bmatrix}1 - 2&amp;1\ - 2&amp;4 - 2\end{bmatrix}=\begin{bmatrix}-1&amp;1\ - 2&amp;2\end{bmatrix}$.</li><li>The augmented matrix is $\begin{bmatrix}-1&amp;1&amp;0\ - 2&amp;2&amp;0\end{bmatrix}$, which row - reduces to
$$
\begin{bmatrix}1&-1&0\\0&0&0\end{bmatrix}
  $$
Let
$$
x=\begin{bmatrix}x_{1}\\x_{2}\end{bmatrix}
$$
, then $x_{1}=x_{2}$, so
$$
x = x_{2}\begin{bmatrix}1\\1\end{bmatrix}
$$
Choosing $x_{2}=1$, we get the eigenvector
$$
x=\begin{bmatrix}1\\1\end{bmatrix}
$$</li></ul></li></ul><h4 id=example---binary-communication-channel-1>Example - BINARY COMMUNICATION CHANNEL
<a class=anchor href=#example---binary-communication-channel-1>#</a></h4><ol><li><strong>Calculating Eigenvalues of the Binary Communication Channel Matrix</strong>:</li></ol><p>Given the binary communication channel matrix $P=\begin{bmatrix}1-\alpha&\alpha\\beta&amp;1 - \beta\end{bmatrix}$, we find its eigenvalues by solving $\text{det}(P-\lambda I)=0$.</p><p>$\text{det}(P-\lambda I)=\begin{vmatrix}1-\alpha-\lambda&\alpha\\beta&amp;1 - \beta-\lambda\end{vmatrix}=(1-\alpha-\lambda)(1 - \beta-\lambda)-\alpha\beta$.</p><p>Expand: $1-\beta-\lambda-\alpha+\alpha\beta+\alpha\lambda-\lambda+\lambda\beta+\lambda^{2}-\alpha\beta=\lambda^{2}-\lambda(2-\alpha-\beta)+(1-\alpha-\beta)=0$.</p><p>$\lambda_1 = 1$ and $\lambda_2 = 1-\alpha-\beta &lt; 1$</p><ol start=2><li><strong>Finding Matrices U and V</strong>:</li></ol><p>After solving for $U$ and $V$ and normalizing, we get</p>$$
U=\begin{bmatrix}1&-\alpha\\1&\beta\end{bmatrix}
$$<p>and</p>$$
V=\frac{1}{\alpha+\beta}\begin{bmatrix}\beta&\alpha\\ - 1&1\end{bmatrix}
$$<p>3. <strong>Calculating the \(n\)-step Transition Probability Matrix</strong>:</p>$$
P^{n}=\frac{1}{\alpha+\beta}\begin{bmatrix}1\\1\end{bmatrix}(\beta,\alpha)+\frac{(1-\alpha-\beta)^{n}}{\alpha+\beta}\begin{bmatrix}-\alpha\\\beta\end{bmatrix}(-1,1)
$$<ul><li>Expand further:
$$
P^{n}=\frac{1}{\alpha+\beta}\begin{bmatrix}\beta&\alpha\\\beta&\alpha\end{bmatrix}+\frac{(1-\alpha-\beta)^{n}}{\alpha+\beta}\begin{bmatrix}\alpha&-\alpha\\-\beta&\beta\end{bmatrix}
$$</li></ul><ol><li><strong>Calculating Conditional Probabilities</strong>:</li></ol><ul><li>For example, to find $P(x_{0}=1|x_{n}=1)$:<ul><li>$P(x_{0}=1|x_{n}=1)=\frac{P{x_{0}=1|x_{0}=1}P(x_{0}=1)}{P(x_{n}=1)}=\frac{p_{11}^{(n)}p}{p_{11}^{(n)}p + p_{01}^{(n)}q}$.</li><li>Substitute $p_{11}^{(n)}$ and $p_{01}^{(n)}$ from $P^{n}$: $P(x_{0}=1|x_{n}=1)=\frac{[\alpha+(1-\alpha-\beta)^{n}\beta]p}{\alpha+(1-\alpha-\beta)^{n}(\beta p-\alpha q)}$.</li></ul></li></ul><h3 id=closed-sets>CLOSED SETS
<a class=anchor href=#closed-sets>#</a></h3><ol><li><strong>Definition of a Closed Set</strong>:<ul><li>A set of states $C$ is closed if no state outside $C$ can be reached from any state in $C$.</li><li>In a random walk with absorbing barrier, if $C$ is a closed set and $e_{i}\in C$, $e_{j}\notin C$, then $p_{ij}=0$ and $p_{ij}(n)=0$ for $n\geq1$.</li></ul></li><li><strong>Absorbing State</strong>:<ul><li>A closed set with only one state is an absorbing state. If $e_{i}$ is an absorbing state, then $p_{ii}=1$ and $p_{ij}=0$ for $j\neq i$.</li></ul></li></ol><h3 id=irreducible-markov-chain>IRREDUCIBLE MARKOV CHAIN
<a class=anchor href=#irreducible-markov-chain>#</a></h3><ol><li><strong>Accessibility and Communication</strong>:<ul><li>State $e_{j}$ is accessible from state $e_{i}$ if $p_{ij}(n)>0$ for some $n$.</li><li>If $e_{i}$ and $e_{j}$ are accessible from each other, they communicate.</li><li>A Markov chain is irreducible if every state is accessible from every other state. Equivalently, a Markov chain is irreducible <strong>if there exists no closed set other than the set of all states.</strong></li></ul></li><li><strong>Example</strong>:<ul><li>In a simple random walk, every state can be reached from every other state, so it is an irreducible chain. Also, a binary communication channel is irreducible.</li></ul></li></ol><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-20-22-37-01.png alt></p><h3 id=persistent-recurrent-state>PERSISTENT (RECURRENT) STATE
<a class=anchor href=#persistent-recurrent-state>#</a></h3><ol><li><strong>First Passage Probability</strong>:</li></ol><p>Define $f_{ij}^{(n)} = P[x_{n}=e_{j},x_{m}\neq e_{j},0 &lt; m &lt; n|x_{0}=e_{i}]$, the first passage probability from $e_{i}$ to $e_{j}$ in $n$ steps.</p><p><strong>We have $p_{ij}^{(n)}=\sum_{r = 1}^{n}f_{ij}^{(r)}p_{jj}^{(n - r)}$ for $n\geq1$(not must be the first time)</strong></p><p>Let $P(z)=\sum_{n = 0}^{\infty}p_{ij}^{(n)}z^{n}$ and $F(z)=\sum_{n = 1}^{\infty}f_{ij}^{(n)}z^{n}$</p><p>then we have $P_{ij}(z)=p_{ij}^{(0)}+F_{ij}(z)P_{jj}(z)$.</p><p>For $i = j$, $P_{ii}(z)=\frac{1}{1 - F_{ii}(z)}$. and $f_{ij}=\sum_{n = 1}^{\infty}f_{ij}^{(n)}=F_{ij}(1)$(first passage probability)</p><p><strong>Classification of States</strong>:</p><ul><li>State $e_{i}$ is <strong>persistent (recurrent)</strong> if $f_{ii}=1$, start from $e_{i}$ and will return to $e_{i}$ with probability 1.<ul><li>A persistent state $e_{i}$ is <strong>null</strong> state if its mean recurrence time $\mu_{i}=\sum_{n = 1}^{\infty}nf_{ii}^{(n)}=\infty$, and <strong>non-null</strong> if $\mu_{i}&lt;\infty$.</li></ul></li><li>State $e_{i}$ is <strong>transient</strong> if $f_{ii}&lt;1$.</li></ul><h3 id=periodicity>periodicity
<a class=anchor href=#periodicity>#</a></h3><h4 id=definition-of-period>Definition of Period
<a class=anchor href=#definition-of-period>#</a></h4><p>The period $T$ of a state is the greatest common divisor (gcd) of ${n:p_{ii}(n)&lt;>0}$. All the none-zero $p_{ii}(n)$ and get the gcd.(the $p_{ii}(n)$ is the diagonal line elements).</p><p>State $i$ is <strong>periodic</strong> with period $T$ iff $p_{ii}(n)>0$ only when $n$ is a multiple of $T$.</p><p>If $T = 1$, the state is <strong>aperiodic</strong>.</p><h4 id=examples>Examples
<a class=anchor href=#examples>#</a></h4><p>For</p>$$
P=\begin{bmatrix}0&1\\0.5&0.5\end{bmatrix}
$$<p>$P^1_{11}=0$, $P^2_{11}=0.5>0$, $P^3_{11}=0.25>0$, $\text{gcd}(2,3,\cdots)=1$, so state 1 is aperiodic, and state 2 is also aperiodic because 1 and 2 are communicated.</p><p>For</p>$$
P=\begin{bmatrix}0&1\\1&0\end{bmatrix}
$$<p>, $P^{2n + 1}_{11}=0$ and $P^{2n}_{11}=1$ for $n = 1,2,\cdots$, $\text{gcd}(2,4,\cdots)=2$, so state 1 has period 2.</p><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-20-23-18-55.png alt></p><h3 id=positive-recurrence-and-ergodicity>Positive Recurrence AND Ergodicity
<a class=anchor href=#positive-recurrence-and-ergodicity>#</a></h3><ol><li><strong>Positive Recurrence</strong>:<ul><li>State $i$ is <strong>positive recurrent</strong> when the expected value of the <strong>return time</strong> $T_{i}=\min\{n>0:X_{n}=i|X_{0}=i\}$ is <strong>finite</strong>, i.e., $E[T_{i}|x_{0}=i]=\sum_{n = 1}^{\infty}nP(T_{i}=n|x_{0}=i)&lt;\infty$.</li><li>Positive and null recurrence are class properties. Recurrent states in a finite - state MC are positive recurrent.</li></ul></li><li><strong>Ergodicity</strong>:<ul><li>Jointly positive recurrent and aperiodic states are ergodic.</li><li>An irreducible MC with ergodic states is an ergodic MC.</li></ul></li></ol><h4 id=examples-1>Examples
<a class=anchor href=#examples-1>#</a></h4><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-20-23-32-18.png alt></p><h3 id=key-facts-for-a-markov-chain>KEY FACTS FOR A MARKOV CHAIN
<a class=anchor href=#key-facts-for-a-markov-chain>#</a></h3><ol><li>A transient state will only be visited a finite number of times.</li><li>In a finite-state Markov chain, at least one state must be persistent (not all states can be transient). because if all states are transient, the chain will not converge to a steady state.</li><li>If state $i$ is persistent and state $i$ <strong>communicates</strong> with state $j$, then state $j$ is persistent.</li><li>If state $i$ is transient and state $i$ <strong>communicates</strong> with state $j$, then state $j$ is transient.</li><li><strong>All states in a finite irreducible Markov chain are persistent.</strong>， which can be reduced from the previous facts.</li><li>A class of states is persistent if <strong>all states</strong> in the <strong>class</strong> are persistent, and transient if all states in the class are transient.</li></ol><h3 id=examples-2>Examples
<a class=anchor href=#examples-2>#</a></h3><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-20-23-41-07.png alt></p><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-20-23-47-00.png alt></p><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-20-23-50-29.png alt></p><ul><li>For the state 0 and 1:<ul><li>From state 0, $p_{00} = 0.5$ and $p_{01}=0.5$; from state 1, $p_{10} = 0.5$ and $p_{11}=0.5$. This means that state 0 and 1 can transfer to each other, and they cannot transfer to states 2, 3, or 4. so ${0,1}$ forms a closed set.</li></ul></li><li>For the state 2 and 3:<ul><li>From state 2, $p_{22}=0.5$ and $p_{23}=0.5$; from state 3, $p_{32}=0.5$ and $p_{33}=0.5$. State 2 and 3 can transfer to each other, and they cannot transfer to states 0, 1, or 4. so ${2,3}$ forms a closed set.</li></ul></li><li>For the state 4:<ul><li>From state 4, $p_{40}=0.25$，$p_{41}=0.25$，$p_{42}=0.5$，it can transfer to other states, but there is no other state can transfer to it (the transfer probability from other states to state 4 is 0).</li></ul></li></ul><p><strong>According to the definition of persistent and transient states, we can judge</strong></p><ul><li><strong>Persistent state (persistent state)</strong>:<ul><li>If a state is in a closed set, then it is a persistent state. Because from the state in the closed set, it will not leave the set, and it will eventually return to the state (probability is \(1\)). So states \(0\), \(1\), \(2\), and \(3\) are persistent states.</li></ul></li><li><strong>Transient state (transient state)</strong>:<ul><li>State \(4\) is not a part of the closed set, from state \(4\), after a finite number of steps, it will enter the closed set \(\{0,1\}\) or \(\{2,3\}\), once it enters the closed set, it will not return to state \(4\), so state \(4\) is a transient state.</li></ul></li></ul><p>In conclusion, states \(0\), \(1\), \(2\), and \(3\) are persistent states, and state \(4\) is a transient state.</p><h3 id=one-dimensional-random-walk>ONE-DIMENSIONAL RANDOM WALK
<a class=anchor href=#one-dimensional-random-walk>#</a></h3><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-20-23-56-15.png alt></p><p><strong>Transition Probabilities</strong>:</p><p>For a one-dimensional random walk with</p>$$
p_{j}=\begin{cases}p&j = i + 1\\1 - p&j = i - 1\\0&otherwise\end{cases}
$$<p>, for any state $e_{i}$,</p>$$
p_{ii}^{(n)}=\begin{cases}u_{2k}&n = 2k\\0&n = 2k+1\end{cases}
$$<p>, with $u_{2n}=\binom{2n}{n}(pq)^{n}$.</p><p>All states are periodic with period 2.</p><p>Look at the image:</p><ul><li>If $p\neq q$, $4pq&lt;1$, $\sum_{i = 0}^{\infty}p_{ii}^{(2)}&lt;\infty$ for every $e_{i}$, all states are transient.</li><li>If $p = q$ ($4pq = 1$), the series diverges and every state is persistent.</li></ul><h3 id=2-d-random-walk>2-D RANDOM WALK
<a class=anchor href=#2-d-random-walk>#</a></h3><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-21-00-03-55.png alt></p><p><strong>State Accessibility and Periodicity</strong>:</p><ul><li>The particle moves in unit steps in \(x\) and \(y\) directions independently. Every state is accessible from every other state, and all states are periodic with period 2.</li><li>The particle can return to the origin when the positive steps and negative steps are the same.</li></ul><p><strong>Return Probability</strong>:</p><ul><li>n is the total steps of x y and k is the positive x.</li><li>$p_{ii}^{(2n)}=\sum_{k = 0}^{n}\frac{2n!}{k!k!(n - k)!(n - k)!}(\frac{1}{2})^{4n}$. Using the approximation $\binom{2n}{n}\simeq\frac{\sqrt{4\pi n}(2n)^{2n}e^{-2n}}{(\sqrt{2\pi n}n^{n}e^{-n})^{2}}$, we get $p_{ii}^{(2n)}\simeq\frac{1}{\pi n}$ as $n\rightarrow\infty$.</li><li>The series diverges, so all states are <strong>persistent null</strong> with <strong>periodicity 2</strong>.</li></ul><h3 id=3---d-random-walk>3 - D RANDOM WALK
<a class=anchor href=#3---d-random-walk>#</a></h3><p><strong>State Accessibility and Periodicity</strong>:</p><ul><li>The particle moves independently in one of six directions. Every state is accessible from every other state and all states are periodic with period 2.</li></ul><p><strong>Return Probability</strong>:</p>$$
p_{ii}^{(2n)}\simeq P\{s_{2n}^{(x)}=0\}P\{s_{2n}^{(y)}=0\}P\{s_{2n}^{(z)}=0\}=\left\{\binom{2n}{n}2^{-2n}\right\}^{3}\simeq\frac{1}{\pi^{3/2}n^{3/2}}
$$<p>as $n\rightarrow\infty$.</p><ul><li>The series converges, so all states are transient. After some steps will never return to the origin.(the probability &lt; 1)</li></ul><h3 id=stationary-distributions>STATIONARY DISTRIBUTIONS
<a class=anchor href=#stationary-distributions>#</a></h3><p><strong>Existence of Limiting Distribution</strong>:</p><ul><li>every persistent state belongs to an irreducible closed set C containing similar states. Since no exit from C is ever possible.</li><li>In an irreducible ergodic chain, <strong>the limits $q_{k}=\lim_{n\rightarrow\infty}p_{jk}^{(n)}>0$ exist independent of the initial state $e_{j}$.</strong> These limiting probabilities satisfy $q_{j}=\sum_{i}q_{i}p_{ij}$.</li><li>Conversely, if a chain is irreducible and aperiodic and there exist non - negative numbers $q_{j}$ satisfying the above equations, then the chain is ergodic, and $q_{k}=\frac{1}{\mu_{k}}>0$, where $\mu_{j}$ is the mean recurrence time for the persistent non-null state $e_{j}$.</li><li>If $e_{j}$ is either a transient state or a persistent null state, then $q_{j}=0$.</li></ul><h4 id=example-1>Example
<a class=anchor href=#example-1>#</a></h4><p>Consider the weather forecast as a two-state Markov chain. if $\alpha = 0.7$ and $\beta = 0.4$, then calculate the probability that it will rain four days from today given that it is raining today.</p>$$
P=\begin{bmatrix}
0.7 & 0.3 \\
0.4 & 0.6
\end{bmatrix}
$$<p>Hence</p>$$
P^{(1)} = P=\begin{bmatrix}
0.7 & 0.3 \\
0.4 & 0.6
\end{bmatrix}
$$$$
P^{(2)}=P^{2}=\begin{bmatrix}
0.7 & 0.3 \\
0.4 & 0.6
\end{bmatrix}\begin{bmatrix}
0.7 & 0.3 \\
0.4 & 0.6
\end{bmatrix}=\begin{bmatrix}
0.61 & 0.39 \\
0.52 & 0.48
\end{bmatrix}
$$$$
P^{(4)} = P^{4}=(P^{2})^{2}=\begin{bmatrix}
0.61 & 0.39 \\
0.52 & 0.48
\end{bmatrix}\begin{bmatrix}
0.61 & 0.39 \\
0.52 & 0.48
\end{bmatrix}=\begin{bmatrix}
0.5769 & 0.4231 \\
0.5683 & 0.4332
\end{bmatrix}
$$<p>$P^{(4)}$ is the desired probability that it will rain four days from today given that it is raining today.</p>$$
P^{(8)}=\left[\begin{array}{ll} 0.572 & 0.428 \\ 0.570 & 0.430 \end{array}\right]
$$<p><strong>Note</strong>:</p><ol><li>$P^{(8)}$ is almost identical to $P^{(4)}$</li><li>Each of the rows of $P^{(8)}$ has almost identical entries</li></ol><h3 id=steady---state-matrix>STEADY - STATE MATRIX
<a class=anchor href=#steady---state-matrix>#</a></h3><p><strong>Convergence of Transition Probabilities</strong></p><ul><li>For an ergodic (irreducible, aperiodic and positive recurrent) Markov chain, $\lim_{n\rightarrow\infty}P_{ij}^{n}$ exists and is independent of $i$.</li><li>Let $\pi_{j}=\lim_{n\rightarrow\infty}P_{ij}^{n}$, then $\pi_{j}$ is the unique non-negative solution to the system $\pi_{j}=\sum_{i = 0}^{K}\pi_{i}P_{ij}$ ($\pi P=\pi$ in matrix form) and $\sum_{j = 0}^{K}\pi_{j}=1$.</li></ul><p>This set of equations can be expanded as</p>$$
\begin{array}{r} \pi_{0} P_{00}+\pi_{1} P_{10}+\pi_{2} P_{20}+...+\pi_{K} P_{K 0}=\pi_{0} \\ \pi_{0} P_{01}+\pi_{1} P_{11}+\pi_{2} P_{21}+...+\pi_{K} P_{K 1}=\pi_{1} \\ \vdots \\ \pi_{0} P_{0 K}+\pi_{1} P_{1 K}+\pi_{2} P_{2 K}+...+\pi_{K} P_{K}=\pi_{K} \\ \pi_{0}+\pi_{1}+\pi_{2}+...+\pi_{K}=1 \end{array}
$$<p>Note:</p>$$\begin{aligned} P_{k j}^{n+1} & =\sum_{i=0}^{\infty} P\left(X_{n+1}=j | X_{n}=i, X_{0}=k\right) P_{k}^{n} \\ & =\sum_{i=0}^{\infty} P_{i j} P_{k i}^{n} \end{aligned}
$$<p>If the limits exists, for sufficiently large n</p>$$
\pi_{j}=\sum_{i=0}^{\infty} \pi_{i} P_{i j}
$$<p><strong>Interpretation of Limiting Probability</strong></p><p>The limiting probability $\pi_{j}$ that the process will be in state $j$ at time $n$ also <strong>equals the long-run proportion of time that the process will be in state $j$</strong>.</p><p>Consider the probability that the process will be in state j at time n +1</p>$$
P\left(X_{n+1}=j\right)=\sum_{i=0}^{\infty} P\left(X_{n+1}=j | X_{n}=i\right) P\left(X_{n}=i\right)=\sum_{i=0}^{\infty} P_{i j} P\left(X_{n}=i\right)
$$<p>Now,if we let n $\rightarrow\infty$,the result follows.</p><h3 id=vectormatrix-notation-matrix-limit>VECTOR/MATRIX NOTATION: MATRIX LIMIT
<a class=anchor href=#vectormatrix-notation-matrix-limit>#</a></h3><p>For a finite-state Markov chain, $\lim_{n\rightarrow\infty}P^{n}=\begin{bmatrix}\pi_{1}&\pi_{2}&\cdots&\pi_{J}\\pi_{1}&\pi_{2}&\cdots&\pi_{J}\\vdots&\vdots&\ddots&\vdots\\pi_{1}&\pi_{2}&\cdots&\pi_{J}\end{bmatrix}$, where $\pi_{j}$ is the limiting probability of being in state $j$.</p><ul><li><strong>Same probabilities for all rows ==> Independent of initial state</strong></li><li>$\lim_{n\rightarrow\infty}p(n)=\lim_{n\rightarrow\infty}(P^{T})^{n}p(0)=[\pi_{1},\cdots,\pi_{J}]^{T}$, which is independent of initial condition $p(0)$.</li></ul><h3 id=vectormatrix-notation-eigenvector>VECTOR/MATRIX NOTATION: EIGENVECTOR
<a class=anchor href=#vectormatrix-notation-eigenvector>#</a></h3><p>The vector limit (steady-state) distribution is $\boldsymbol{\pi}=[\pi_1,\pi_2,\cdots,\pi_J]$.</p><p>The limit distribution is the unique solution of $\boldsymbol{\pi}P^{T} = \boldsymbol{\pi}$ and $\boldsymbol{\pi}^{T}\mathbf{1}=1$, where $\mathbf{1}=(1,1,\cdots,1)^{T}$.</p><p>$\boldsymbol{\pi}$ is the eigenvector associated with eigenvalue 1 of $P^{T}$.</p><p>Eigenvectors are defined up to a scaling factor, and we normalize them to sum to 1. All other eigenvalues of $P$ have modulus smaller than 1. If not, $P^{n}$ diverges, but we know $P^{n}$ contains $n$-step transition probabilities. So, $\boldsymbol{\pi}$ is the eigenvector associated with the largest eigenvalue of $P$. Computing it as an eigenvector is often computationally efficient.</p><h4 id=example-weather-forecast-again>Example: Weather forecast again
<a class=anchor href=#example-weather-forecast-again>#</a></h4>$$
P=\begin{bmatrix}
\alpha & 1-\alpha \\
\beta & 1-\beta
\end{bmatrix}
$$<p>If we say that the state is 0(whose limit prob is $\pi_{0}$) when it rains and 1 when it does not rain (the limiting probability $\pi_{1}$). We can deduce:</p>$$
\pi_{0}=\alpha \pi_{0}+\beta \pi_{1}
$$$$
\pi_{1}=(1-\alpha) \pi_{0}+(1-\beta) \pi_{1}
$$$$
\pi_{0}=\frac{\beta}{1+\beta-\alpha} \quad \pi_{1}=\frac{1-\alpha}{1+\beta-\alpha}
$$$$
\pi_{0}+\pi_{1}=1
$$<p>For example, if $\alpha = 0.7$ and $\beta = 0.4$, then the limiting probability of rain is $\pi_{0}=\frac{0.4}{1 + 0.4 - 0.7}=0.57$.</p><h4 id=example-2>Example
<a class=anchor href=#example-2>#</a></h4><p>Does $P$ correspond to an ergodic MC?</p>$$
P=\begin{bmatrix}
0 & 0.3 & 0.7 \\
0.1 & 0.5 & 0.4 \\
0.1 & 0.2 & 0.7
\end{bmatrix}
$$<ul><li>Irreducible: all states communicate with state 2.</li><li>positive recurrent, irreducible, and finite.</li><li>It is also aperiodic since the period of state 2 is 1.</li><li>There exist $\pi_{1}$, $\pi_{2}$, and $\pi_{3}$ such that $\pi_{j}=\lim_{n\rightarrow\infty}P_{ij}^{n}$ independent of $i$.</li></ul><p>We solve the system of linear equations</p>$$
\pi_{j}=\sum_{i = 1}^{3}\pi_{i}P_{ij}\quad\text{and}\quad\sum_{j = 1}^{3}\pi_{j}=1
$$$$
\begin{bmatrix}
\pi_{1} \\ \pi_{2} \\ \pi_{3} \\ 1
\end{bmatrix}=\begin{bmatrix}
0 & 0.1 & 0.1 \\ 0.3 & 0.5 & 0.2 \\ 0.7 & 0.4 & 0.7 \\ 1 & 1 & 1
\end{bmatrix}\begin{bmatrix} \pi_{1} \\ \pi_{2} \\ \pi_{3} \end{bmatrix}
$$<p>There are three variables and four equations. Some equations might be linearly dependent.</p><p>Indeed, summing the first three equations: $\pi_{1}+\pi_{2}+\pi_{3}=\pi_{1}+\pi_{2}+\pi_{3}$</p><p>This is always true because the probabilities in rows of $P$ sum up to 1, which is a manifestation of the rank deficiency of $1 - P$.</p><p>The solution yields $\pi_{1}=0.0909$, $\pi_{2}=0.2987$, and $\pi_{3}=0.6104$.</p><h3 id=ergodicity>ERGODICITY
<a class=anchor href=#ergodicity>#</a></h3><p>The fraction of time $T_{i}^{(n)}$ spent in the $i$-th state by time $n$ is</p>$$
T_{i}^{(n)}:=\frac{1}{n}\sum_{m = 1}^{n}\mathbb{I}\{X_{m}=i\}
$$<blockquote><p>The $\mathbb{I}{X_{m}=i}$ is an indicator function that is 1 if $X_{m}=i$ and 0 otherwise.</p></blockquote><p>We compute the <strong>expected value</strong> of $T_{i}^{(n)}$:</p>$$
E\left[T_{i}^{(n)}\right]=\frac{1}{n}\sum_{m = 1}^{n}E\left[\mathbb{I}\{X_{m}=i\}\right]=\frac{1}{n}\sum_{m = 1}^{n}P\left(X_{m}=i\right)
$$<p>As $n\rightarrow\infty$, the probabilities $P(X_{m}=i)\rightarrow\pi_{i}$, so</p>$$
\lim_{n \to \infty}E\left[T_{i}^{(n)}\right]=\lim_{n \to \infty}\frac{1}{n}\sum_{m = 1}^{n}P\left(X_{m}=i\right)=\pi_{i}
$$<p><strong>For ergodic Markov chains</strong>, the following holds without taking the expected value (almost surely a.s.):</p>$$
\lim_{n \to \infty}T_{i}^{(n)}=\lim_{n \to \infty}\frac{1}{n}\sum_{m = 1}^{n}\mathbb{I}\{X_{m}=i\}=\pi_{i}
$$<h4 id=example-3>Example
<a class=anchor href=#example-3>#</a></h4>$$
P:=\begin{bmatrix}
0 & 0.3 & 0.7 \\
0.1 & 0.5 & 0.4 \\
0.1 & 0.2 & 0.7
\end{bmatrix}
$$<p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-21-13-49-56.png alt></p><p>The visits to states $nT_{i}^{(n)}$ are ergodic averages. These ergodic averages slowly converge to $\boldsymbol{\pi}=[0.09,0.29,0.61]^{\top}$.</p><h3 id=functions-ergodic-average>FUNCTION&rsquo;S ERGODIC AVERAGE
<a class=anchor href=#functions-ergodic-average>#</a></h3><p>Consider an ergodic Markov chain with states $x_{n}=0,1,2,\cdots$ and stationary probabilities $\boldsymbol{\pi}$. Let $f(X_{n})$ be a bounded function of the state. Then,</p>$$
\lim_{n \to \infty}\frac{1}{n}\sum_{m = 1}^{n}f\left(X_{m}\right)=\sum_{j = 1}^{\infty}f(j)\pi_{j},\quad \text{a.s.}
$$<p>The ergodic average is equivalent to the expectation under the stationary distribution.</p><p>The use of ergodic averages is more general than $T_{i}^{(n)}$(the time propotion), and $T_{i}^{(n)}$ is a special case with $f(X_{m})=\mathbb{I}(X_{m}=i)$.</p><p><strong>Think of $f(X_{n})$ as a reward (or cost) associated with the state. Then $\overline{f_{n}}=(1 / n)\sum_{m = 1}^{n}f(X_{m})$ is the time average of rewards (costs).</strong></p><h3 id=ergodicity-in-periodic-markov-chains>ERGODICITY IN PERIODIC MARKOV CHAINS
<a class=anchor href=#ergodicity-in-periodic-markov-chains>#</a></h3><p>Ergodic averages still converge if the Markov chain is periodic.</p><p>For an irreducible, positive recurrent Markov chain (periodic or aperiodic), we define</p>$$
\pi_{j}=\sum_{i = 0}^{\infty}\pi_{i}P_{i j},\quad\sum_{j = 0}^{\infty}\pi_{j}=1
$$<ul><li><strong>Claim 1</strong>: A <strong>unique solution exists</strong> (we say $\pi_{j}$ are well - defined).</li><li><strong>Claim 2</strong>: The fraction of time spent in <strong>state $i$ converges to</strong>
$$
\lim_{n \to \infty}T_{i}^{(n)}=\lim_{n \to \infty}\frac{1}{n}\sum_{m = 1}^{n}\mathbb{I}\{X_{m}=i\}\to\pi_{i}
$$</li></ul><p>If the Markov chain is periodic, the probabilities $p_{ij}^{n}$ oscillate. But the fraction of time spent in state $i$ converges to $\pi_{i}$.</p><h3 id=example-an-perlodic-irreducible-markov-chain>Example An perlodic Irreducible Markov chain
<a class=anchor href=#example-an-perlodic-irreducible-markov-chain>#</a></h3><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-21-14-05-40.png alt></p><h3 id=reducible-markov-chains>REDUCIBLE MARKOV CHAINS
<a class=anchor href=#reducible-markov-chains>#</a></h3><p>If a Markov chain is not irreducible, it can be decomposed into transient ($T$), ergodic ($E_{k}$), periodic ($P_{k}$), and null recurrent ($N_{k}$) components. All these are (communication) class properties.</p><ol><li><p>the limit probabilities for transient states are null, i.e., $P(X_{n}=i)\to0$ for all $i\in T$.</p></li><li><p>For an arbitrary ergodic component $\epsilon_{k}$, we define conditional limits $\pi_{j}=\lim_{n \to \infty}P(X_{n}=j|X_{0}\in \epsilon_{k})$ for all $j\in \epsilon_{k}$. And $\pi_{j}$ satisfies</p>$$
\pi_{j}=\sum_{i\in \epsilon_{k}}\pi_{i}P_{i j},\quad\sum_{j\in \epsilon_{k}}\pi_{j}=1
$$</li><li><p>For an arbitrary periodic component $P_{k}$, we re - define $\pi_{j}$ as</p>$$
\pi_{j}=\sum_{i\in P_{k}}\pi_{i}P_{i j},\quad\sum_{j\in P_{k}}\pi_{j}=1
$$</li></ol><ul><li>The probabilities $P(X_{n}=j|X_{0}\in P_{k})$ do not converge (they oscillate).</li><li>In addition,
$$
\lim_{n \to \infty}T_{j}^{(n)}:=\lim_{n \to \infty}\frac{1}{n}\sum_{m = 1}^{n}\mathbb{I}\{x_{m}=i|X_{0}\in P_{k}\}\to\pi_{i}
$$</li></ul><ol start=4><li>The limit probabilities for null-recurrent states are null, i.e., $P(X_{n}=i)\to0$ for all $i\in N$.</li></ol><h4 id=example---reducible-markov-chain>Example - Reducible Markov chain
<a class=anchor href=#example---reducible-markov-chain>#</a></h4><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-21-14-18-45.png alt></p><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-21-14-22-40.png alt></p><h2>Related readings</h2><ul><li><a href="/posts/markov-processes/?utm_source=related">Markov Processes</a></li><li><a href="/posts/stock-prices/?utm_source=related">Stock Prices</a></li><li><a href="/posts/possion-processes/?utm_source=related">Possion Processes</a></li><li><a href="/posts/random-walks/?utm_source=related">Random Walks</a></li><li><a href="/posts/stochastic-processes/?utm_source=related">Stochastic Processes</a></li></ul><hr><div class=post-nav><a href="https://blog.timerring.com/posts/markov-processes/?utm_source=nav">&lt;&lt; prev | Markov...</a>
<a onclick=goToRandomPost() style=cursor:pointer>Continue strolling
</a><a href="https://blog.timerring.com/posts/hidden-markov-models/?utm_source=nav">Hidden Markov... ｜ next >></a></div><hr>If you find this blog useful and want to support my blog, need my skill for something, or have a coffee chat with me, feel free to:<div class=support><a href=https://ko-fi.com/polls/What-is-the-custom-topic-for-the-next-month-C0C219LHQJ class=book-btn target=_blank>Subscribe to participate in blog topics and custom services
</a><a href=https://ko-fi.com/timerring class=book-btn target=_blank>Buy me a coffee</a></div><div style=margin-bottom:8px></div><script>function goToRandomPost(){const e=["/posts/hidden-markov-models/?utm_source=random","/posts/markov-chains/?utm_source=random","/posts/markov-processes/?utm_source=random","/posts/stock-prices/?utm_source=random","/posts/possion-processes/?utm_source=random","/posts/random-walks/?utm_source=random","/posts/0418-find-the-formula-rendering-issue-of-hugo/?utm_source=random","/posts/stochastic-processes/?utm_source=random","/posts/multi-platform-builds-docker/?utm_source=random","/posts/0404-The-invitation-only-mechanism-from-clubhouse-to-manus/?utm_source=random","/posts/how-to-use-git-submodule/?utm_source=random","/posts/live-streaming-infra-and-protocols/?utm_source=random","/posts/implement-danmaku-rendering-algorithm-from-scratch/?utm_source=random","/posts/is-there-an-rss-renaissance-in-the-ai-era/?utm_source=random","/posts/docker-with-gpu/?utm_source=random","/posts/some-useful-tools/?utm_source=random","/posts/a-shopping-experience-in-pangdonglai-supermarket/?utm_source=random","/posts/langchain-and-rag-best-practices/?utm_source=random","/posts/sequences-of-random-variables/?utm_source=random","/posts/random-variables/?utm_source=random","/posts/repeated-trials/?utm_source=random","/posts/probability-and-its-axioms/?utm_source=random","/posts/further-understanding-of-proc/?utm_source=random","/posts/the-iftop/?utm_source=random","/posts/some-useful-commands-to-share/?utm_source=random","/posts/reflections-on-trending-topics/?utm_source=random","/posts/cpu-can-only-see-the-threads/?utm_source=random","/posts/go-common-test/?utm_source=random","/posts/go-concurrency-and-parallelism/?utm_source=random","/posts/go-cheatsheet/?utm_source=random","/posts/video-technology-101/?utm_source=random","/posts/the-main-kind-of-message-queue/?utm_source=random","/posts/the-method-to-manage-traffic/?utm_source=random","/posts/the-different-kind-of-api-design/?utm_source=random","/posts/the-encode-and-decode-in-python/?utm_source=random","/posts/about-the-systemd/?utm_source=random","/posts/the-tips-about-dockerfile/?utm_source=random","/posts/docker-cheatsheet/?utm_source=random","/posts/docker-101/?utm_source=random","/posts/the-instance-class-static-magic-method-in-python/?utm_source=random","/posts/the-review-and-plan-for-bilive/?utm_source=random","/posts/the-overview-of-security/?utm_source=random","/posts/how-to-publish-your-code-as-a-pip-module/?utm_source=random","/posts/some-good-things-to-share-about-the-packages/?utm_source=random","/posts/introduction-to-the-http-and-https-protocol/?utm_source=random","/posts/mail-service-and-protocol/?utm_source=random","/posts/understanding-clash-through-configuration/?utm_source=random","/posts/thinking-about-advertisement-from-an-open-source-perspective/?utm_source=random","/posts/a-brief-introduction-to-dns/?utm_source=random","/posts/real-computer-network/?utm_source=random","/posts/python-generator-iterator-and-decorator/?utm_source=random","/posts/python-underlying-mechanism/?utm_source=random","/posts/python-files-exceptions-and-modules/?utm_source=random","/posts/python-object-oriented-programming/?utm_source=random","/posts/python-cheatsheet/?utm_source=random","/posts/python-function-parameter/?utm_source=random","/posts/python-control-structure/?utm_source=random","/posts/python-composite-data-type/?utm_source=random","/posts/python-basic-data-type/?utm_source=random","/posts/python-basic-syntax-elements/?utm_source=random","/posts/deploy-github-pages-with-gpg-signing/?utm_source=random","/posts/gpg-101/?utm_source=random","/posts/housewarming-2024/?utm_source=random"],t=Math.floor(Math.random()*e.length);window.location.href=e[t]}</script></article><div class=book-comments><div class="book-tabs-content markdown-inner"><div id=tcomment></div><script src=https://giscus.app/client.js data-repo=timerring/blog data-repo-id=R_kgDONeZYZg data-category=Announcements data-category-id=DIC_kwDONeZYZs4ClRWs data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=en crossorigin=anonymous async></script></div></div><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script><div class=busuanzi-footer><p style=text-align:center>&copy; 2022 - present <a href=https://github.com/timerring>timerring</a> | PV: <span id=busuanzi_value_site_pv></span> | UV: <span id=busuanzi_value_site_uv></span></p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#markov-chain>MARKOV CHAIN</a><ul><li><a href=#markov-property>MARKOV PROPERTY</a></li><li><a href=#example>Example</a><ul><li><a href=#1-d-random-walk>1-D RANDOM WALK</a></li><li><a href=#branching-processes>BRANCHING PROCESSES</a></li><li><a href=#nuclear-chain-reaction>NUCLEAR CHAIN REACTION</a></li><li><a href=#waiting-linesqueues>WAITING LINES/QUEUES</a></li></ul></li><li><a href=#transition-probabilities>TRANSITION PROBABILITIES</a></li><li><a href=#homogeneous-markov-chain>HOMOGENEOUS MARKOV CHAIN</a></li><li><a href=#stochastic-matrix>STOCHASTIC MATRIX</a><ul><li><a href=#example---binary-communication-channel>EXAMPLE - BINARY COMMUNICATION CHANNEL</a></li><li><a href=#example---weather-forecasting>EXAMPLE - Weather Forecasting</a></li><li><a href=#example--mood-of-gary>EXAMPLE – Mood of Gary</a></li><li><a href=#example---transforming-a-non-markov-process-into-a-markov-chain>EXAMPLE - Transforming a (non-Markov) Process into a Markov Chain</a></li></ul></li><li><a href=#example---back-to-random-walk>EXAMPLE - BACK TO RANDOM WALK</a><ul><li><a href=#random-walk-with-absorbing-barriers-in-the-edge-nodes>RANDOM WALK WITH ABSORBING BARRIERS in the edge nodes</a></li><li><a href=#random-walk-with-reflecting-barriers>RANDOM WALK WITH REFLECTING BARRIERS</a></li><li><a href=#cyclic-random-walks>CYCLIC RANDOM WALKS</a></li><li><a href=#ehrenfests-diffusion-model-non-uniform-random-walk>EHRENFEST&rsquo;S DIFFUSION MODEL (NON-UNIFORM RANDOM WALK)</a></li><li><a href=#success-runs-random-walk>SUCCESS RUNS (RANDOM WALK)</a></li></ul></li><li><a href=#random-occupancy>RANDOM OCCUPANCY</a></li><li><a href=#imbedded-markov-chains>IMBEDDED MARKOV CHAINS</a></li><li><a href=#chapman-kolmogorov-equation>CHAPMAN KOLMOGOROV EQUATION</a></li><li><a href=#n-step-transition-probabilities>N-STEP TRANSITION PROBABILITIES</a><ul><li><a href=#example---weather-forecasting-a-two-state-markov-chain>Example - Weather Forecasting, a two-state Markov chain:</a></li><li><a href=#example---transforming-a-non-markov-process-into-a-markov-chain-cont-given-that-it-rained-on-monday-and-tuesday-what-is-the-probability-that-it-will-rain-on-thursday>Example - Transforming a (non-Markov) Process into a Markov Chain (cont.) Given that it rained on Monday and Tuesday, what is the probability that it will rain on Thursday?</a></li></ul></li><li><a href=#solving-for-pn>SOLVING FOR P(N)</a><ul><li><a href=#the-calculation-process>The calculation process</a></li><li><a href=#example--eigenvalues-and-eigenvectors>EXAMPLE – EIGENVALUES AND EIGENVECTORS</a></li><li><a href=#example---binary-communication-channel-1>Example - BINARY COMMUNICATION CHANNEL</a></li></ul></li><li><a href=#closed-sets>CLOSED SETS</a></li><li><a href=#irreducible-markov-chain>IRREDUCIBLE MARKOV CHAIN</a></li><li><a href=#persistent-recurrent-state>PERSISTENT (RECURRENT) STATE</a></li><li><a href=#periodicity>periodicity</a><ul><li><a href=#definition-of-period>Definition of Period</a></li><li><a href=#examples>Examples</a></li></ul></li><li><a href=#positive-recurrence-and-ergodicity>Positive Recurrence AND Ergodicity</a><ul><li><a href=#examples-1>Examples</a></li></ul></li><li><a href=#key-facts-for-a-markov-chain>KEY FACTS FOR A MARKOV CHAIN</a></li><li><a href=#examples-2>Examples</a></li><li><a href=#one-dimensional-random-walk>ONE-DIMENSIONAL RANDOM WALK</a></li><li><a href=#2-d-random-walk>2-D RANDOM WALK</a></li><li><a href=#3---d-random-walk>3 - D RANDOM WALK</a></li><li><a href=#stationary-distributions>STATIONARY DISTRIBUTIONS</a><ul><li><a href=#example-1>Example</a></li></ul></li><li><a href=#steady---state-matrix>STEADY - STATE MATRIX</a></li><li><a href=#vectormatrix-notation-matrix-limit>VECTOR/MATRIX NOTATION: MATRIX LIMIT</a></li><li><a href=#vectormatrix-notation-eigenvector>VECTOR/MATRIX NOTATION: EIGENVECTOR</a><ul><li><a href=#example-weather-forecast-again>Example: Weather forecast again</a></li><li><a href=#example-2>Example</a></li></ul></li><li><a href=#ergodicity>ERGODICITY</a><ul><li><a href=#example-3>Example</a></li></ul></li><li><a href=#functions-ergodic-average>FUNCTION&rsquo;S ERGODIC AVERAGE</a></li><li><a href=#ergodicity-in-periodic-markov-chains>ERGODICITY IN PERIODIC MARKOV CHAINS</a></li><li><a href=#example-an-perlodic-irreducible-markov-chain>Example An perlodic Irreducible Markov chain</a></li><li><a href=#reducible-markov-chains>REDUCIBLE MARKOV CHAINS</a><ul><li><a href=#example---reducible-markov-chain>Example - Reducible Markov chain</a></li></ul></li></ul></li></ul></nav><style>.book-toc .book-toc-content a{color:var(--body-font-color)}</style></div></aside></main></body></html>