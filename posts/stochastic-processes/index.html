<!doctype html><html lang=en dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Compared to deterministic model, which is specified by a set of equations that describe exactly how the system will evolve over time. In a stochastic model, the evolution is at least partially random and if the process is run several times, it will not give identical results."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://blog.timerring.com/posts/stochastic-processes/"><meta property="og:site_name" content="timerring"><meta property="og:title" content="Stochastic Processes"><meta property="og:description" content="Compared to deterministic model, which is specified by a set of equations that describe exactly how the system will evolve over time. In a stochastic model, the evolution is at least partially random and if the process is run several times, it will not give identical results."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-15T19:30:50+08:00"><meta property="article:tag" content="Math"><meta property="article:tag" content="Random Process"><meta itemprop=name content="Stochastic Processes"><meta itemprop=description content="Compared to deterministic model, which is specified by a set of equations that describe exactly how the system will evolve over time. In a stochastic model, the evolution is at least partially random and if the process is run several times, it will not give identical results."><meta itemprop=datePublished content="2025-04-15T19:30:50+08:00"><meta itemprop=dateModified content="2025-04-15T19:30:50+08:00"><meta itemprop=wordCount content="4053"><meta itemprop=image content="https://blog.timerring.com/favicon.png"><meta itemprop=keywords content="Math,Random Process"><meta name=twitter:image content="https://blog.timerring.com/favicon.png"><meta name=twitter:card content="summary"><meta name=twitter:title content="Stochastic Processes"><meta name=twitter:description content="Compared to deterministic model, which is specified by a set of equations that describe exactly how the system will evolve over time. In a stochastic model, the evolution is at least partially random and if the process is run several times, it will not give identical results."><meta name=twitter:site content="@imjohnhowe"><title>Stochastic Processes | timerring</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.b2496f568c7d479e460b580e0e81d4b97841f83539eba8c770dc9f8e1aa7682e.css integrity="sha256-sklvVox9R55GC1gODoHUuXhB+DU566jHcNyfjhqnaC4=" crossorigin=anonymous><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.e52a063e2cd07b60d2172308e5ff350e5cf78f82ba099d910d20034b05fd82fc.js integrity="sha256-5SoGPizQe2DSFyMI5f81Dlz3j4K6CZ2RDSADSwX9gvw=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script defer>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1}]})})</script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/favicon.png alt=Logo><span>timerring</span></a></h2><hr><p>Rhythmic trend.</p><div class=book-search><div class=gcse-search></div></div><script async src="https://cse.google.com/cse.js?cx=40782701766144f96"></script><ul><li><a href=/>Dashboard</a></li><li><a href=/posts/>Blogs</a></li><li><a href=/categories/weekly/>Newsletter</a></li><li><a href=/index.xml>RSS</a></li><li><a href=/friends/>Friends</a></li><li><a href=/about/>About</a></li></ul><script>function goToRandomPost(){const e=["/posts/hidden-markov-models/?utm_source=random","/posts/markov-chains/?utm_source=random","/posts/markov-processes/?utm_source=random","/posts/stock-prices/?utm_source=random","/posts/possion-processes/?utm_source=random","/posts/random-walks/?utm_source=random","/posts/0418-find-the-formula-rendering-issue-of-hugo/?utm_source=random","/posts/stochastic-processes/?utm_source=random","/posts/multi-platform-builds-docker/?utm_source=random","/posts/0404-The-invitation-only-mechanism-from-clubhouse-to-manus/?utm_source=random","/posts/how-to-use-git-submodule/?utm_source=random","/posts/live-streaming-infra-and-protocols/?utm_source=random","/posts/implement-danmaku-rendering-algorithm-from-scratch/?utm_source=random","/posts/is-there-an-rss-renaissance-in-the-ai-era/?utm_source=random","/posts/docker-with-gpu/?utm_source=random","/posts/some-useful-tools/?utm_source=random","/posts/a-shopping-experience-in-pangdonglai-supermarket/?utm_source=random","/posts/langchain-and-rag-best-practices/?utm_source=random","/posts/sequences-of-random-variables/?utm_source=random","/posts/random-variables/?utm_source=random","/posts/repeated-trials/?utm_source=random","/posts/probability-and-its-axioms/?utm_source=random","/posts/further-understanding-of-proc/?utm_source=random","/posts/the-iftop/?utm_source=random","/posts/some-useful-commands-to-share/?utm_source=random","/posts/reflections-on-trending-topics/?utm_source=random","/posts/cpu-can-only-see-the-threads/?utm_source=random","/posts/go-common-test/?utm_source=random","/posts/go-concurrency-and-parallelism/?utm_source=random","/posts/go-cheatsheet/?utm_source=random","/posts/video-technology-101/?utm_source=random","/posts/the-main-kind-of-message-queue/?utm_source=random","/posts/the-method-to-manage-traffic/?utm_source=random","/posts/the-different-kind-of-api-design/?utm_source=random","/posts/the-encode-and-decode-in-python/?utm_source=random","/posts/about-the-systemd/?utm_source=random","/posts/the-tips-about-dockerfile/?utm_source=random","/posts/docker-cheatsheet/?utm_source=random","/posts/docker-101/?utm_source=random","/posts/the-instance-class-static-magic-method-in-python/?utm_source=random","/posts/the-review-and-plan-for-bilive/?utm_source=random","/posts/the-overview-of-security/?utm_source=random","/posts/how-to-publish-your-code-as-a-pip-module/?utm_source=random","/posts/some-good-things-to-share-about-the-packages/?utm_source=random","/posts/introduction-to-the-http-and-https-protocol/?utm_source=random","/posts/mail-service-and-protocol/?utm_source=random","/posts/understanding-clash-through-configuration/?utm_source=random","/posts/thinking-about-advertisement-from-an-open-source-perspective/?utm_source=random","/posts/a-brief-introduction-to-dns/?utm_source=random","/posts/real-computer-network/?utm_source=random","/posts/python-generator-iterator-and-decorator/?utm_source=random","/posts/python-underlying-mechanism/?utm_source=random","/posts/python-files-exceptions-and-modules/?utm_source=random","/posts/python-object-oriented-programming/?utm_source=random","/posts/python-cheatsheet/?utm_source=random","/posts/python-function-parameter/?utm_source=random","/posts/python-control-structure/?utm_source=random","/posts/python-composite-data-type/?utm_source=random","/posts/python-basic-data-type/?utm_source=random","/posts/python-basic-syntax-elements/?utm_source=random","/posts/deploy-github-pages-with-gpg-signing/?utm_source=random","/posts/gpg-101/?utm_source=random","/posts/housewarming-2024/?utm_source=random"],t=Math.floor(Math.random()*e.length);window.location.href=e[t]}</script><a class=random onclick=goToRandomPost()>Stroll</a></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>timerring</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a><ul><li><a href=#stochastic-processes>Stochastic Processes</a></li><li><a href=#compare-with-rv>Compare with RV</a></li></ul></li><li><a href=#interpretations-of-xt><strong>Interpretations of $X(t)$</strong></a></li><li><a href=#continuous-vs-discrete>Continuous vs Discrete</a></li><li><a href=#dtcv-process---record-the-temperature>DTCV Process - Record the temperature</a></li><li><a href=#dtdv-process---people-flipping-coins>DTDV Process - People flipping coins</a></li><li><a href=#ctcv-process--brownian-motion>CTCV Process – Brownian Motion</a></li><li><a href=#regular-process>Regular Process</a><ul><li><a href=#predictable-process--voltage-of-ac-generator>Predictable Process – Voltage of AC Generator</a></li></ul></li><li><a href=#equality>Equality</a></li><li><a href=#first-order-distribution--density-function>First-Order Distribution & Density Function</a><ul><li><a href=#frequency-interpretation>Frequency Interpretation</a></li></ul></li><li><a href=#second-order-and-n-th-order-properties>Second-Order and N-th Order Properties</a><ul><li><a href=#second-order-properties>Second-Order Properties</a></li></ul></li><li><a href=#examples>Examples</a><ul><li><a href=#possion-random-variable>Possion Random Variable</a></li><li><a href=#properties-of-independent-poisson-processes>Properties of Independent Poisson Processes</a></li></ul></li><li><a href=#point-and-renewal-processes>Point and Renewal Processes</a></li><li><a href=#poisson-process>Poisson Process</a><ul><li><a href=#random-selection-of-poisson-points>Random Selection of Poisson Points</a></li></ul></li><li><a href=#poisson-points-and-binomial-distribution>Poisson Points and Binomial Distribution</a></li><li><a href=#some-general-properties>Some General Properties</a></li><li><a href=#relationships-between-two-processes>Relationships Between Two Processes</a></li><li><a href=#normal-processes>Normal Processes</a></li><li><a href=#stationary-stochastic-processes>Stationary Stochastic Processes</a><ul><li><a href=#strict-sense-stationarity>Strict-Sense Stationarity</a></li></ul></li><li><a href=#wide-sense-stationarity>Wide-Sense Stationarity</a></li><li><a href=#centered-process>Centered Process</a><ul><li><a href=#other-forms-of-stationarity>OTHER FORMS OF STATIONARITY</a></li><li><a href=#mean-square-periodicity>MEAN SQUARE PERIODICITY</a></li></ul></li><li><a href=#stochastic-systems>STOCHASTIC SYSTEMS</a></li><li><a href=#stochastic-models>STOCHASTIC MODELS</a></li><li><a href=#how-stochastic-models-work>HOW STOCHASTIC MODELS WORK?</a></li><li><a href=#systems-with-stochastic-inputs>Systems with Stochastic Inputs</a><ul><li><a href=#k-th-order-joint-cumulative-distribution-function>K-TH ORDER JOINT CUMULATIVE DISTRIBUTION FUNCTION</a></li></ul></li><li><a href=#deterministic-vs-stochastic>DETERMINISTIC VS. STOCHASTIC</a></li><li><a href=#memoryless-systems>Memoryless Systems</a></li><li><a href=#statistics-of-y--gxt>STATISTICS OF Y = G[X(T)]</a></li><li><a href=#square---law-detector>SQUARE - LAW DETECTOR</a></li><li><a href=#hard-limiter>HARD LIMITER</a></li><li><a href=#linear-systems>Linear Systems</a></li><li><a href=#time-invariant-system>Time Invariant System</a></li><li><a href=#fundamental-theorem>FUNDAMENTAL THEOREM</a><ul><li><a href=#output-statistics>Output Statistics</a></li></ul></li><li><a href=#white-noise-process>White Noise Process</a></li><li><a href=#gaussian-vectors>GAUSSIAN VECTORS</a></li><li><a href=#gaussian-processes>GAUSSIAN PROCESSES</a></li><li><a href=#discrete-time-stochastic-processes>DISCRETE TIME STOCHASTIC PROCESSES</a><ul><li><a href=#the-delta-response-of-a-linear-discrete-system>THE DELTA RESPONSE OF A LINEAR DISCRETE SYSTEM</a></li></ul></li></ul></nav><style>.book-toc .book-toc-content a{color:var(--body-font-color)}</style></aside></header><article class=markdown><h1>Stochastic Processes</h1><h5>April 15, 2025
·
20 min read
<span id=busuanzi_container_page_pv>· Page View: <span id=busuanzi_value_page_pv></span></span></h5><div><a href=/categories/tutorial/ style=color:#000>Tutorial</a></div><div><a href=/tags/math/ style=color:#000>Math</a> <span style=color:#000>| </span><a href=/tags/random-process/ style=color:#000>Random Process</a></div><blockquote class="book-hint info">If you have any questions, feel free to comment below.</blockquote><hr><p>Compared to deterministic model, which is specified by a set of equations that describe exactly how the system will evolve over time. In a stochastic model, the evolution is at least partially random and if the process is run several times, it will not give identical results.</p><h2 id=introduction>Introduction
<a class=anchor href=#introduction>#</a></h2><h3 id=stochastic-processes>Stochastic Processes
<a class=anchor href=#stochastic-processes>#</a></h3><p>Let $\xi$ be the random result of an experiment, and for each such result, there is a waveform $X(t,\xi)$. Here, $t$ usually represents time, and $\xi$ represents a random factor.</p><ul><li>The set of these waveforms constitutes a stochastic process.</li><li>The random result set ${\xi_k}$ and the time index $t$ can be continuous or discrete(countable infinite or finite).</li></ul><p>eg. Brownian motion, stock market, queue system.</p><h3 id=compare-with-rv>Compare with RV
<a class=anchor href=#compare-with-rv>#</a></h3><ul><li><strong>R.V. X</strong>: A rule that assigns a number $x(e)$ to every outcome $e$ of an experiment $S$.</li><li><strong>Random/stochastic process $X(t)$</strong>: A family of time functions depending on the parameter $e$, or equivalently, a function of both $t$ and $e$. Here, $S={e}$ represents all possible outcomes, and $R = {\text{possible values for }t}$.</li><li><strong>Continuous vs Discrete</strong>:<ul><li>If $R$ is the real axis, $X(t)$ is a continuous - state process.</li><li>If $R$ takes only integer values, $X(t)$ is a discrete - state process, also known as a random sequence ${X_n}$ with countable values.</li></ul></li></ul><h2 id=interpretations-of-xt><strong>Interpretations of $X(t)$</strong>
<a class=anchor href=#interpretations-of-xt>#</a></h2><ol><li>When $t$ and $e$ are variables, it is a family of functions $x(t,e)$.</li><li>When $t$ is a variable and $e$ is fixed, it is a single time function (or a sample of the given process).</li><li>When $t$ is fixed and $e$ is variable, $x(t)$ is a random variable representing the state of the given process at time $t$.</li><li>When $t$ and $e$ are fixed, $x(t)$ is a number.</li></ol><h2 id=continuous-vs-discrete>Continuous vs Discrete
<a class=anchor href=#continuous-vs-discrete>#</a></h2><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-15-19-54-57.png alt></p><h2 id=dtcv-process---record-the-temperature>DTCV Process - Record the temperature
<a class=anchor href=#dtcv-process---record-the-temperature>#</a></h2><p>This is a discrete time, continuous value (DTCV) stochastic process. eg. Record the temperature at noon every day for a year.</p><h2 id=dtdv-process---people-flipping-coins>DTDV Process - People flipping coins
<a class=anchor href=#dtdv-process---people-flipping-coins>#</a></h2><p>This is a discrete time, discrete value (DTDV) stochastic process. eg. Suppose there is a large number of people, each flipping a fair coin every minute. Assigning the value 1 to a head and 0 to a tail.</p><h2 id=ctcv-process--brownian-motion>CTCV Process – Brownian Motion
<a class=anchor href=#ctcv-process--brownian-motion>#</a></h2><p>The motion of microscopic particles colliding with fluid.</p><ul><li>Molecules results in a process $X(t)$ that consists of the motions of all particles.</li><li>A single realization $X(t,\xi_i)$ is the motion of a specific particle (sample).</li></ul><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-15-20-09-20.png alt></p><h2 id=regular-process>Regular Process
<a class=anchor href=#regular-process>#</a></h2><p>Under certain conditions, the statistics of a regular process $X(t)$ can be determined from a single sample.</p><h3 id=predictable-process--voltage-of-ac-generator>Predictable Process – Voltage of AC Generator
<a class=anchor href=#predictable-process--voltage-of-ac-generator>#</a></h3><p>For $x(t)=r\cos(\omega t + \varphi)$ where both the amplitude $r$ and phase $\varphi$ are random, a single sample is $x(t,\xi_i)=r(\xi_i)\cos[\omega t+\varphi(\xi_i)]$. This process is a family of pure sine waves and is completely specified and predictable (or deterministic) with random variables $r$ and $\varphi$.</p><p>However, a single sample <strong>does not fully specify the properties of the entire process</strong> as it depends on the specific values of $r(\xi)$ and $\varphi(\xi)$.</p><h2 id=equality>Equality
<a class=anchor href=#equality>#</a></h2><p>Two stochastic processes $X(t)$ and $Y(t)$ are equal (everywhere) <strong>if their respective samples $X(t)$ and $Y(t)$ are equal in the mean-square (MS) sense, i.e., $E{|X(t)-Y(t)|^{2}}=0$,</strong> or $X(t,\xi)=Y(t,\xi)$ for all sample $\xi$(X(t, $\xi$) - Y(t, $\xi$) = 0 with probability 1 -> w.p.1).</p><h2 id=first-order-distribution--density-function>First-Order Distribution & Density Function
<a class=anchor href=#first-order-distribution--density-function>#</a></h2><p>If $X(t)$ is a stochastic process, for a fixed $t$, $X(t)$ represents a random variable.</p><p>Its distribution function is $F_{x}(x,t)=P{X(t)\leq x}$, which depends on $t$ since different $t$ values result in different random variables, means fixed $t$, $X(t)$ will not exceed $x$ with probability $F_{x}(x,t)$.</p><p>The first-order probability density function of the process $X(t)$ is $f_{x}(x,t)=\frac{dF_{x}(x,t)}{dx}$.</p><h3 id=frequency-interpretation>Frequency Interpretation
<a class=anchor href=#frequency-interpretation>#</a></h3><p>For $X(t,\xi)$, if we conduct $n$ experiments with outcomes $\xi_i$ ($i = 1,\cdots,n$), we get $n$ functions $X(t,\xi_i)$.</p><p>Let $n_{t}(x)$ be the number of trials such that at time $t$, $x(t,\xi)\leq x$. Then $F(x,t)\approx\frac{n_{t}(x)}{n}$. <strong>So we can think of $F(x,t)$ as the probability that $X(t)$ will not exceed $x$</strong>, when the n is large.</p><h2 id=second-order-and-n-th-order-properties>Second-Order and N-th Order Properties
<a class=anchor href=#second-order-and-n-th-order-properties>#</a></h2><ul><li><strong>Second - Order Distribution/Density Function</strong>: For $t = t_1$ and $t = t_2$, $X(t)$ represents two different random variables $x_1 = X(t_1)$ and $x_2 = X(t_2)$. Their joint distribution is $F_{x}(x_1,x_2,t_1,t_2)=P{X(t_1)\leq x_1,X(t_2)\leq x_2}$, and the joint density function is $f_{x}(x_1,x_2,t_1,t_2)=\frac{\partial^{2}F_{x}(x_1,x_2,t_1,t_2)}{\partial x_1\partial x_2}$. Given $F(x_1,x_2,t_1,t_2)$, we can find $F(x_1,t_1)=F(x_1,\infty,t_1,t_2)$ and $f(x_1,t_1)=\int_{-\infty}^{\infty}f(x_1,x_2,t_1,t_2)dx_2$.</li><li><strong>N-th Order Density Function</strong>: $f_{x}(x_1,x_2,\cdots,x_n,t_1,t_2,\cdots,t_n)$ represents the $n$-th order density function of the process $X(t)$. <strong>Completely specifying the stochastic process $X(t)$ requires</strong> knowing $f_{x}(x_1,x_2,\cdots,x_n,t_1,t_2,\cdots,t_n)$ for all $t_i$ ($i = 1,2,\cdots,n$) and all $n$, which is almost <strong>impossible</strong> in reality.</li></ul><h3 id=second-order-properties>Second-Order Properties
<a class=anchor href=#second-order-properties>#</a></h3><blockquote><p><code>*</code> means Complex Conjugate</p></blockquote><ul><li><strong>Mean Value</strong>: The mean value of a process $X(t)$ is:
$$
\mu(t)=E\{X(t)\}=\int_{-\infty}^{+\infty}xf_{x}(x,t)dx
$$</li></ul><p>In general, the mean of a process can <strong>depend on</strong> the time index $t$.</p><ul><li><strong>Autocorrelation Function</strong>: Defined as
$$
R_{xx}(t_1,t_2)=E\{X(t_1)X^{\ast}(t_2)\}=\iint x_1 x_1^{\ast}f_{x}(x_1,x_2,t_1,t_2)dx_1dx_2
$$<ul><li>It satisfies $R_{xx}(t_1,t_2)=R_{xx}^{*}(t_2,t_1)$ and is a non-negative definite function.</li><li>i.e., $\sum_{i = 1}^{n}\sum_{j = 1}^{n}a_{i}a_{j}^{*}R_{xx}(t_i,t_j)\geq0$ for any set of constants ${a_{i}}_{i = 1}^{n}$.</li><li>The average power of $X(t)$ is $E{|X(t)|^{2}}=R_{xx}(t,t)>0$.</li></ul></li><li><strong>Autocovariance</strong>: The autocovariance of the random variables $X(t_1)$ and $X(t_2)$ is $C_{xx}(t_1,t_2)=R_{xx}(t_1,t_2)-\mu_{x}(t_1)\mu_{x}^{*}(t_2)$.<ul><li>$C_{xx}(t, t) = Var(X(t))$</li></ul></li></ul><h2 id=examples>Examples
<a class=anchor href=#examples>#</a></h2><ol><li>For a deterministic $x(t)$, specific calculations can be made according to relevant rules.</li><li>Given a process with $\mu(t)=3$ and $R(t_1,t_2)=9 + 4e^{-0.2|t_2 - t_1|}$, we can calculate the means, variances, and covariances of related random variables.</li><li>For $z=\int_{-T}^{T}X(t)dt$, $E[|z|^{2}]=\int_{-T}^{T}\int_{-T}^{T}E{X(t_1)X^{*}(t_2)}dt_1dt_2=\int_{-T}^{T}\int_{-T}^{T}R_{xx}(t_1,t_2)dt_1dt_2$.</li><li>For $X(t)=a\cos(\omega_0t+\varphi)$ with $\varphi\sim U(0,2\pi)$, $\mu_{x}(t)=0$ and $R_{xx}(t_1,t_2)=\frac{a^{2}}{2}\cos\omega_0(t_1 - t_2)$.</li></ol><h3 id=possion-random-variable>Possion Random Variable
<a class=anchor href=#possion-random-variable>#</a></h3><p>For the $t$ ，$X(t)$ is a Poisson random variable with parameter $\lambda t$ ，its mean $E{X(t)}=\eta(t)=\lambda t$ 。This indicates that the average level of the value of the random variable at time $t$ is determined by $\lambda t$ , and $\lambda$ can be understood as the average rate of events occurring in a unit time.</p><ul><li><strong>Autocorrelation Function</strong>:
$$
R(t_1,t_2)=\begin{cases}\lambda t_2+\lambda^{2}t_1t_2 & t_1\geq t_2\\\lambda t_1+\lambda^{2}t_1t_2 & t_1\leq t_2\end{cases}
 $$
it describes the correlation degree between the Poisson process at different times $t_1$ and $t_2$ 。</li><li><strong>Autocovariance Function</strong>：
$$
C(t_1,t_2)=\lambda\min(t_1,t_2)=\lambda t_1U(t_2 - t_1)+ \lambda t_2U(t_1 - t_2)
$$
where $U(\cdot)$ is the unit step function. The autocovariance measures the degree of deviation of random variables from their respective means at different times.</li></ul><h3 id=properties-of-independent-poisson-processes>Properties of Independent Poisson Processes
<a class=anchor href=#properties-of-independent-poisson-processes>#</a></h3><ul><li><strong>Sum of Independent Poisson Processes</strong>: If $X_1(t)$ and $X_2(t)$ are independent Poisson processes with parameters $\lambda_1t$ and $\lambda_2t$ respectively, then their sum $X_1(t)+X_2(t)$ is also a Poisson process with parameter $(\lambda_1 + \lambda_2)t$. This property highlights the special nature of Poisson processes when dealing with multiple independent, similar random event streams.</li><li><strong>Difference of Independent Poisson Processes</strong>: The difference $X_1(t)-X_2(t)$ of two independent Poisson processes is not a Poisson process, indicating that Poisson processes <strong>do not</strong> possess the simple properties of addition and subtraction like other random processes.</li></ul><h2 id=point-and-renewal-processes>Point and Renewal Processes
<a class=anchor href=#point-and-renewal-processes>#</a></h2><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-15-21-09-44.png alt></p><ul><li><strong>Point Process</strong>: A set of random points $t$ on the time axis. Defining a stochastic process $X(t)$ as the number of points in the interval $(0,t)$ gives a Poisson process.</li><li><strong>Renewal Process</strong>: For a point process, we can associate a sequence of random variables $Z_1 = t_0$, $Z_n=t_n - t_{n - 1}$. This sequence is called a renewal process.</li><li>This can be exemplified by the life history of light bulbs(renewal processes $Z_n$) that are replaced when they fail(point processes $t_n$).</li></ul><h2 id=poisson-process>Poisson Process
<a class=anchor href=#poisson-process>#</a></h2><ul><li><strong>Properties</strong>:<ul><li>P1: The number $n(t_1,t_2)$ of <strong>points</strong> $T_i$ in an interval $(t_1,t_2)$ of length $t=t_2 - t_1$ is a Poisson random variable with parameter $\lambda t$, i.e.,
$$
P\{n(t_1,t_2)=k\}=\frac{e^{-\lambda t}(\lambda t)^{k}}{k!}
$$</li><li>P2: If the intervals $(t_1,t_2)$ and $(t_3,t_4)$ are non-overlapping, then the random variables $n(t_1,t_2)$ and $n(t_3,t_4)$ are independent.</li></ul></li><li>Defining $X(t)=n(0,t)$ gives a discrete-state process consisting of a family of increasing staircase functions with discontinuities at the points $t_i$.</li></ul><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-15-21-14-53.png alt></p><h3 id=random-selection-of-poisson-points>Random Selection of Poisson Points
<a class=anchor href=#random-selection-of-poisson-points>#</a></h3><p>Let $X(t)\sim P(\lambda t)$ be a Poisson process with parameter $\lambda t$. If each occurrence of $X(t)$ is tagged (selected) independently with probability $p$, then $Y(t)$ (the total number of tagged events in $(0,t)$) $\sim P(\lambda p t)$ and $Z(t)$ (the total number of un-tagged events in $(0,t)$) $\sim P(\lambda q t)$ ($q = 1 - p$).</p><h2 id=poisson-points-and-binomial-distribution>Poisson Points and Binomial Distribution
<a class=anchor href=#poisson-points-and-binomial-distribution>#</a></h2><p>For $t_1\lt t_2$,</p>$$
P\{x(t_1)=k|x(t_2)=n\}=\binom{n}{k}(\frac{t_1}{t_2})^{k}(1 - \frac{t_1}{t_2})^{n - k}\sim B(n,\frac{t_1}{t_2})
$$<p>for $k = 0,1,\cdots,n$.</p><p>Given that only one Poisson occurrence has taken place in an interval of length $T$, the conditional probability density function of the corresponding arrival instant is <strong>uniform</strong> in that interval.</p><h2 id=some-general-properties>Some General Properties
<a class=anchor href=#some-general-properties>#</a></h2><ul><li>The statistical properties of a real stochastic process $x(t)$ are completely determined by its $n$-th order distribution. $F_x(x_1,x_2,\cdots,x_n,t_1,t_2,\cdots,t_n)$.</li><li><strong>Autocorrelation</strong>: $R_{xx}(t_1,t_2)=E\{x(t_1)x^{\ast}(t_2)\}$ is a positive-definite function, $R(t_2,t_1)=E\{x(t_2)x^{\ast}(t_1)\}=R^{*}(t_1,t_2)$ and $R(t,t)=E{|x(t)|^{2}}\geq0$.</li><li><strong>Autocovariance</strong>: $C(t_1,t_2)=R(t_1,t_2)-\eta(t_1)\eta^{*}(t_2)$ with $\eta(t)=E{x(t)}$.</li><li><strong>Correlation Coefficient</strong>: $r(t_1,t_2)=\frac{C(t_1,t_2)}{\sqrt{C(t_1,t_1)C(t_2,t_2)}}$.</li><li><strong>Cross-Correlation</strong>: The cross-correlation of two processes $X(t)$ and $Y(t)$ is $R_{xy}(t_1,t_2)=E\{x(t_1)y^{\ast}(t_2)\}=R_{yx}^{*}(t_2,t_1)$.</li><li><strong>Cross-Covariance</strong>: Their <strong>cross-covariance</strong> is $C_{x,y}(t_1,t_2)=R_{xy}(t_1,t_2)-\eta_{x}(t_1)\eta_{y}^{*}(t_2)$.</li></ul><h2 id=relationships-between-two-processes>Relationships Between Two Processes
<a class=anchor href=#relationships-between-two-processes>#</a></h2><ul><li><strong>Orthogonal</strong>: Two processes $X(t)$ and $Y(t)$ are (mutually) orthogonal if $R(t_1,t_2)=0$ for every $t_1$ and $t_2$.</li><li><strong>Uncorrelated</strong>: They are uncorrelated if $C(t_1,t_2)=0$ for every $t_1$ and $t_2$. This means that the two processes are <strong>independent</strong> at different times.</li><li><strong>$\alpha$-dependent Processes</strong>: If values $x(t)$ for $t\lt t_0$ and $t\gt t_0+\alpha$ are mutually independent, then $C(t_1,t_2)=0$ for $|t_1 - t_2|>\alpha$.</li></ul><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-15-21-52-35.png alt></p><ul><li><strong>Independent</strong>: The two processes are independent if $X(t_1)$&mldr;$X(t_n)$ and $Y(t_1)$&mldr;$Y(t_n)$ are mutually independent.</li><li><strong>White Noise</strong>: A process $V(t)$ is white noise if its values $V(t_i)$ and $V(t_j)$ are uncorrelated for every $t_i\neq t_j$ ($C(t_i,t_j)=0$). If $V(t_i)$ and $V(t_j)$ are not only uncorrelated but also independent, then $V(t)$ is <strong>strictly white noise</strong> and $E[V(t)] = 0$.</li></ul><h2 id=normal-processes>Normal Processes
<a class=anchor href=#normal-processes>#</a></h2><p>For any $n$ and any time $t_1$,⋯,$t_n$, the random variables $X(t_1)$&mldr;$X(t_n)$ are jointly normal.</p><ul><li>The first-order density function $f(x,t)$ is the normal density $N[\eta(t);\sqrt{C(t,t)}]$, which means that only the mean and autocovariance at a specific time are needed to determine the probability density function of the random variable at that time.</li><li>The second-order density function $f(x_1,x_2;t_1,t_2)$ is the joint normal density $N(\eta_1(t),\eta_2(t);\sqrt{C(t_1,t_1)},\sqrt{C(t_2,t_2)},r(t_1,t_2))$, which involves the means, autocovariances, and correlation coefficients of two different times, representing the joint probability distribution of the random variables at these two times.</li><li>Given any function $\eta(t)$ and a positive-definite function $C(t_1,t_2)$, we can construct a normal process with mean $\eta(t)$ and autocovariance $C(t_1,t_2)$. This indicates that theoretically, we can construct a normal random process according to specific mean function and autocovariance function.</li></ul><h2 id=stationary-stochastic-processes>Stationary Stochastic Processes
<a class=anchor href=#stationary-stochastic-processes>#</a></h2><p>Stationary processes exhibit statistical properties that are invariant to shift in the time index.</p><h3 id=strict-sense-stationarity>Strict-Sense Stationarity
<a class=anchor href=#strict-sense-stationarity>#</a></h3><ul><li>First-order stationarity: The processes $X(t)$ and $X(t + c)$ have the same statistics for any $c$.</li><li>Second-order stationarity: The processes ${x(t_1),x(t_2)}$ and ${X(t + c),X(t_2 + c)}$ have the same statistics for any $c$.</li><li>A process is nth-order strict-sense stationary if $f_{x}(x_1,x_2,\cdots,x_n,t_1,t_2,\cdots,t_n)\equiv f_{x}(x_1,x_2,\cdots,x_n,t_1 + c,t_2 + c,\cdots,t_n + c)$ for any $c$, all $i = 1,\cdots,n$, and $n = 1,2,\cdots$.</li></ul><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-15-22-12-22.png alt></p><ul><li>For a first-order strict-sense stationary process, $f_{x}(x,t)\equiv f_{x}(x,t + c)$ for any $c$, and $E[X(t)] = \int_{-\infty}^{\infty}xf_{x}(x)dx = \mu$ is a constant.</li><li>For a second-order strict-sense stationary process, the second-order density function depends only on the difference of the time indices $t_1 - t_2 = \tau$, and the autocorrelation function $R_{xx}(t_1,t_2)=R_{xx}(t_1 - t_2)$.</li></ul><p>In that case the autocorrelation function is given by</p>$$
\begin{align*}
R_{xx}(t_1,t_2) &= E\{X(t_1)X^{\ast}(t_2)\} \\
&= \iint x_1x_2^{\ast} f_x(x_1,x_2,\tau = t_1 - t_2)dx_1dx_2\\
&= R_{xx}(t_1 - t_2)=\hat{R}_{xx}(\tau)=R_{xx}^{\ast}(-\tau),
\end{align*}
$$<h2 id=wide-sense-stationarity>Wide-Sense Stationarity
<a class=anchor href=#wide-sense-stationarity>#</a></h2><p>A process $X(t)$ is wide-sense stationary if:</p><ol><li>$E\{X(t)\}=\mu$ (constant)</li><li>Autocorrelation function: $E\{X(t_1)X^{*}(t_2)\}=R_{xx}(t_1 - t_2)$</li></ol><p>Strict-sense stationarity always implies wide-sense stationarity, but the converse is generally not true, except for <strong>Gaussian processes</strong>. For a Gaussian process, wide-sense stationarity implies strict-sense stationarity because the joint probability density function of Gaussian random variables depends only on their second-order statistics. The Gaussian process:</p>$$
\phi_{X}(\omega_1,\omega_2,\cdots,\omega_n)=e^{j\sum_{k = 1}^{n}\mu\omega_k-\frac{1}{2}\sum_{i = 1}^{n}\sum_{k = 1}^{n}C_{ik}(t_i - t_k)\omega_i\omega_k}
$$<p>In general, in the Gaussion Process, the wide sense stationary process(w.s.s) $\Rightarrow$ strict-sense stationary(s.s.s).</p><ul><li><strong>Examples</strong>:<ul><li>For a white noise process, $E{V(t)}=0$ and $E{V(t_1)V^{*}(t_2)}=0$ for $t_1\neq t_2$.</li><li>For a Poisson process, $E{n(t)}=\lambda t$ and $E{n(t_1)n(t_2)}=\lambda^2t_1t_2$ for $t_1\neq t_2$.</li></ul></li></ul><h2 id=centered-process>Centered Process
<a class=anchor href=#centered-process>#</a></h2><ul><li>Given a process $X(t)$ with mean $\eta(t)$ and autocovariance $C_{x}(t_1,t_2)$. The difference $\bar{X}(t)=X(t)-\eta(t)$ is called <strong>the centered process</strong> associated with the $X(t)$.</li><li>$E{\bar{X}(t)} = 0$, $R_{\bar{X}}(t_1,t_2)=C_{x}(t_1,t_2)$.<ul><li>Due to the fact that $E{\bar{X}(t)} = 0$(which meets the requirement that the mean is constant), and if the process $X(t)$ is covariance stationary, that is, if $C_{x}(t_1,t_2)=C_{x}(t_1 - t_2)$ = $C_{x}(t_2 - t_1)$ = $R_{\bar{X}}(t_1,t_2)$, then its centered process $\bar{X}(t)$ is WSS.</li></ul></li></ul><h3 id=other-forms-of-stationarity>OTHER FORMS OF STATIONARITY
<a class=anchor href=#other-forms-of-stationarity>#</a></h3><ul><li><p><strong>Asymptotically stationary</strong>: if the statistics of the r.v. $X(t_1 + c), \ldots, X(t_n + c)$ do not depend on $c$ if $c$ is large. More precisely, the function $f(x_1,\ldots,x_n, t_1 + c, \ldots, t_n + c)$ tends to a limit (that does not depend on $c$) as $c \to \infty$.</p></li><li><p><strong>Nth-order stationary</strong>: if $f_x(x_1,x_2,\cdots,x_n, t_1,t_2,\cdots,t_n) \equiv f_x(x_1,x_2,\cdots,x_n, t_1 + c,t_2 + c,\cdots,t_n + c)$ holds not for every $n$, but <strong>only for $n &lt;= N$</strong></p></li><li><p><strong>Stationary in an interval</strong>: if the above holds for every $t_i$ and $t_i + c$ in this interval.</p></li><li><p>$X(t)$ is a process with <strong>stationary increments</strong> if its increments $Y(t)=X(t + h)-X(t)$ form a stationary process for every $h$. Remember the Poisson process?</p></li></ul><h3 id=mean-square-periodicity>MEAN SQUARE PERIODICITY
<a class=anchor href=#mean-square-periodicity>#</a></h3><ul><li>MS periodic if $E{|X(t + T)-X(t)|^2}=0$ for every $t$<ul><li>For a specific $t$, $X(t + T)=X(t)$ w.p. 1 with probability 1</li></ul></li><li>A process $X(t)$ is MS periodic iff(if and only if) its autocorrelation is doubly periodic, that is, if $R(t_1 + mT, t_2 + nT)=R(t_1, t_2)$ for every integer $n$ and $m$.</li></ul><h2 id=stochastic-systems>STOCHASTIC SYSTEMS
<a class=anchor href=#stochastic-systems>#</a></h2><p>A few components of stochastic systems:</p><ul><li>Random time-delays</li><li>Noisy (modelled as random) disturbances</li><li>Stochastic dynamic processes</li></ul><h2 id=stochastic-models>STOCHASTIC MODELS
<a class=anchor href=#stochastic-models>#</a></h2><p>The resulting distribution provides an estimate of which outcomes are most likely to occur and the potential range of outcomes.</p><p>Creating a stochastic model involves a set of “equations” with inputs that represent uncertainties over time. Therefore, stochastic models will produce different results every time the model is run.</p><h2 id=how-stochastic-models-work>HOW STOCHASTIC MODELS WORK?
<a class=anchor href=#how-stochastic-models-work>#</a></h2><p>To estimate the probability of each outcome, one or more of the inputs must allow for random variation over time. It results in an estimation of the probability distributions.</p><h2 id=systems-with-stochastic-inputs>Systems with Stochastic Inputs
<a class=anchor href=#systems-with-stochastic-inputs>#</a></h2><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-18-22-35-37.png alt></p><p>A deterministic system transforms each input waveform $X(t, \xi_{i})$ into an output waveform $Y(t, \xi_{i}) = T[X(t, \xi_{i})]$ operating only on the time variable $t$. Thus a set of realizations at the input corresponding to a process $X(t)$ generates a new set of realizations $Y(t,\xi_{i})$ at the output associated with a new process $Y(t)$.</p><p>Our goal is to study the output process statistics in terms of the input process statistics and the system function.</p><p><strong>A stochastic system on the other hand operates on both the variables $t$ and $\xi$.</strong></p><ul><li><p>RANDOM BINARY SEQUENCE: $\zeta=\sum_{i=1}^{\infty} b_{i} 2^{-i}, b_{i} \in{0,1} .$
$P[X(1, \zeta)=0]=P\left[0\leq\zeta&lt;\frac{1}{2}\right]=\frac{1}{2}$
$P[X(1, \zeta)=0\text{ and }X(2, \zeta)=1]=P\left[\frac{1}{4}\leq\zeta&lt;\frac{1}{2}\right]=\frac{1}{4}$
since all points in the interval $[0\leq\zeta\leq1]$ begin with $b_{1}=0$ and all points in $[\frac{1}{4},\frac{1}{2})$ begin with $b_{1}=0$ and $b_{2}=1$. Clearly, any sequence of $k$ bits has a corresponding subinterval of length (and hence probability) $2^{-k}$.</p></li><li><p>RANDOM SINUSOIDS</p><ul><li>$X(t, \zeta)=\zeta\cos(2\pi t),-\infty&lt;t&lt;\infty$</li><li>$Y(t, \zeta)=\cos(2\pi t+\zeta),-\infty&lt;t&lt;\infty$</li></ul></li><li><p>BINOMIAL COUNTING PROCESS</p><ul><li>Let $X_{n}$ be a sequence of independent, identically distributed Bernoulli random variables with $p = 1/2$.</li><li>Let $S_{n}$ be the number of 1 in the first $n$ trials: $S_{n}=X_{1}+X_{2}+\cdots+X_{n}\text{ for }n = 0,1,&mldr;$</li><li>$S_{n}$ is a binomial random variable with parameters $n$ and $p = 1/2$.</li></ul></li><li><p>FILTERED NOISY SIGNAL(Additive Noise)</p><ul><li>Let $x_{j}$ be a sequence of independent, identically distributed observations of a signal voltage $\mu$ corrupted by zero-mean Gaussian noise $N_{j}$ with variance $\sigma^{2}$: $X_{j}=\mu+N_{j}\text{ for }j = 0,1,&mldr;$</li><li>Consider the signal that results from averaging the sequence of observations: $S_{n}=\left(X_{1}+X_{2}+\cdots+X_{n}\right)/n\text{ for }n = 0,1,&mldr;$</li><li>From previous chapters we know that $S_{n}$ is the sample mean of an i.i.d sequence of Gaussian random variables. We know that $S_{n}$ itself is a Gaussian random variable with mean $\mu$ and variance $\sigma^{2}/n$, and so it tends towards the value $\mu$ as $n$ increases.</li></ul></li></ul><h3 id=k-th-order-joint-cumulative-distribution-function>K-TH ORDER JOINT CUMULATIVE DISTRIBUTION FUNCTION
<a class=anchor href=#k-th-order-joint-cumulative-distribution-function>#</a></h3><p>Let $X_{1},X_{2},&mldr;,X_{n}$ be the $k$ random variables obtained by sampling the random process $X(t, \zeta)$ at the times $t_{1},t_{2},\ldots,t_{k}$, $X_{1}=X\left(t_{1}, \zeta\right),X_{2}=X\left(t_{2}, \zeta\right),&mldr;,X_{k}=X\left(t_{k}, \zeta\right)$</p>$$
F_{X_{1},...,X_{k}}\left(x_{1},x_{2},...,x_{k}\right)=P\left[X\left(t_{1}\right)\leq x_{1},X\left(t_{2}\right)\leq x_{2},...,X\left(t_{k}\right)\leq x_{k}\right]
$$<p>Then</p>$$
f_{X_{1},...,X_{k}}(x_{1},x_{2},...,x_{k})dx_{1}...dx_{k}=
P\{x_{1} \leq X(t_{1}) \leq x_{1} + dx_{1},...,x_{k} \leq X(t_{k}) \leq x_{k} + dx_{k}\}
$$<p>Discrete Case:</p>$$
P_{X_{1},...,X_{k}}\left(x_{1},x_{2},...,x_{k}\right)=P\left[X\left(t_{1}\right)=x_{1},X\left(t_{2}\right)=x_{2},...,X\left(t_{k}\right)=x_{k}\right]
$$<h2 id=deterministic-vs-stochastic>DETERMINISTIC VS. STOCHASTIC
<a class=anchor href=#deterministic-vs-stochastic>#</a></h2><p>Transformation $T: Y(t)=T[X(t)]$ is:</p><ul><li><strong>deterministic</strong> if it operates only on the variable $t$ treating $\xi$ as a parameter.</li><li><strong>stochastic</strong> if $T$ operates on both variables $t$ and $\xi$. There exist two outcomes $\xi_{1}$ and $\xi_{2}$ such that $X(t, \xi_{1}) = X(t, \xi_{2})$ identically in $t$ but $Y(t, \xi_{1})\neq Y(t, \xi_{2})$.</li></ul><h2 id=memoryless-systems>Memoryless Systems
<a class=anchor href=#memoryless-systems>#</a></h2><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-18-23-15-47.png alt></p><ul><li>Memoryless: $Y(t)=2X(t)$, and $Y(t)=\cos(X(t))$</li><li>Not-memoryless: $Y(t)=X(t + \tau)+X(t)$, and $Y(t)=X(t)e^{X(t-\tau)}$</li></ul><p>The output $Y(t)$ in this system depends only on the present value of the input $X(t)$, $Y(t)=g{X(t)}$. The current output value $Y(t)$ can be determined from knowing only the value of the current input $X(t)$.</p><ul><li>Strict-sense stationary input -> Memoryless system -> Strict-sense stationary output.</li><li>Wide-sense stationary input -> Memoryless system -> Need not be stationary in any sense.</li><li>$x(t)$ stationary Gaussian with Memoryless system, $Y(t)$ stationary, but not Gaussian with $R_{Y}(\tau)=\eta^2R_{X}(\tau)$.</li></ul><h2 id=statistics-of-y--gxt>STATISTICS OF Y = G[X(T)]
<a class=anchor href=#statistics-of-y--gxt>#</a></h2><ul><li><p>1st order: $f_{y}(y ; t)\to f_{x}(x ; t)$, similar as for functions of random variables
$E{y(t)}=\int_{-\infty}^{\infty}g(x)f_{x}(x ; t)dx$</p></li><li><p>2nd order: $f_{y}(y_{1},y_{2} ; t_{1},t_{2})\to f_{x}(x_{1},x_{2} ; t_{1},t_{2})$</p>$$
E\left\{y\left(t_{1}\right)y\left(t_{2}\right)\right\}=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g\left(x_{1}\right)g\left(x_{2}\right)f_{x}\left(x_{1},x_{2} ; t_{1},t_{2}\right)dx_{1}dx_{2}
$$</li><li><p>nth - order: $f_{y}(y_{1},&mldr;,y_{n} ; t_{1},&mldr;,t_{n})\to f_{x}(x_{1},&mldr;,x_{n} ; t_{1},&mldr;,t_{n})$ need to solve the system $g(x_{1}) = y_{1},\cdots,g(x_{n}) = y_{n}$. If this system has a unique solution
$f_{y}\left(y_{1},&mldr;,y_{n} ; t_{1},&mldr;,t_{n}\right)=\frac{f_{x}\left(x_{1},&mldr;,x_{n} ; t_{1},&mldr;,t_{n}\right)}{\left|g^{\prime}\left(x_{1}\right)\cdots g^{\prime}\left(x_{n}\right)\right|}$</p></li><li><p>If $X(t)$ is stationary of order N, then $Y(t)$ is stationary of order $N$.</p></li><li><p>If $X(t)$ is stationary in an interval, then $Y(t)$ is stationary in the same interval.</p></li><li><p>If $x(t)$ is WSS stationary, then $y(t)$ might not be stationary in any sense.</p></li></ul><h2 id=square---law-detector>SQUARE - LAW DETECTOR
<a class=anchor href=#square---law-detector>#</a></h2><p>A memory-less system whose output equals $Y(t)=X^{2}(t)$. The system $y = x^{2}$ has the two solutions $\pm\sqrt{y}$ and $y^{\prime}(x)=\pm2\sqrt{y}$</p>$$
f_{y}(y ; t)=\frac{1}{2\sqrt{y}}\left[f_{x}(\sqrt{y} ; t)+f_{x}(-\sqrt{y} ; t)\right]
$$<p>If $y_{1}>0$ and $y_{2}>0$, then the system $y_{1}=x^{2},y_{2}=x^{2}$ has the four solutions $(\pm\sqrt{y_{1}},\pm\sqrt{y_{2}})$. Its Jacobian is $\pm4\sqrt{y_{1}y_{2}}$</p>$$
f_{y}\left(y_{1},y_{2} ; t_{1},t_{2}\right)=\frac{1}{4\sqrt{y_{1}y_{2}}}\sum f_{x}\left(\pm\sqrt{y_{1}},\pm\sqrt{y_{2}} ; t_{1},t_{2}\right)
$$<p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-18-23-31-21.png alt></p><h2 id=hard-limiter>HARD LIMITER
<a class=anchor href=#hard-limiter>#</a></h2><p>A memoryless system with $g(x)=\begin{cases}1& \text{if }x>0\ - 1& \text{else}\end{cases}$</p>$$
P(Y(t)=1)=P(X(t)>0)=1 - P(X(t)\leq0)
$$$$
E(Y(t))=P(X(t)>0)-P(X(t)<0)
$$$$
R_{Y}(0)=1\times P(Y(0)=1)-(-1)\times P(Y(0)= - 1)=1 - 2P(X(0)\leq0)
$$$$
Y(t+\tau)Y(t)=\begin{cases}1& \text{if }X(t+\tau)X(t)\geq0\\ - 1& \text{else}\end{cases}\Rightarrow R_{Y}(\tau)=P\{X(t+\tau)X(t)>0\}-P\{X(t+\tau)X(t)<0\}
$$<h2 id=linear-systems>Linear Systems
<a class=anchor href=#linear-systems>#</a></h2><p>$L$ represents a linear system if for any $a_{1},a_{2},X_{1}(t),X_{2}(t)$:</p>$$
L\left\{a_{1}X\left(t_{1}\right)+a_{2}X\left(t_{2}\right)\right\}=a_{1}L\left\{X\left(t_{1}\right)\right\}+a_{2}L\left\{X\left(t_{2}\right)\right\}
$$<p>True even if $a_{1},a_{2}$ are r.v.s.</p><p>Let $Y(t)=L\{X(t)\}$ represent the output of a linear system.</p><h2 id=time-invariant-system>Time Invariant System
<a class=anchor href=#time-invariant-system>#</a></h2><p>$L$ represents a time invariant system if $Y(t)=L{X(t)}\Rightarrow L\{X(t - t_{0})\}=Y(t - t_{0})$. Shift in the input results in the same shift in the output also.</p><ul><li>If $L$ satisfies both equations, then it corresponds to a linear time invariant (LTI) system.</li><li>LTI systems can be uniquely represented in terms of their output to a delta function $h(t)=L[\delta(t)]$, the system impulse response. Then $Y(t)=\int_{-\infty}^{+\infty}h(t-\tau)X(\tau)d\tau=\int_{-\infty}^{+\infty}h(\tau)X(t-\tau)d\tau$</li></ul><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-18-23-37-39.png alt></p><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-18-23-42-04.png alt></p><h2 id=fundamental-theorem>FUNDAMENTAL THEOREM
<a class=anchor href=#fundamental-theorem>#</a></h2><ul><li>For any linear system $E\{L[X(t)]\}=L[E\{X(t)\}]$<ul><li>$\eta_y(t)=L[\eta_x(t)]$</li><li>$E{y(t)}=\int_{-\infty}^{\infty}E\{x(t - \alpha)\}h(\alpha)d\alpha=\eta_x(t)*h(t)$</li></ul></li><li>Frequency interpretation: At the $i$-th trial the input to the system is a function $X(t,\xi_i)$ yielding output function $Y(t,\xi_j)=L[X(t,\xi_j)]$.</li></ul><p>For large $n$,</p>$$
E\{y(t)\}\simeq\frac{y(t,\zeta_1)+\cdots+y(t,\zeta_n)}{n}=\frac{L[x(t,\zeta_1)]+\cdots+L[x(t,\zeta_n)]}{n}\Rightarrow E[L[X(t)]]
$$<h3 id=output-statistics>Output Statistics
<a class=anchor href=#output-statistics>#</a></h3><p>The mean of the output process:</p>$$
\begin{align*}
\mu_y(t)&=E\{Y(t)\}=\int_{-\infty}^{\infty}E\{X(\tau)\}h(t - \tau)d\tau\\
&=\int_{-\infty}^{\infty}\mu_x(\tau)h(t - \tau)d\tau=\mu_x(t)*h(t)
\end{align*}
$$<p>The cross-correlation function between the input and output processes is:</p>$$
\begin{align*}
R_{xy}(t_1,t_2)&=E\{X(t_1)Y^*(t_2)\}\\
&=E\left\{X(t_1)\int_{-\infty}^{\infty}X^*(t_2-\alpha)h^*(\alpha)d\alpha\right\}\\
&=\int_{-\infty}^{\infty}E\left\{X(t_1)X^*(t_2-\alpha)\right\}h^*(\alpha)d\alpha\\
&=\int_{-\infty}^{\infty}R_{xx}(t_1,t_2-\alpha)h^*(\alpha)d\alpha\\
&=R_{xx}(t_1,t_2)*h^*(-t_2)
\end{align*}
$$<p>Finally the output autocorrelation function is given by:</p>$$
\begin{align*}
R_{yy}(t_1,t_2)&=E\{Y(t_1)Y^*(t_2)\}\\
&=E\left\{\int_{-\infty}^{\infty}X(t_1-\beta)h(\beta)d\beta Y^*(t_2)\right\}\\
&=\int_{-\infty}^{\infty}E\left\{X(t_1-\beta)Y^*(t_2)\right\}h(\beta)d\beta\\
&=\int_{-\infty}^{\infty}R_{xy}(t_1-\beta,t_2)h(\beta)d\beta\\
&=R_{xy}(t_1,t_2)*h(t_1)
\end{align*}
$$<p>or</p>$$
R_{yy}(t_1,t_2)=R_{xx}(t_1,t_2)*h^*(-t_2)*h(t_1)
$$<p>Review that:</p>$$
Y(t) = X(t)*h(t)
$$<p>So in finally, we can deduce that:</p>$$
\begin{matrix}
\mu_x(t) &\xrightarrow{h(t)}& \mu_y(t)\\
\end{matrix}
$$$$
\begin{matrix}
R_{xx}(t_1,t_2) &\xrightarrow{h^*(-t_2)}& R_{xy}(t_1,t_2) &\xrightarrow{h(t_1)}& R_{yy}(t_1,t_2)
\end{matrix}
$$<p>In particular if $X(t)$ is wide-sense stationary</p><ul><li><p>We have $\mu_x(t)=\mu_x$</p>$$
\mu_y(t)=\mu_x\int_{-\infty}^{\infty}h(\tau)d\tau=\mu_x c, \text{ a constant}
$$</li><li><p>Also $R_{xx}(t_1,t_2)=R_{xx}(t_1 - t_2)$, so that</p>$$
\begin{align*}
R_{xy}(t_1,t_2)&=\int_{-\infty}^{\infty}R_{xx}(t_1 - t_2+\alpha)h^*(\alpha)d\alpha\\
&=R_{xx}(\tau)*h^*(-\tau)=R_{xy}(\tau),\tau = t_1 - t_2
\end{align*}
$$<p>Thus $X(t)$ and $Y(t)$ are jointly w.s.s.</p></li><li><p>Further, the output autocorrelation simplifies to</p>$$
\begin{align*}
R_{yy}(t_1,t_2)&=\int_{-\infty}^{\infty}R_{xy}(t_1-\beta - t_2)h(\beta)d\beta,\tau = t_1 - t_2\\
&=R_{xy}(\tau)*h(\tau)=R_{yy}(\tau)
\end{align*}
$$</li><li><p><strong>Finally we obtain</strong></p>$$
R_{yy}(\tau)=R_{xx}(\tau)*h^*(-\tau)*h(\tau)
$$</li></ul><p>Thus the output process is also wide-sense stationary. This gives rise to the following representation:</p><ul><li><p><strong>(a)</strong></p>$$
\begin{matrix}
X(t)\\
\text{wide-sense}\\
\text{stationary process}
\end{matrix}
\stackrel{\text{LTI system }h(t)}{\longrightarrow}
\begin{matrix}
Y(t)\\
\text{wide-sense}\\
\text{stationary process}
\end{matrix}
$$</li><li><p><strong>(b)</strong></p>$$
\begin{matrix}
X(t)\\
\text{strict-sense}\\
\text{stationary process}
\end{matrix}
\stackrel{\text{LTI system }h(t)}{\longrightarrow}
\begin{matrix}
Y(t)\\
\text{strict-sense}\\
\text{stationary process}\\
\text{(see Text for proof)}
\end{matrix}
$$</li><li><p><strong>(c)</strong></p>$$
\begin{matrix}
X(t)\\
\text{Gaussian}\\
\text{process (also stationary)}
\end{matrix}
\stackrel{\text{Linear system}}{\longrightarrow}
\begin{matrix}
Y(t)\\
\text{Gaussian}\\
\text{process (also stationary)}
\end{matrix}
$$</li></ul><h2 id=white-noise-process>White Noise Process
<a class=anchor href=#white-noise-process>#</a></h2><ol><li><p>$W(t)$ is said to be a white noise process <strong>if $R_{ww}(t_1,t_2)=q(t_1)\delta(t_1 - t_2)$, i.e., $E[W(t_1)W(t_2)] = 0$ unless $t_1 = t_2$.</strong>, <strong>which means that different samples in different times are uncorrelated.</strong></p></li><li><p>$W(t)$ is said to be wide-sense stationary (w.s.s) white noise if $E[W(t)]=\text{constant}$, and $R_{ww}(t_1,t_2)=q\delta(t_1 - t_2)=q\delta(\tau)$.</p></li><li><p>If $W(t)$ is also a Gaussian process (white Gaussian process), then all of its samples are independent random variables (why?). <strong>Because the joint probability distribution of a Gaussian process is determined by its mean and covariance, and white noise has zero covariance between different times(remember the 1st principle?), it follows that the samples are independent.</strong> Remember in general, uncorrelated does not imply independent, but for Gaussian distribution, uncorrelated implies independent. When $W(t)$ is both white noise and Gaussian process, the samples of different times are Gaussian distributed and the covariance between different times is 0 (because it is white noise, uncorrelated). According to the property of Gaussian distribution that uncorrelated implies independent, all samples of $W(t)$ are independent random variables.</p></li><li><p>For w.s.s white noise input $W(t)$, we have:</p></li></ol>$$
\begin{matrix}
\text{White noise}\\
W(t)
\end{matrix}
\stackrel{\text{LTI system }h(t)}{\longrightarrow}
\begin{matrix}
\text{Colored noise}\\
N(t)=h(t)*W(t)
\end{matrix}
$$<p>The mean of the output process is:</p>$$
E[N(t)]=\mu_w\int_{-\infty}^{\infty}h(\tau)d\tau,\text{ a constant}
$$<p>and</p>$$
\begin{align*}
R_N(\tau)&=q\delta(\tau)*h^*(-\tau)*h(\tau)\\
&=q\delta(-\tau)*h(\tau)=q\rho(\tau)
\end{align*}
$$$$
\rho(\tau)=h(\tau)*h^*(-\tau)=\int_{-\infty}^{\infty}h(\alpha)h^*(\alpha + \tau)d\alpha
$$<p>Thus the output of a white noise process through an LTI system represents a (colored) noise process.</p><p>Note: “White noise” need not be Gaussian. “White” which means the power spectral density is flat(so in frequency domain, the power is the same for all frequencies), and “Gaussian” which means the distribution of the random variables is Gaussian, they are two different concepts!</p><h2 id=gaussian-vectors>GAUSSIAN VECTORS
<a class=anchor href=#gaussian-vectors>#</a></h2><ul><li>A random vector $\mathbf{X}=(X_1,\cdots,X_n)$ is said to be Gaussian <strong>if $a\cdot\mathbf{X}=a_1X_1+\cdots + a_nX_n$ is Gaussian</strong> for any vector $\mathbf{a}=(a_1,\cdots,a_n)$.<ul><li>Recall that if $X_1,\cdots,X_n$ are independent Gaussian variables, then any linear combination $a_1X_1+\cdots + a_nX_n$ is also Gaussian $\Rightarrow\mathbf{X}=X_1,\cdots,X_n$ is Gaussian. The converse is not necessarily true though!</li></ul></li><li>A random vector $\mathbf{X}=(X_1,\cdots,X_n)$ is Gaussian <strong>if and only if</strong> it has the form $X_i = \mu_i+\sum_{j = 1}^{n}a_{ij}Z_j$ where $Z_1,\cdots,Z_n$ are independent standard $N(0,1)$ r.v.s.</li><li>If $\mathbf{X}=(X_1,\cdots,X_n)$ is Gaussian, $\mathbf{X}$ and $\mathbf{Y}$ are independent if and only if $\text{Cov}(\mathbf{X},\mathbf{Y}) = 0$. <strong>More generally</strong>, if $\mathbf{X}$ is Gaussian, then $X_1,\cdots,X_n$ are independent if and only if the covariance matrix is <strong>diagonal</strong>, that is $\sigma_{ij}=0$ for all $i\neq j$, where $\sigma_{ij}=\text{Cov}(X_i,X_j)$.</li></ul><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-19-17-19-37.png alt></p><h2 id=gaussian-processes>GAUSSIAN PROCESSES
<a class=anchor href=#gaussian-processes>#</a></h2><ul><li>Let $\mathbf{X}={X_t: t\in T}$ be a stochastic process with state space $\mathbb{R}$ (that is, $X_t\in\mathbb{R}$). Then $\mathbf{X}$ is a Gaussian process if $\mathbf{X}<em>{t_1,\cdots,t_n}=(X</em>{t_1},\cdots,X_{t_n})$ is Gaussian for all choices of $n\geq2$ and $t_1,\cdots,t_n$.</li><li>If $\mathbf{X}$ is a Gaussian process, the distribution of $\mathbf{X}$ is specified for all $n$ and $t_1,\cdots,t_n$ <strong>once we specify</strong> the <strong>mean function</strong> $\mu(t)=E[X_t]$ and the <strong>covariance function</strong> $C(s, t)=\text{Cov}(X_s, X_t)=E[(X_s-\mu(s))(X_t-\mu(t))]=R(s,t)-\mu(s)\cdot\mu(t)$.<ul><li>$\mu(t)$ is arbitrary, but $C(s, t)$ must be symmetric (i.e., $C(s, t)=C(t, s)$, ensure the result is not related to the order of time) and positive-definite.</li><li>In most applications, $\mu(t)=0$.</li></ul></li></ul><h2 id=discrete-time-stochastic-processes>DISCRETE TIME STOCHASTIC PROCESSES
<a class=anchor href=#discrete-time-stochastic-processes>#</a></h2><p><img src=https://cdn.jsdelivr.net/gh/timerring/scratchpad2023/2024/2025-04-19-17-42-27.png alt></p><p>A discrete time stochastic process $\mathbf{X}={X(nT)}$ is a sequence of random variables with mean, autocorrelation and covariance functions:</p>$$
\begin{align*}
\mu_n&=E\{X(nT)\}\\
R(n_1,n_2)&=E\{X(n_1T)X^*(n_2T)\}\\
C(n_1,n_2)&=R(n_1,n_2)-\mu_{n_1}\mu_{n_2}^*
\end{align*}
$$<p>As before strict-sense stationarity and wide-sense stationarity definitions apply.</p><ul><li>For example, $X(nT)$ is wide-sense stationary if
$$
\begin{align*}
E\{X(nT)\}&=\mu,\text{ a constant}\\
E\{X((k + n)T)X^*(kT)\}&=R(n)=r_n=r_{-n}^*
\end{align*}
$$
with $R(n_1,n_2)=R(n_1 - n_2)$ and $n_2 = 0$.</li></ul><p>If $X(nT)$ represents a wide-sense stationary input to a discrete-time system $h(nT)$, and $Y(nT)$ the system output, then as before the cross-correlation function satisfies $R_{xy}(n)=R_x(n)*h(-n)$</p><p>and the output autocorrelation function is given by</p>$$
R_y(n)=R_{xy}(n)*h(n)
$$<p>or</p>$$
R_y(n)=R_x(n)*h(-n)*h(n)
$$<p>Thus wide-sense stationarity from input to output is preserved for discrete-time systems also.</p><h3 id=the-delta-response-of-a-linear-discrete-system>THE DELTA RESPONSE OF A LINEAR DISCRETE SYSTEM
<a class=anchor href=#the-delta-response-of-a-linear-discrete-system>#</a></h3><ul><li>The delta response $h[n]$ of a linear system is its response to the delta sequence $\delta[n]$, with system function $H(z)=\sum_{n = -\infty}^{\infty}h[n]z^{-n}$, the $z$-transform of $h[n]$.</li><li>If $x[n]$ is the input to a digital system, the resulting output is the digital convolution of $x[n]$ with $h[n]$:
$$
\begin{align*}
y[n]&=\sum_{k = -\infty}^{\infty}x[n - k]h[k]=x[n]*h[n]\\
R_{xx}[n_1,n_2]&=\sum_{n = -\infty}^{\infty}x[n_1 - n]x^*[n_2 - n]\\
R_{xy}[n_1,n_2]&=\sum_{n = -\infty}^{\infty}x[n_1 - n]y^*[n_2 - n]
\end{align*}
$$</li></ul><h2>Related readings</h2><ul><li><a href="/posts/sequences-of-random-variables/?utm_source=related">Sequences of Random Variables</a></li><li><a href="/posts/random-variables/?utm_source=related">Random Variables</a></li><li><a href="/posts/repeated-trials/?utm_source=related">Repeated Trials</a></li><li><a href="/posts/probability-and-its-axioms/?utm_source=related">Probability and Its Axioms</a></li></ul><hr><div class=post-nav><a href="https://blog.timerring.com/posts/multi-platform-builds-docker/?utm_source=nav">&lt;&lt; prev | Multi Platform...</a>
<a onclick=goToRandomPost() style=cursor:pointer>Continue strolling
</a><a href="https://blog.timerring.com/posts/0418-find-the-formula-rendering-issue-of-hugo/?utm_source=nav">Find the... ｜ next >></a></div><hr>If you find this blog useful and want to support my blog, need my skill for something, or have a coffee chat with me, feel free to:<div class=support><a href=https://ko-fi.com/polls/What-is-the-custom-topic-for-the-next-month-C0C219LHQJ class=book-btn target=_blank>Subscribe to participate in blog topics and custom services
</a><a href=https://ko-fi.com/timerring class=book-btn target=_blank>Buy me a coffee</a></div><div style=margin-bottom:8px></div><script>function goToRandomPost(){const e=["/posts/hidden-markov-models/?utm_source=random","/posts/markov-chains/?utm_source=random","/posts/markov-processes/?utm_source=random","/posts/stock-prices/?utm_source=random","/posts/possion-processes/?utm_source=random","/posts/random-walks/?utm_source=random","/posts/0418-find-the-formula-rendering-issue-of-hugo/?utm_source=random","/posts/stochastic-processes/?utm_source=random","/posts/multi-platform-builds-docker/?utm_source=random","/posts/0404-The-invitation-only-mechanism-from-clubhouse-to-manus/?utm_source=random","/posts/how-to-use-git-submodule/?utm_source=random","/posts/live-streaming-infra-and-protocols/?utm_source=random","/posts/implement-danmaku-rendering-algorithm-from-scratch/?utm_source=random","/posts/is-there-an-rss-renaissance-in-the-ai-era/?utm_source=random","/posts/docker-with-gpu/?utm_source=random","/posts/some-useful-tools/?utm_source=random","/posts/a-shopping-experience-in-pangdonglai-supermarket/?utm_source=random","/posts/langchain-and-rag-best-practices/?utm_source=random","/posts/sequences-of-random-variables/?utm_source=random","/posts/random-variables/?utm_source=random","/posts/repeated-trials/?utm_source=random","/posts/probability-and-its-axioms/?utm_source=random","/posts/further-understanding-of-proc/?utm_source=random","/posts/the-iftop/?utm_source=random","/posts/some-useful-commands-to-share/?utm_source=random","/posts/reflections-on-trending-topics/?utm_source=random","/posts/cpu-can-only-see-the-threads/?utm_source=random","/posts/go-common-test/?utm_source=random","/posts/go-concurrency-and-parallelism/?utm_source=random","/posts/go-cheatsheet/?utm_source=random","/posts/video-technology-101/?utm_source=random","/posts/the-main-kind-of-message-queue/?utm_source=random","/posts/the-method-to-manage-traffic/?utm_source=random","/posts/the-different-kind-of-api-design/?utm_source=random","/posts/the-encode-and-decode-in-python/?utm_source=random","/posts/about-the-systemd/?utm_source=random","/posts/the-tips-about-dockerfile/?utm_source=random","/posts/docker-cheatsheet/?utm_source=random","/posts/docker-101/?utm_source=random","/posts/the-instance-class-static-magic-method-in-python/?utm_source=random","/posts/the-review-and-plan-for-bilive/?utm_source=random","/posts/the-overview-of-security/?utm_source=random","/posts/how-to-publish-your-code-as-a-pip-module/?utm_source=random","/posts/some-good-things-to-share-about-the-packages/?utm_source=random","/posts/introduction-to-the-http-and-https-protocol/?utm_source=random","/posts/mail-service-and-protocol/?utm_source=random","/posts/understanding-clash-through-configuration/?utm_source=random","/posts/thinking-about-advertisement-from-an-open-source-perspective/?utm_source=random","/posts/a-brief-introduction-to-dns/?utm_source=random","/posts/real-computer-network/?utm_source=random","/posts/python-generator-iterator-and-decorator/?utm_source=random","/posts/python-underlying-mechanism/?utm_source=random","/posts/python-files-exceptions-and-modules/?utm_source=random","/posts/python-object-oriented-programming/?utm_source=random","/posts/python-cheatsheet/?utm_source=random","/posts/python-function-parameter/?utm_source=random","/posts/python-control-structure/?utm_source=random","/posts/python-composite-data-type/?utm_source=random","/posts/python-basic-data-type/?utm_source=random","/posts/python-basic-syntax-elements/?utm_source=random","/posts/deploy-github-pages-with-gpg-signing/?utm_source=random","/posts/gpg-101/?utm_source=random","/posts/housewarming-2024/?utm_source=random"],t=Math.floor(Math.random()*e.length);window.location.href=e[t]}</script></article><div class=book-comments><div class="book-tabs-content markdown-inner"><div id=tcomment></div><script src=https://giscus.app/client.js data-repo=timerring/blog data-repo-id=R_kgDONeZYZg data-category=Announcements data-category-id=DIC_kwDONeZYZs4ClRWs data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=en crossorigin=anonymous async></script></div></div><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script><div class=busuanzi-footer><p style=text-align:center>&copy; 2022 - present <a href=https://github.com/timerring>timerring</a> | PV: <span id=busuanzi_value_site_pv></span> | UV: <span id=busuanzi_value_site_uv></span></p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a><ul><li><a href=#stochastic-processes>Stochastic Processes</a></li><li><a href=#compare-with-rv>Compare with RV</a></li></ul></li><li><a href=#interpretations-of-xt><strong>Interpretations of $X(t)$</strong></a></li><li><a href=#continuous-vs-discrete>Continuous vs Discrete</a></li><li><a href=#dtcv-process---record-the-temperature>DTCV Process - Record the temperature</a></li><li><a href=#dtdv-process---people-flipping-coins>DTDV Process - People flipping coins</a></li><li><a href=#ctcv-process--brownian-motion>CTCV Process – Brownian Motion</a></li><li><a href=#regular-process>Regular Process</a><ul><li><a href=#predictable-process--voltage-of-ac-generator>Predictable Process – Voltage of AC Generator</a></li></ul></li><li><a href=#equality>Equality</a></li><li><a href=#first-order-distribution--density-function>First-Order Distribution & Density Function</a><ul><li><a href=#frequency-interpretation>Frequency Interpretation</a></li></ul></li><li><a href=#second-order-and-n-th-order-properties>Second-Order and N-th Order Properties</a><ul><li><a href=#second-order-properties>Second-Order Properties</a></li></ul></li><li><a href=#examples>Examples</a><ul><li><a href=#possion-random-variable>Possion Random Variable</a></li><li><a href=#properties-of-independent-poisson-processes>Properties of Independent Poisson Processes</a></li></ul></li><li><a href=#point-and-renewal-processes>Point and Renewal Processes</a></li><li><a href=#poisson-process>Poisson Process</a><ul><li><a href=#random-selection-of-poisson-points>Random Selection of Poisson Points</a></li></ul></li><li><a href=#poisson-points-and-binomial-distribution>Poisson Points and Binomial Distribution</a></li><li><a href=#some-general-properties>Some General Properties</a></li><li><a href=#relationships-between-two-processes>Relationships Between Two Processes</a></li><li><a href=#normal-processes>Normal Processes</a></li><li><a href=#stationary-stochastic-processes>Stationary Stochastic Processes</a><ul><li><a href=#strict-sense-stationarity>Strict-Sense Stationarity</a></li></ul></li><li><a href=#wide-sense-stationarity>Wide-Sense Stationarity</a></li><li><a href=#centered-process>Centered Process</a><ul><li><a href=#other-forms-of-stationarity>OTHER FORMS OF STATIONARITY</a></li><li><a href=#mean-square-periodicity>MEAN SQUARE PERIODICITY</a></li></ul></li><li><a href=#stochastic-systems>STOCHASTIC SYSTEMS</a></li><li><a href=#stochastic-models>STOCHASTIC MODELS</a></li><li><a href=#how-stochastic-models-work>HOW STOCHASTIC MODELS WORK?</a></li><li><a href=#systems-with-stochastic-inputs>Systems with Stochastic Inputs</a><ul><li><a href=#k-th-order-joint-cumulative-distribution-function>K-TH ORDER JOINT CUMULATIVE DISTRIBUTION FUNCTION</a></li></ul></li><li><a href=#deterministic-vs-stochastic>DETERMINISTIC VS. STOCHASTIC</a></li><li><a href=#memoryless-systems>Memoryless Systems</a></li><li><a href=#statistics-of-y--gxt>STATISTICS OF Y = G[X(T)]</a></li><li><a href=#square---law-detector>SQUARE - LAW DETECTOR</a></li><li><a href=#hard-limiter>HARD LIMITER</a></li><li><a href=#linear-systems>Linear Systems</a></li><li><a href=#time-invariant-system>Time Invariant System</a></li><li><a href=#fundamental-theorem>FUNDAMENTAL THEOREM</a><ul><li><a href=#output-statistics>Output Statistics</a></li></ul></li><li><a href=#white-noise-process>White Noise Process</a></li><li><a href=#gaussian-vectors>GAUSSIAN VECTORS</a></li><li><a href=#gaussian-processes>GAUSSIAN PROCESSES</a></li><li><a href=#discrete-time-stochastic-processes>DISCRETE TIME STOCHASTIC PROCESSES</a><ul><li><a href=#the-delta-response-of-a-linear-discrete-system>THE DELTA RESPONSE OF A LINEAR DISCRETE SYSTEM</a></li></ul></li></ul></nav><style>.book-toc .book-toc-content a{color:var(--body-font-color)}</style></div></aside></main></body></html>